{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Business Analyst Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will first create a business analyst agent with Amazon Bedrock Agents that will be able to detect AI/MLuse cases in a given database.\n",
    "\n",
    "Then we will evaluate the agents's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['AWS_ACCOUNT'] = os.getenv('AWS_ACCOUNT')\n",
    "os.environ['BUSINESSANALYST_AGENT_PROFILE_ARN'] = os.getenv('BUSINESSANALYST_AGENT_PROFILE_ARN')\n",
    "os.environ['BUSINESSANALYST_AGENT_EVAL_PROFILE_ARN'] = os.getenv('BUSINESSANALYST_AGENT_EVAL_PROFILE_ARN')\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "AWS_ACCOUNT = os.environ['AWS_ACCOUNT']\n",
    "BUSINESSANALYST_AGENT_PROFILE_ARN = os.environ['BUSINESSANALYST_AGENT_PROFILE_ARN']\n",
    "BUSINESSANALYST_AGENT_EVAL_PROFILE_ARN = os.environ['BUSINESSANALYST_AGENT_EVAL_PROFILE_ARN']\n",
    "\n",
    "MODEL_ID =  \"anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore.config\n",
    "config = botocore.config.Config(\n",
    "    connect_timeout=600,  # 10 minutes\n",
    "    read_timeout=600,     # 10 minutes\n",
    "    retries={'max_attempts': 3}\n",
    ")\n",
    "\n",
    "session = boto3.Session(region_name=REGION)\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "bedrock_agent_client = session.client('bedrock-agent', config=config)\n",
    "bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', config=config)\n",
    "bedrock_runtime_client = session.client('bedrock-runtime', config=config)\n",
    "bedrock_client = session.client('bedrock', config=config)\n",
    "lambda_client = session.client('lambda', config=config)\n",
    "iam_resource = session.resource('iam')\n",
    "iam_client = session.client('iam')\n",
    "athena_client = session.client('athena')\n",
    "s3_client = session.client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Business Analyst agent lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../businessanalyst/bedrock_business_analyst_agent.py\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from pydantic import BaseModel\n",
    "import time\n",
    "\n",
    "class Parameters(BaseModel):\n",
    "    AthenaDatabase: Optional[str] = None\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# get the environment variables\n",
    "if 'S3_BUCKET_NAME' not in globals():\n",
    "    S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
    "    logger.info(f\"S3_BUCKET_NAME: {S3_BUCKET_NAME}\")\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    MODEL_ID = os.getenv('MODEL_ID')\n",
    "    logger.info(f\"MODEL_ID: {MODEL_ID}\")\n",
    "\n",
    "if 'ATHENA_QUERY_EXECUTION_LOCATION' not in globals():\n",
    "    ATHENA_QUERY_EXECUTION_LOCATION = f's3://{S3_BUCKET_NAME}/athena_results/'\n",
    "    logger.info(f\"ATHENA_QUERY_EXECUTION_LOCATION: {ATHENA_QUERY_EXECUTION_LOCATION}\")\n",
    "\n",
    "# check if session python variable exists\n",
    "if 'SESSION_PROFILE' in globals():\n",
    "    logger.info(f\"Session profile found: {SESSION_PROFILE}\")\n",
    "    session = boto3.Session(profile_name=SESSION_PROFILE, region_name=REGION)\n",
    "else:\n",
    "    logger.info('No session profile found, using default session')\n",
    "    session = boto3.Session()\n",
    "\n",
    "s3_client = session.client('s3')\n",
    "athena_client = session.client('athena')\n",
    "\n",
    "\n",
    "\n",
    "logger.info('start athena result location configuration')\n",
    "try:\n",
    "    response = athena_client.get_work_group(WorkGroup='primary')\n",
    "    ConfigurationUpdates={}\n",
    "    ConfigurationUpdates['EnforceWorkGroupConfiguration']= True\n",
    "    ResultConfigurationUpdates= {}\n",
    "    athena_location = \"s3://\"+ S3_BUCKET_NAME +\"/athena_results/\"\n",
    "    ResultConfigurationUpdates['OutputLocation']=athena_location\n",
    "    EngineVersion = response['WorkGroup']['Configuration']['EngineVersion']\n",
    "    ConfigurationUpdates['ResultConfigurationUpdates']=ResultConfigurationUpdates\n",
    "    ConfigurationUpdates['PublishCloudWatchMetricsEnabled']= response['WorkGroup']['Configuration']['PublishCloudWatchMetricsEnabled']\n",
    "    ConfigurationUpdates['EngineVersion']=EngineVersion\n",
    "    ConfigurationUpdates['RequesterPaysEnabled']= response['WorkGroup']['Configuration']['RequesterPaysEnabled']\n",
    "    response2 = athena_client.update_work_group(WorkGroup='primary',ConfigurationUpdates=ConfigurationUpdates,State='ENABLED')\n",
    "    logger.info(f\"athena output location updated to s3://{S3_BUCKET_NAME}/athena_results/\")  \n",
    "except Exception as e:\n",
    "    logger.error(str(e))\n",
    "\n",
    "class BusinessAnalystTools:\n",
    "    \"\"\"Collection of tools for the Business Analyst Agent to use\"\"\"\n",
    "    \n",
    "    def __init__(self, session, s3_bucket_name: str, athena_database: str):\n",
    "        logger.info(f\"Initializing BusinessAnalystTools with bucket: {s3_bucket_name}, database: {athena_database}\")\n",
    "        self.s3_client = session.client('s3')\n",
    "        self.athena_client = session.client('athena')\n",
    "        self.s3_bucket_name = s3_bucket_name\n",
    "        self.athena_database = athena_database\n",
    "        \n",
    "    def get_database_schema(self) -> str:\n",
    "        \"\"\"Retrieve the SQL database schema from S3\"\"\"\n",
    "        schema_prefix = 'metadata/sql_table_definition'\n",
    "        logger.info(f\"Retrieving database schema from s3://{self.s3_bucket_name}/{schema_prefix}\")\n",
    "        \n",
    "        sql_database_schema = []\n",
    "        try:\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket_name, \n",
    "                Prefix=schema_prefix\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                logger.warning(f\"No schema files found in s3://{self.s3_bucket_name}/{schema_prefix}\")\n",
    "                return \"[]\"\n",
    "            \n",
    "            logger.info(f\"Found {len(response['Contents'])} schema files\")\n",
    "            \n",
    "            for item in response['Contents']:\n",
    "                if item['Key'].endswith('/'):\n",
    "                    continue\n",
    "                    \n",
    "                logger.info(f\"Reading schema file: {item['Key']}\")\n",
    "                try:\n",
    "                    content = self.s3_client.get_object(\n",
    "                        Bucket=self.s3_bucket_name, \n",
    "                        Key=item['Key']\n",
    "                    )['Body'].read().decode('utf-8')\n",
    "                    sql_database_schema.append(content)\n",
    "                    logger.debug(f\"Successfully read schema from {item['Key']}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading schema file {item['Key']}: {str(e)}\")\n",
    "            \n",
    "            logger.info(f\"Successfully retrieved {len(sql_database_schema)} schema definitions\")\n",
    "            return json.dumps(sql_database_schema)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in get_database_schema: {str(e)}\", exc_info=True)\n",
    "            return \"[]\"\n",
    "\n",
    "    def get_use_cases(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve available AI/ML use cases from S3\"\"\"\n",
    "        use_cases_path = 'metadata/use_cases/use_case_details.jsonl'\n",
    "        logger.info(f\"Retrieving use cases from s3://{self.s3_bucket_name}/{use_cases_path}\")\n",
    "        \n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile() as tmp:\n",
    "                self.s3_client.download_file(\n",
    "                    self.s3_bucket_name,\n",
    "                    use_cases_path,\n",
    "                    tmp.name\n",
    "                )\n",
    "                df = pd.read_json(tmp.name, lines=True)\n",
    "                use_cases = df.to_dict('records')\n",
    "                logger.info(f\"Successfully retrieved {len(use_cases)} use cases\")\n",
    "                logger.debug(f\"Use cases: {json.dumps(use_cases, indent=2)}\")\n",
    "                return use_cases\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in get_use_cases: {str(e)}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def execute_query(self, query: str) -> Tuple[str, Optional[pd.DataFrame], Optional[str]]:\n",
    "        \"\"\"Execute an Athena query and return results with detailed error info\"\"\"\n",
    "        logger.info(f\"Executing Athena query in database {self.athena_database}\")\n",
    "        logger.debug(f\"Query: {query}\")\n",
    "        \n",
    "        try:\n",
    "            response = self.athena_client.start_query_execution(\n",
    "                QueryString=query,\n",
    "                QueryExecutionContext={\n",
    "                    'Database': self.athena_database,\n",
    "                    'Catalog': 'AwsDataCatalog'\n",
    "                },\n",
    "                ResultConfiguration={\n",
    "                    'OutputLocation': f's3://{self.s3_bucket_name}/athena_results/'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            query_id = response['QueryExecutionId']\n",
    "            logger.info(f\"Query execution started with ID: {query_id}\")\n",
    "            \n",
    "            # Wait for completion\n",
    "            while True:\n",
    "                status = self.athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "                state = status['QueryExecution']['Status']['State']\n",
    "                logger.debug(f\"Query state: {state}\")\n",
    "                \n",
    "                if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Get detailed error information if query failed\n",
    "            error_info = None\n",
    "            if state != 'SUCCEEDED':\n",
    "                status_details = status['QueryExecution']['Status']\n",
    "                error_info = {\n",
    "                    'state': state,\n",
    "                    'reason': status_details.get('StateChangeReason', 'Unknown error'),\n",
    "                    'athena_error': status_details.get('AthenaError', {}),\n",
    "                    'query_id': query_id\n",
    "                }\n",
    "                error_msg = (\n",
    "                    f\"Query failed with state {state}.\\n\"\n",
    "                    f\"Reason: {error_info['reason']}\\n\"\n",
    "                    f\"Query ID: {query_id}\"\n",
    "                )\n",
    "                if 'AthenaError' in status_details:\n",
    "                    error_msg += f\"\\nAthena Error: {status_details['AthenaError']}\"\n",
    "                logger.error(error_msg)\n",
    "                return state, None, error_msg\n",
    "            \n",
    "            # Query succeeded\n",
    "            logger.info(f\"Query {query_id} completed successfully\")\n",
    "            results = self.athena_client.get_query_results(QueryExecutionId=query_id)\n",
    "            df = self._convert_results_to_df(results)\n",
    "            logger.info(f\"Query returned {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return 'SUCCEEDED', df, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error executing query: {str(e)}\"\n",
    "            logger.error(error_msg, exc_info=True)\n",
    "            return 'ERROR', None, error_msg\n",
    "\n",
    "    def save_dataset(self, df: pd.DataFrame, use_case_name: str) -> str:\n",
    "        \"\"\"Save a dataset to S3 and return its location\"\"\"\n",
    "        logger.info(f\"Saving dataset for use case: {use_case_name}\")\n",
    "        logger.debug(f\"Dataset shape: {df.shape}\")\n",
    "        \n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(suffix='.csv') as tmp:\n",
    "                df.to_csv(tmp.name, index=False)\n",
    "                s3_path = f'ml_datasets/{use_case_name}_dataset.csv'\n",
    "                \n",
    "                logger.info(f\"Uploading dataset to s3://{self.s3_bucket_name}/{s3_path}\")\n",
    "                self.s3_client.upload_file(\n",
    "                    tmp.name,\n",
    "                    self.s3_bucket_name,\n",
    "                    s3_path\n",
    "                )\n",
    "                \n",
    "                location = f's3://{self.s3_bucket_name}/{s3_path}'\n",
    "                logger.info(f\"Dataset successfully saved to {location}\")\n",
    "                return location\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in save_dataset: {str(e)}\", exc_info=True)\n",
    "            return ''\n",
    "\n",
    "    def _convert_results_to_df(self, query_results: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Convert Athena query results to a pandas DataFrame\"\"\"\n",
    "        try:\n",
    "            columns = [col['Name'] for col in query_results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n",
    "            logger.debug(f\"Converting query results with columns: {columns}\")\n",
    "            \n",
    "            data = []\n",
    "            for row in query_results['ResultSet']['Rows'][1:]:  # Skip header row\n",
    "                data.append([item.get('VarCharValue', '') for item in row['Data']])\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            logger.debug(f\"Converted results to DataFrame with shape: {df.shape}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in _convert_results_to_df: {str(e)}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def execute_and_save_query(self, query: str, use_case_name: str = None) -> Tuple[str, Optional[Dict]]:\n",
    "        \"\"\"Execute query, save full results, and return samples\"\"\"\n",
    "        logger.info(f\"Executing and saving query for {use_case_name if use_case_name else 'analysis'}\")\n",
    "        logger.debug(f\"Query: {query}\")\n",
    "        \n",
    "        try:\n",
    "            status, df, error_msg = self.execute_query(query)\n",
    "            \n",
    "            if status == 'SUCCEEDED' and df is not None:\n",
    "                result_info = {\n",
    "                    \"status\": status,\n",
    "                    \"total_rows\": len(df),\n",
    "                    \"total_columns\": len(df.columns),\n",
    "                    \"columns\": list(df.columns),\n",
    "                    \"sample_data\": df.head(5).to_dict('records')  # Only return 5 sample rows\n",
    "                }\n",
    "                \n",
    "                # Save the full dataset if we have a use case name\n",
    "                if use_case_name and not df.empty:\n",
    "                    dataset_location = self.save_dataset(df, use_case_name)\n",
    "                    result_info[\"dataset_location\"] = dataset_location\n",
    "                    logger.info(f\"Saved full dataset ({len(df)} rows) to {dataset_location}\")\n",
    "                \n",
    "                return status, result_info\n",
    "            else:\n",
    "                return status, {\n",
    "                    \"status\": status,\n",
    "                    \"error\": error_msg\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in execute_and_save_query: {str(e)}\"\n",
    "            logger.error(error_msg, exc_info=True)\n",
    "            return 'ERROR', {\n",
    "                \"status\": 'ERROR',\n",
    "                \"error\": error_msg\n",
    "            }\n",
    "\n",
    "def truncate_response(data: Any, max_size: int = 20000) -> Any:\n",
    "    \"\"\"Truncate response data to stay within size limits\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        serialized = json.dumps(data)\n",
    "        if len(serialized) <= max_size:\n",
    "            return data\n",
    "            \n",
    "        # For dictionary responses, try to preserve structure while reducing content\n",
    "        truncated = data.copy()\n",
    "        if 'data' in truncated and isinstance(truncated['data'], list):\n",
    "            # Calculate approximate size per record\n",
    "            record_count = len(truncated['data'])\n",
    "            if record_count > 0:\n",
    "                avg_record_size = len(json.dumps(truncated['data'])) / record_count\n",
    "                # Calculate how many records we can keep\n",
    "                safe_record_count = int((max_size * 0.8) / avg_record_size)  # 80% of max size\n",
    "                truncated['data'] = truncated['data'][:safe_record_count]\n",
    "                truncated['truncated'] = True\n",
    "                truncated['total_records'] = record_count\n",
    "                truncated['showing_records'] = safe_record_count\n",
    "                return truncated\n",
    "                \n",
    "    elif isinstance(data, list):\n",
    "        serialized = json.dumps(data)\n",
    "        if len(serialized) <= max_size:\n",
    "            return data\n",
    "            \n",
    "        # For list responses, truncate the list\n",
    "        original_length = len(data)\n",
    "        # Calculate approximate size per item\n",
    "        if original_length > 0:\n",
    "            avg_item_size = len(serialized) / original_length\n",
    "            safe_item_count = int((max_size * 0.8) / avg_item_size)  # 80% of max size\n",
    "            return {\n",
    "                'data': data[:safe_item_count],\n",
    "                'truncated': True,\n",
    "                'total_items': original_length,\n",
    "                'showing_items': safe_item_count\n",
    "            }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:\n",
    "    try:\n",
    "        logger.info(f\"Received event: {json.dumps(event)}\")\n",
    "        \n",
    "        # Extract parameters from requestBody if present\n",
    "        parameters_dict = {}\n",
    "        if 'requestBody' in event and 'content' in event['requestBody']:\n",
    "            content = event['requestBody']['content']\n",
    "            if 'application/json' in content and 'properties' in content['application/json']:\n",
    "                for prop in content['application/json']['properties']:\n",
    "                    parameters_dict[prop['name']] = prop['value']\n",
    "        \n",
    "        # Create Parameters object with the extracted values\n",
    "        parameters = Parameters(**parameters_dict)\n",
    "\n",
    "        # Initialize tools\n",
    "        tools = BusinessAnalystTools(\n",
    "            session=session,\n",
    "            s3_bucket_name=S3_BUCKET_NAME,\n",
    "            athena_database=parameters.AthenaDatabase\n",
    "        )\n",
    "        \n",
    "        # Extract APIPath from event\n",
    "        api_path = event.get('apiPath', '').strip('/')\n",
    "        \n",
    "        logger.info(f\"API Path: {api_path}\")\n",
    "        logger.info(f\"Parameters: {parameters}\")\n",
    "\n",
    "        response_data = None\n",
    "\n",
    "        if api_path == 'GetDatabaseSchema':\n",
    "            # Get database schema\n",
    "            schema = tools.get_database_schema()\n",
    "            response_data = json.loads(schema)  # Convert string to JSON array\n",
    "            \n",
    "        elif api_path == 'GetUseCases':\n",
    "            # Get available use cases\n",
    "            use_cases = tools.get_use_cases()\n",
    "            response_data = use_cases\n",
    "            \n",
    "        elif api_path == 'ExecuteQuery':\n",
    "            # Execute Athena query\n",
    "            query = parameters_dict.get('Query')\n",
    "            use_case_name = parameters_dict.get('UseCaseName')  # Optional parameter\n",
    "            \n",
    "            if not query:\n",
    "                raise ValueError(\"Query parameter is required\")\n",
    "                \n",
    "            status, result_info = tools.execute_and_save_query(query, use_case_name)\n",
    "            if status != 'SUCCEEDED':\n",
    "                # Instead of raising ValueError, return the error info directly\n",
    "                logger.info(f\"Query failed with info: {result_info}\")\n",
    "                response_data = result_info\n",
    "            else:\n",
    "                response_data = result_info\n",
    "            \n",
    "        elif api_path == 'SaveDataset':\n",
    "            # This endpoint becomes optional since datasets are saved automatically\n",
    "            # but keep it for explicit saves\n",
    "            use_case_name = parameters_dict.get('UseCaseName')\n",
    "            data = parameters_dict.get('Data')\n",
    "            \n",
    "            if not use_case_name or not data:\n",
    "                raise ValueError(\"UseCaseName and Data parameters are required\")\n",
    "                \n",
    "            df = pd.DataFrame(data)\n",
    "            location = tools.save_dataset(df, use_case_name)\n",
    "            response_data = {\"location\": location}\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown API path: {api_path}\")\n",
    "                \n",
    "        # Check and truncate response size if needed\n",
    "        response_size = sys.getsizeof(json.dumps(response_data))\n",
    "        if response_size > 20000:  # 20KB limit\n",
    "            logger.warning(f\"Response size {response_size} exceeds limit. Truncating content...\")\n",
    "            response_data = truncate_response(response_data)\n",
    "                \n",
    "        response_body = {\n",
    "            'application/json': {\n",
    "                'body': response_data\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Set response code based on status if it exists\n",
    "        response_code = 200\n",
    "        if isinstance(response_data, dict) and response_data.get('status') in ['FAILED', 'CANCELLED', 'ERROR']:\n",
    "            response_code = 400\n",
    "\n",
    "        action_response = {\n",
    "            'actionGroup': event['actionGroup'],\n",
    "            'apiPath': event['apiPath'],\n",
    "            'httpMethod': event['httpMethod'],\n",
    "            'httpStatusCode': response_code,\n",
    "            'responseBody': response_body\n",
    "        }\n",
    "\n",
    "        return {'messageVersion': '1.0', 'response': action_response}\n",
    "            \n",
    "    except ValueError as e:\n",
    "        # Handle bad request errors (400)\n",
    "        logger.error(f\"Validation error: {str(e)}\")\n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event.get('actionGroup'),\n",
    "                \"apiPath\": event.get('apiPath'),\n",
    "                \"httpMethod\": event.get('httpMethod', 'POST'),\n",
    "                \"httpStatusCode\": 400,\n",
    "                \"responseBody\": {\n",
    "                    \"application/json\": {\n",
    "                        \"body\": {\n",
    "                            \"error\": str(e)\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Handle internal server errors (500)\n",
    "        logger.error(f\"Internal error: {str(e)}\", exc_info=True)\n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event.get('actionGroup'),\n",
    "                \"apiPath\": event.get('apiPath'),\n",
    "                \"httpMethod\": event.get('httpMethod', 'POST'),\n",
    "                \"httpStatusCode\": 500,\n",
    "                \"responseBody\": {\n",
    "                    \"application/json\": {\n",
    "                        \"body\": {\n",
    "                            \"error\": f\"Internal server error: {str(e)}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../businessanalyst/businessanalyst.Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.11\n",
    "\n",
    "# Install build dependencies first\n",
    "RUN yum install libgomp git gcc gcc-c++ make -y \\\n",
    " && yum clean all -y && rm -rf /var/cache/yum\n",
    "\n",
    "\n",
    "RUN python3 -m pip --no-cache-dir install --upgrade --trusted-host pypi.org --trusted-host files.pythonhosted.org pip \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade wheel setuptools \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pandas \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade boto3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade opensearch-py \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade Pillow \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pyarrow \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade fastparquet \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade urllib3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pydantic\n",
    "\n",
    "# Copy function code\n",
    "WORKDIR /var/task\n",
    "COPY ../businessanalyst/bedrock_business_analyst_agent.py .\n",
    "COPY ../notebooks/utils/ utils/ \n",
    "\n",
    "# Set handler environment variable\n",
    "ENV _HANDLER=\"bedrock_business_analyst_agent.lambda_handler\"\n",
    "\n",
    "# Let's go back to using the default entrypoint\n",
    "ENTRYPOINT [ \"/lambda-entrypoint.sh\" ]\n",
    "CMD [ \"bedrock_business_analyst_agent.lambda_handler\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run local docker container to test the businessanalyst-lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run local docker container\n",
    "!docker build -t businessanalyst-lambda -f ../businessanalyst/businessanalyst.Dockerfile .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run with tailing log\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "!docker run -d \\\n",
    "-e AWS_ACCESS_KEY_ID={credentials.access_key} \\\n",
    "-e AWS_SECRET_ACCESS_KEY={credentials.secret_key} \\\n",
    "-e AWS_SESSION_TOKEN={credentials.token} \\\n",
    "-e AWS_DEFAULT_REGION={REGION} \\\n",
    "-e REGION={REGION} \\\n",
    "-e AWS_LAMBDA_FUNCTION_TIMEOUT=900 \\\n",
    "-e S3_BUCKET_NAME={S3_BUCKET_NAME} \\\n",
    "-e MODEL_ID={MODEL_ID} \\\n",
    "-p 9000:8080 businessanalyst-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --filter ancestor=businessanalyst-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect SQL database schema\n",
    "request_body = {\n",
    "    \"apiPath\": \"/GetDatabaseSchema\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"BusinessAnalystActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get use case catalog\n",
    "request_body = {\n",
    "    \"apiPath\": \"/GetUseCases\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"BusinessAnalystActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the container\n",
    "!docker stop $(docker ps -q --filter ancestor=businessanalyst-lambda)\n",
    "!docker ps --filter ancestor=businessanalyst-lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload docker image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ECR repository for dataengineer-lambda (if not already created in 1_environmentSetup.ipynb)\n",
    "#!aws ecr create-repository --repository-name automatedinsights/lambda_businessanalyst --region {REGION} --profile {SESSION_PROFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload docker image to ECR\n",
    "!aws ecr get-login-password --region {REGION} --profile {SESSION_PROFILE} | docker login --username AWS --password-stdin {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com\n",
    "!docker tag businessanalyst-lambda:latest {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_businessanalyst:latest\n",
    "!docker push {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_businessanalyst:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Test Bedrock Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "from utils.bedrock_agent import BedrockAgentScenarioWrapper\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "agent_name = \"BusinessAnalyst\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "prompt = f\"Review the data and identify AI/ML use cases that can be performed on the data in the Athena database name: {S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "\n",
    "\n",
    "instruction = \"\"\"# Optimized AI Agent Instructions\n",
    "\n",
    "## Role and Purpose\n",
    "You are an expert business analyst specializing in AI/ML solution development. Your task is to identify high-value machine learning opportunities from available data sources.\n",
    "\n",
    "## Core Capabilities\n",
    "- Analyze SQL database schemas\n",
    "- Execute and refine SQL queries against Athena databases\n",
    "- Evaluate potential ML use cases for feasibility with available data\n",
    "- Focus specifically on classification and regression problem types\n",
    "\n",
    "## Primary Objective\n",
    "Identify and thoroughly document ONE high-value machine learning use case that can be implemented with the available data.\n",
    "\n",
    "## Analysis Workflow\n",
    "1. Review the database schema to understand available tables and relationships\n",
    "2. Explore data characteristics through sample queries\n",
    "3. Identify a promising classification or regression use case\n",
    "4. Develop and test an Athena SQL query that produces the required dataset\n",
    "5. Document the complete use case specification\n",
    "\n",
    "## Required Deliverable Components\n",
    "For your identified ML use case, provide:\n",
    "\n",
    "1. **Use Case Name**: Clear, descriptive title\n",
    "2. **Description**: Concise explanation of the ML problem and approach\n",
    "3. **Business Justification**: Specific business value and expected outcomes\n",
    "4. **Target Column Specification**: Precise definition of what you're predicting\n",
    "5. **ML Dataset Location**: Exact storage location of the prepared dataset\n",
    "6. **Validated Athena SQL Query**: Working query that generates the complete dataset with target column\n",
    "\n",
    "## Query Development Protocol\n",
    "- Ensure all SQL follows Amazon Athena syntax standards\n",
    "- Test your SQL query in Athena before finalizing\n",
    "- If errors occur, analyze the error message and revise the query\n",
    "- Maximum 3 attempts to fix any query issues\n",
    "- If query cannot be resolved after 3 attempts, select an alternative use case\n",
    "\n",
    "## Final Verification\n",
    "Before submitting your final response, verify that:\n",
    "- The SQL query executes successfully in Athena\n",
    "- All required components are included in your documentation\n",
    "- The ML dataset location is explicitly specified\"\"\"\n",
    "\n",
    "postfix = \"\".join(\n",
    "    random.choice(string.ascii_lowercase + \"0123456789\") for _ in range(8)\n",
    ")\n",
    "\n",
    "agent_name = agent_name + \"_\" + postfix\n",
    "\n",
    "IMAGE_URI = f'{AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_businessanalyst:latest'\n",
    "\n",
    "agentCollaboration = 'DISABLED' #'SUPERVISOR' #|'SUPERVISOR_ROUTER'|'DISABLED'\n",
    "\n",
    "sub_agents_list = []\n",
    "promptOverrideConfiguration = None\n",
    "\n",
    "\n",
    "lambda_environment_variables = {\n",
    "    \"S3_BUCKET_NAME\": S3_BUCKET_NAME,\n",
    "    \"MODEL_ID\": MODEL_ID\n",
    "}\n",
    "\n",
    "scenario = BedrockAgentScenarioWrapper(\n",
    "    bedrock_agent_client=bedrock_agent_client,\n",
    "    runtime_client=bedrock_agent_runtime_client,\n",
    "    lambda_client=lambda_client,\n",
    "    iam_resource=iam_resource,\n",
    "    postfix=postfix,\n",
    "    agent_name=agent_name,\n",
    "    model_id=MODEL_ID,\n",
    "    sub_agents_list=sub_agents_list,\n",
    "    prompt=prompt,\n",
    "    lambda_image_uri=IMAGE_URI,\n",
    "    lambda_environment_variables=lambda_environment_variables,\n",
    "    action_group_schema_path=\"action_groups/businessanalyst_open_api_schema.yml\",\n",
    "    instruction=instruction,\n",
    "    agentCollaboration=agentCollaboration,\n",
    "    promptOverrideConfiguration=promptOverrideConfiguration\n",
    ")\n",
    "try:\n",
    "    scenario.run_scenario()\n",
    "except Exception as e:\n",
    "    logging.exception(f\"Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"Review the data and identify AI/ML use cases that can be performed on the data in the Athena database name: {S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "\n",
    "# scenario.prompt = prompt\n",
    "# print(scenario.prompt)\n",
    "\n",
    "# scenario.chat_with_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =scenario.agent\n",
    "\n",
    "AGENT_ID = agent.get('agentId')\n",
    "\n",
    "# get agent alias id\n",
    "agent_aliases = bedrock_agent_client.list_agent_aliases(agentId= AGENT_ID)\n",
    "\n",
    "AGENT_ALIAS_ID =  agent_aliases.get('agentAliasSummaries')[0].get('agentAliasId')\n",
    "print(f\"AGENT_ID: {AGENT_ID}\")\n",
    "print(f\"AGENT_ALIAS_ID: {AGENT_ALIAS_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save agent config to json file for evaluation\n",
    "agent_config = {\n",
    "    \"agent_id\": AGENT_ID,\n",
    "    \"agent_alias_id\": AGENT_ALIAS_ID,\n",
    "    \"human_id\": \"User\",\n",
    "    \"agent_name\": \"BusinessAnalyst\",\n",
    "    \"agent_instruction\": instruction,\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"tool_name\": \"BusinessAnalystAPI\",\n",
    "            \"name\": \"BusinessAnalystAPI\",\n",
    "            \"description\": \"Business Analyst API for database analysis and ML dataset preparation\",\n",
    "            \"actions\": [\n",
    "                {\n",
    "                    \"name\": \"GetDatabaseSchema\",\n",
    "                    \"description\": \"Retrieve the SQL database schema from S3\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"AthenaDatabase\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"The Athena database name\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"AthenaDatabase\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"table_name\": {\n",
    "                                    \"data_type\": \"string\"\n",
    "                                },\n",
    "                                \"columns\": {\n",
    "                                    \"data_type\": \"array\",\n",
    "                                    \"items\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"name\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"type\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"GetUseCases\",\n",
    "                    \"description\": \"Retrieve available AI/ML use cases from S3\",\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"description\": \"Name of the use case\"\n",
    "                                },\n",
    "                                \"description\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"description\": \"Detailed description of the use case\"\n",
    "                                },\n",
    "                                \"required_columns\": {\n",
    "                                    \"data_type\": \"array\",\n",
    "                                    \"items\": {\n",
    "                                        \"data_type\": \"string\"\n",
    "                                    },\n",
    "                                    \"description\": \"Required columns for this use case\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"ExecuteQuery\",\n",
    "                    \"description\": \"Execute an Athena query, save results, and return samples\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"AthenaDatabase\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"The Athena database name\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"Query\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"The SQL query to execute\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"UseCaseName\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"Optional use case name to save full results\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"AthenaDatabase\",\n",
    "                            \"Query\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"status\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"enum\": [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\", \"ERROR\"]\n",
    "                            },\n",
    "                            \"total_rows\": {\n",
    "                                \"data_type\": \"integer\",\n",
    "                                \"description\": \"Total number of rows in the full result\"\n",
    "                            },\n",
    "                            \"total_columns\": {\n",
    "                                \"data_type\": \"integer\",\n",
    "                                \"description\": \"Total number of columns\"\n",
    "                            },\n",
    "                            \"columns\": {\n",
    "                                \"data_type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"data_type\": \"string\"\n",
    "                                },\n",
    "                                \"description\": \"List of column names\"\n",
    "                            },\n",
    "                            \"sample_data\": {\n",
    "                                \"data_type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"data_type\": \"object\",\n",
    "                                    \"additionalProperties\": True\n",
    "                                },\n",
    "                                \"description\": \"Sample rows from the query results\"\n",
    "                            },\n",
    "                            \"dataset_location\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"S3 location of saved dataset (if use case provided)\"\n",
    "                            },\n",
    "                            \"error\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"Detailed error message if query failed\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"SaveDataset\",\n",
    "                    \"description\": \"Save a dataset to S3\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"UseCaseName\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"Name of the use case for the dataset\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"Data\": {\n",
    "                                \"data_type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"data_type\": \"object\",\n",
    "                                    \"additionalProperties\": True\n",
    "                                },\n",
    "                                \"description\": \"Dataset to save as array of records\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"UseCaseName\",\n",
    "                            \"Data\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"S3 location where the dataset was saved\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                }\n",
    "            ],\n",
    "            \"tool_type\": \"Module\",\n",
    "            \"meta\": {}\n",
    "        }\n",
    "    ],\n",
    "    \"reachable_agents\": []\n",
    "}\n",
    "# save agent config to json file\n",
    "with open('../businessanalyst/agent.json', 'w') as f:\n",
    "    json.dump(agent_config, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define different evaluation scenarios\n",
    "evaluation_scenarios = {\n",
    "    \"scenarios\": [\n",
    "        {\n",
    "            \"scenario\": \"DetectUseCases\",\n",
    "            \"input_problem\": f\"Detect AI/ML use cases that can be performed on the data. Include the reasoning and the S3 location of the prepared dataset in the final response. Athena database name: {S3_BUCKET_NAME.replace('-', '_')}\",   \n",
    "            \"assertions\": [\n",
    "                \"agent: GetDatabaseSchema is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                \"agent: GetUseCases is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                \"agent: ExecuteQuery is executed to create a ML dataset\",\n",
    "                \"agent: SaveDataset is executed to save the ML dataset to S3\",\n",
    "                \"agent: The AI/ML use cases along with the respective S3 location of the ML dataset(s) and target column name are returned in the final response\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "# save evaluation scenarios to json file\n",
    "with open('../businessanalyst/scenarios.json', 'w') as f:\n",
    "    json.dump(evaluation_scenarios, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent evaluation\n",
    "from utils.benchmark import run_agent_evaluation\n",
    "\n",
    "dataset_dir = \"../businessanalyst\"\n",
    "results = run_agent_evaluation(\n",
    "    scenario_filepath = f\"{dataset_dir}/scenarios.json\",\n",
    "    agent_filepath = f\"{dataset_dir}/agent.json\",\n",
    "    llm_judge_id = BUSINESSANALYST_AGENT_EVAL_PROFILE_ARN,\n",
    "    region = REGION,\n",
    "    session = session\n",
    ")\n",
    "\n",
    "# Check if results is not None before proceeding\n",
    "if results is not None:\n",
    "    # Create high-level metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'user_gsr': [results['user_gsr']],\n",
    "        'system_gsr': [results['system_gsr']],\n",
    "        'overall_gsr': [results['overall_gsr']],\n",
    "        'partial_gsr': [results['partial_gsr']],\n",
    "        'scenario_count': [results['scenario_count']],\n",
    "        'conversation_count': [results['conversation_count']]\n",
    "    })\n",
    "\n",
    "    # Create detailed assertions DataFrame\n",
    "    assertions_list = []\n",
    "    for eval_result in results['conversation_evals']:\n",
    "        trajectory_index = eval_result['trajectory_index']\n",
    "        for assertion in eval_result['report']:\n",
    "            assertions_list.append({\n",
    "                'trajectory_index': trajectory_index,\n",
    "                'assertion_type': assertion['assertion_type'],\n",
    "                'assertion': assertion['assertion'],\n",
    "                'answer': assertion['answer'],\n",
    "                'evidence': assertion['evidence']\n",
    "            })\n",
    "\n",
    "    assertions_df = pd.DataFrame(assertions_list)\n",
    "\n",
    "    # Display results\n",
    "    print(\"High-level Metrics:\")\n",
    "    display(metrics_df)\n",
    "\n",
    "    print(\"\\nDetailed Assertions:\")\n",
    "    display(assertions_df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: Please check for errors in the evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We first created a Docker container that contains all of the available tools/functions that the agent can use.\n",
    "- We then created a Bedrock Agent with an Action Group that uses the Docker container in a Lambda function as the execution environment.\n",
    "- We then created a set of evaluation scenarios that cover different aspects of the agent's behavior.\n",
    "- We then ran the agent evaluation and reviewed the results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
