{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Supervisor Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create a supervisor agent with Amazon Bedrock Agents that will be able to orchestrate the execution of the data engineer, business analyst and data scientist agents.\n",
    "\n",
    "And then we will evaluate the multi-agent orchestration in different scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.bedrock import BedrockLLMWrapper\n",
    "import mlflow\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['AWS_ACCOUNT'] = os.getenv('AWS_ACCOUNT')\n",
    "os.environ['MAC_EVAL_PROFILE_ARN'] = os.getenv('MAC_EVAL_PROFILE_ARN')\n",
    "os.environ['MLFLOW_SERVER_NAME'] = os.getenv('MLFLOW_SERVER_NAME')\n",
    "os.environ['BEDROCK_AGENT_ID'] = os.getenv('BEDROCK_AGENT_ID')\n",
    "os.environ['USER_FEEDBACK_TABLE_NAME'] = os.getenv('USER_FEEDBACK_TABLE_NAME')\n",
    "os.environ['TRACE_TABLE_NAME'] = os.getenv('TRACE_TABLE_NAME')\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "AWS_ACCOUNT = os.environ['AWS_ACCOUNT']\n",
    "MAC_EVAL_PROFILE_ARN = os.environ['MAC_EVAL_PROFILE_ARN']\n",
    "MLFLOW_SERVER_NAME = os.environ['MLFLOW_SERVER_NAME']\n",
    "BEDROCK_AGENT_ID = os.environ['BEDROCK_AGENT_ID']\n",
    "USER_FEEDBACK_TABLE_NAME = os.environ['USER_FEEDBACK_TABLE_NAME']\n",
    "TRACE_TABLE_NAME = os.environ['TRACE_TABLE_NAME']\n",
    "\n",
    "# Bedrock Agents does not yet support application inference profiles\n",
    "MODEL_ID =  \"anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore.config\n",
    "config = botocore.config.Config(\n",
    "    connect_timeout=600,  # 10 minutes\n",
    "    read_timeout=600,     # 10 minutes\n",
    "    retries={'max_attempts': 3}\n",
    ")\n",
    "\n",
    "session = boto3.Session(region_name=REGION)\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "\n",
    "bedrock_agent_client = session.client('bedrock-agent', config=config)\n",
    "bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', config=config)\n",
    "bedrock_runtime_client = session.client('bedrock-runtime', config=config)\n",
    "bedrock_client = session.client('bedrock', config=config)\n",
    "lambda_client = session.client('lambda', config=config)\n",
    "iam_resource = session.resource('iam')\n",
    "iam_client = session.client('iam')\n",
    "athena_client = session.client('athena')\n",
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Lambda function that can be used to retrieve user feedback and traces from DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Test Bedrock Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../supervisor/bedrock_supervisor_agent.py\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError, NoCredentialsError, PartialCredentialsError\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from enum import Enum\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, List, TypedDict, Optional\n",
    "from urllib.parse import urlparse\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "from botocore.credentials import Credentials\n",
    "import urllib3\n",
    "from pydantic import BaseModel\n",
    "from utils.bedrock import BedrockLLMWrapper\n",
    "from boto3.dynamodb.conditions import Key\n",
    "import sys\n",
    "\n",
    "# Initialize logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "TRACE_TABLE_NAME = os.getenv('TRACE_TABLE_NAME')\n",
    "USER_FEEDBACK_TABLE_NAME = os.getenv('USER_FEEDBACK_TABLE_NAME')\n",
    "MODEL_ID = os.getenv('MODEL_ID')\n",
    "\n",
    "LESSONS_LEARNED_PROMPT_TEMPLATE = '''Analyze the following information:\n",
    "\n",
    "user feedback:\n",
    "{USER_FEEDBACK}\n",
    "\n",
    "traces from past runs:\n",
    "{PAST_RUNS}\n",
    "\n",
    "1. Review the user feedback and traces from past runs.\n",
    "\n",
    "2. Extract the lessons learned with regards to agent orchestration and function/tool calling based on the past runs and user feedback and return them in a list of strings.\n",
    "\n",
    "Sample output:\n",
    "- when creating a SQL query that requires functions, ensure you have the correct Athena function names, e.g. DATE_DIFF instead of datediff\n",
    "- when calling the data scientist agent, ensure you provide the ml dataset location and target column name, otherwise don't call the data scientist agent\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "class SupervisorTools:\n",
    "    \"\"\"Collection of tools for the Supervisor Agent to use\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        logger.info(f\"Initializing SupervisorTools\")\n",
    "        self.dynamodb = boto3.client('dynamodb')\n",
    "        self.bedrock_llm = BedrockLLMWrapper(model_id=MODEL_ID, \n",
    "                            max_token_count=2000,\n",
    "                            temperature=0\n",
    "                        )\n",
    "    \n",
    "    def get_user_feedback(self, user_id: str, conversation_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve user feedback from DynamoDB\"\"\"\n",
    "        try:\n",
    "            response = self.dynamodb.get_item(\n",
    "                TableName=USER_FEEDBACK_TABLE_NAME,\n",
    "                Key={'user_id': {'S': user_id}, 'conversation_id': {'S': conversation_id}}\n",
    "            )\n",
    "            return response.get('Item', {}).get('feedback', [])\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error retrieving user feedback: {e}\")\n",
    "            return []\n",
    "    \n",
    "    \n",
    "    def get_all_user_feedback(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve all user feedback from DynamoDB\"\"\"\n",
    "        try:\n",
    "            response = self.dynamodb.scan(TableName=USER_FEEDBACK_TABLE_NAME)\n",
    "            return response.get('Items', [])\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error retrieving all user feedback: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_traces_for_user(self, user_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve all traces for a user from DynamoDB\"\"\"\n",
    "        try:\n",
    "            response = self.dynamodb.scan(TableName=TRACE_TABLE_NAME,\n",
    "                                         FilterExpression=Key('user_id').eq(user_id))\n",
    "            return response.get('Items', [])\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error retrieving all traces for user {user_id}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_all_conversation_traces(self, conversation_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve all traces for a conversation from DynamoDB\"\"\"\n",
    "        try:\n",
    "            response = self.dynamodb.scan(TableName=TRACE_TABLE_NAME,\n",
    "                                         FilterExpression=Key('conversation_id').eq(conversation_id))\n",
    "            return response.get('Items', [])\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error retrieving all traces for conversation {conversation_id}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_all_traces(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve all traces from DynamoDB\"\"\"\n",
    "        try:\n",
    "            response = self.dynamodb.scan(TableName=TRACE_TABLE_NAME)\n",
    "            return response.get('Items', [])\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error retrieving all traces: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_lessons_learned_from_past_runs(self):\n",
    "        \"\"\"Get lessons learned from past runs and user feedback\"\"\"\n",
    "        try:\n",
    "            user_feedback = self.get_all_user_feedback()\n",
    "            past_runs = self.get_all_traces()\n",
    "            # convert to a string\n",
    "            past_runs_string = '\\n'.join([str(run) for run in past_runs])\n",
    "            user_feedback_string = '\\n'.join([str(feedback) for feedback in user_feedback])\n",
    "\n",
    "            prompt = LESSONS_LEARNED_PROMPT_TEMPLATE.format(USER_FEEDBACK=user_feedback_string, PAST_RUNS=past_runs_string)\n",
    "            response = self.bedrock_llm.generate(prompt)\n",
    "            return response[0]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting lessons learned from past runs: {e}\", exc_info=True)\n",
    "            return f'Error encountered while getting lessons learned from past runs and user feedback: {e}'\n",
    "\n",
    "\n",
    "\n",
    "def truncate_response(data: Any, max_size: int = 20000) -> Any:\n",
    "    \"\"\"Truncate response data to stay within size limits\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        serialized = json.dumps(data)\n",
    "        if len(serialized) <= max_size:\n",
    "            return data\n",
    "            \n",
    "        # For dictionary responses, try to preserve structure while reducing content\n",
    "        truncated = data.copy()\n",
    "        if 'data' in truncated and isinstance(truncated['data'], list):\n",
    "            # Calculate approximate size per record\n",
    "            record_count = len(truncated['data'])\n",
    "            if record_count > 0:\n",
    "                avg_record_size = len(json.dumps(truncated['data'])) / record_count\n",
    "                # Calculate how many records we can keep\n",
    "                safe_record_count = int((max_size * 0.8) / avg_record_size)  # 80% of max size\n",
    "                truncated['data'] = truncated['data'][:safe_record_count]\n",
    "                truncated['truncated'] = True\n",
    "                truncated['total_records'] = record_count\n",
    "                truncated['showing_records'] = safe_record_count\n",
    "                return truncated\n",
    "                \n",
    "    elif isinstance(data, list):\n",
    "        serialized = json.dumps(data)\n",
    "        if len(serialized) <= max_size:\n",
    "            return data\n",
    "            \n",
    "        # For list responses, truncate the list\n",
    "        original_length = len(data)\n",
    "        # Calculate approximate size per item\n",
    "        if original_length > 0:\n",
    "            avg_item_size = len(serialized) / original_length\n",
    "            safe_item_count = int((max_size * 0.8) / avg_item_size)  # 80% of max size\n",
    "            return {\n",
    "                'data': data[:safe_item_count],\n",
    "                'truncated': True,\n",
    "                'total_items': original_length,\n",
    "                'showing_items': safe_item_count\n",
    "            }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:\n",
    "    try:\n",
    "        logger.info(f\"Received event: {json.dumps(event)}\")\n",
    "        \n",
    "        # # Extract parameters from requestBody if present\n",
    "        # parameters_dict = {}\n",
    "        # if 'requestBody' in event and 'content' in event['requestBody']:\n",
    "        #     content = event['requestBody']['content']\n",
    "        #     if 'application/json' in content and 'properties' in content['application/json']:\n",
    "        #         for prop in content['application/json']['properties']:\n",
    "        #             parameters_dict[prop['name']] = prop['value']\n",
    "        \n",
    "\n",
    "        # Initialize tools\n",
    "        tools = SupervisorTools(\n",
    "        )\n",
    "        \n",
    "        # Extract APIPath from event\n",
    "        api_path = event.get('apiPath', '').strip('/')\n",
    "        \n",
    "        logger.info(f\"API Path: {api_path}\")\n",
    "\n",
    "        response_data = None\n",
    "        \n",
    "        if api_path == 'GetLessonsLearnedFromPastRuns':\n",
    "            lessons_learned = tools.get_lessons_learned_from_past_runs()\n",
    "            response_data = lessons_learned\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid API path: {api_path}\")\n",
    "        \n",
    "        # Check and truncate response size if needed\n",
    "        response_size = sys.getsizeof(json.dumps(response_data))\n",
    "        if response_size > 20000:  # 20KB limit\n",
    "            logger.warning(f\"Response size {response_size} exceeds limit. Truncating content...\")\n",
    "            response_data = truncate_response(response_data)\n",
    "                \n",
    "        response_body = {\n",
    "            'application/json': {\n",
    "                'body': response_data\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Set response code based on status if it exists\n",
    "        response_code = 200\n",
    "        if isinstance(response_data, dict) and response_data.get('status') in ['FAILED', 'CANCELLED', 'ERROR']:\n",
    "            response_code = 400\n",
    "\n",
    "        action_response = {\n",
    "            'actionGroup': event['actionGroup'],\n",
    "            'apiPath': event['apiPath'],\n",
    "            'httpMethod': event['httpMethod'],\n",
    "            'httpStatusCode': response_code,\n",
    "            'responseBody': response_body\n",
    "        }\n",
    "\n",
    "        return {'messageVersion': '1.0', 'response': action_response}\n",
    "            \n",
    "    except ValueError as e:\n",
    "        # Handle bad request errors (400)\n",
    "        logger.error(f\"Validation error: {str(e)}\")\n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event.get('actionGroup'),\n",
    "                \"apiPath\": event.get('apiPath'),\n",
    "                \"httpMethod\": event.get('httpMethod', 'POST'),\n",
    "                \"httpStatusCode\": 400,\n",
    "                \"responseBody\": {\n",
    "                    \"application/json\": {\n",
    "                        \"body\": {\n",
    "                            \"error\": str(e)\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Handle internal server errors (500)\n",
    "        logger.error(f\"Internal error: {str(e)}\", exc_info=True)\n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event.get('actionGroup'),\n",
    "                \"apiPath\": event.get('apiPath'),\n",
    "                \"httpMethod\": event.get('httpMethod', 'POST'),\n",
    "                \"httpStatusCode\": 500,\n",
    "                \"responseBody\": {\n",
    "                    \"application/json\": {\n",
    "                        \"body\": {\n",
    "                            \"error\": f\"Internal server error: {str(e)}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../supervisor/supervisor.Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.11\n",
    "\n",
    "# Install build dependencies first\n",
    "RUN yum install libgomp git gcc gcc-c++ make -y \\\n",
    " && yum clean all -y && rm -rf /var/cache/yum\n",
    "\n",
    "\n",
    "RUN python3 -m pip --no-cache-dir install --upgrade --trusted-host pypi.org --trusted-host files.pythonhosted.org pip \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade wheel setuptools \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pandas \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade boto3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade opensearch-py \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade Pillow \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pyarrow \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade fastparquet \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade urllib3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pydantic\n",
    "\n",
    "# Copy function code\n",
    "WORKDIR /var/task\n",
    "COPY ../supervisor/bedrock_supervisor_agent.py .\n",
    "COPY ../notebooks/utils/ utils/ \n",
    "\n",
    "# Set handler environment variable\n",
    "ENV _HANDLER=\"bedrock_supervisor_agent.lambda_handler\"\n",
    "\n",
    "# Let's go back to using the default entrypoint\n",
    "ENTRYPOINT [ \"/lambda-entrypoint.sh\" ]\n",
    "CMD [ \"bedrock_supervisor_agent.lambda_handler\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run local docker container\n",
    "!docker build -t supervisor-lambda -f ../supervisor/supervisor.Dockerfile .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run with tailing log\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "!docker run -d \\\n",
    "-e AWS_ACCESS_KEY_ID={credentials.access_key} \\\n",
    "-e AWS_SECRET_ACCESS_KEY={credentials.secret_key} \\\n",
    "-e AWS_SESSION_TOKEN={credentials.token} \\\n",
    "-e AWS_DEFAULT_REGION={REGION} \\\n",
    "-e REGION={REGION} \\\n",
    "-e AWS_LAMBDA_FUNCTION_TIMEOUT=900 \\\n",
    "-e MODEL_ID={MODEL_ID} \\\n",
    "-e USER_FEEDBACK_TABLE_NAME={USER_FEEDBACK_TABLE_NAME} \\\n",
    "-e TRACE_TABLE_NAME={TRACE_TABLE_NAME} \\\n",
    "-p 9000:8080 supervisor-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --filter ancestor=supervisor-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user feedback\n",
    "request_body = {\n",
    "    \"apiPath\": \"/GetLessonsLearnedFromPastRuns\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"user_id\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"XXX\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"BusinessAnalystActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the container\n",
    "!docker stop $(docker ps -q --filter ancestor=supervisor-lambda)\n",
    "!docker ps --filter ancestor=supervisor-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ECR repository for dataengineer-lambda (if not already created in 1_environmentSetup.ipynb)\n",
    "#!aws ecr create-repository --repository-name automatedinsights/lambda_supervisor --region {REGION} --profile {SESSION_PROFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload docker image to ECR\n",
    "!aws ecr get-login-password --region {REGION} --profile {SESSION_PROFILE} | docker login --username AWS --password-stdin {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com\n",
    "!docker tag supervisor-lambda:latest {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_supervisor:latest\n",
    "!docker push {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_supervisor:latest    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all agents\n",
    "response = bedrock_agent_client.list_agents()\n",
    "for agent in response.get('agentSummaries', []):\n",
    "    if \"DataEngineer\" in agent.get('agentName') or \"BusinessAnalyst\" in agent.get('agentName') or \"DataScientist\" in agent.get('agentName'):\n",
    "        print(\"-\" * 40)\n",
    "        agent_aliases = bedrock_agent_client.list_agent_aliases(agentId=agent.get('agentId'))\n",
    "        for alias in agent_aliases.get('agentAliasSummaries', []):\n",
    "            if alias.get('agentAliasId') not in ['TSTALIASID']: \n",
    "                if \"DataEngineer\" in agent.get('agentName'):\n",
    "                    data_engineer_agent_alias_arn = f\"arn:aws:bedrock:{REGION}:{AWS_ACCOUNT}:agent-alias/{agent.get('agentId')}/{alias.get('agentAliasId')}\"\n",
    "                    print(f\"data_engineer_agent_alias_arn: {data_engineer_agent_alias_arn}\")\n",
    "                elif \"BusinessAnalyst\" in agent.get('agentName'):\n",
    "                    business_analyst_agent_alias_arn = f\"arn:aws:bedrock:{REGION}:{AWS_ACCOUNT}:agent-alias/{agent.get('agentId')}/{alias.get('agentAliasId')}\"\n",
    "                    print(f\"business_analyst_agent_alias_arn: {business_analyst_agent_alias_arn}\")\n",
    "                elif \"DataScientist\" in agent.get('agentName'):\n",
    "                    data_scientist_agent_alias_arn = f\"arn:aws:bedrock:{REGION}:{AWS_ACCOUNT}:agent-alias/{agent.get('agentId')}/{alias.get('agentAliasId')}\"\n",
    "                    print(f\"data_scientist_agent_alias_arn: {data_scientist_agent_alias_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "from utils.bedrock_agent import BedrockAgentScenarioWrapper\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "agent_name = \"AI_ML_team\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "\n",
    "# s3://{S3_BUCKET_NAME}/uploads/CustomerFeedback.csv,\n",
    "# s3://{S3_BUCKET_NAME}/uploads/CustomerPreferences.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/Customers.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/Interactions.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/LoyaltyProgram.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/MarketingCampaigns.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/OrderItems.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/Orders.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/Products.csv, \n",
    "# s3://{S3_BUCKET_NAME}/uploads/SupportTickets.csv\n",
    "\n",
    "prompt = f\"\"\"Inputs:\n",
    "Here is the Athena database name: \"{S3_BUCKET_NAME.replace('-', '_')}\" \n",
    "And here is the list of all datasets that need to be processed: \n",
    "s3://{S3_BUCKET_NAME}/uploads/Orders.csv\n",
    " \n",
    "\n",
    "Return the final response in XML format and nothing else.\"\"\"\n",
    "\n",
    "print(f\"prompt: {prompt}\")\n",
    "\n",
    "instruction = \"\"\"\n",
    "\n",
    "## Core Role Definition\n",
    "You are a strategic AI Team Manager who orchestrates AI/ML use cases from concept to execution, serving as the central coordinator between business needs, data preparation, and technical implementation.\n",
    "\n",
    "## Key Responsibilities\n",
    "\n",
    "### Data Management\n",
    "- Ensure all ML datasets have clearly defined locations and target column names before proceeding with any modeling\n",
    "- Verify that complete dataset information (not just SQL queries) is provided to the DataScientistAgent\n",
    "- Enforce proper storage organization: models in `models/` directory and predictions in `results/` directory\n",
    "- Validate data quality and completeness before initiating any ML processes\n",
    "\n",
    "### Workflow Optimization\n",
    "- Maintain a knowledge base of lessons learned from previous runs and user feedbacks\n",
    "- Implement a structured approach to use case identification and execution\n",
    "- Prioritize use cases based on business value and technical feasibility\n",
    "\n",
    "### Process Execution\n",
    "1. **Data Preparation Phase**:\n",
    "   - If datasets are provided, prepare them in Amazon Athena first\n",
    "   - Review Athena database schema thoroughly before proceeding\n",
    "   - Ensure all SQL follows Amazon Athena syntax standards\n",
    "\n",
    "2. **Use Case Identification Phase**:\n",
    "   - Analyze available data to identify potential ML use cases\n",
    "   - If no datasets are specified or all datasets are processed, identify ML use cases from existing Athena database\n",
    "\n",
    "3. **Model Development Phase**:\n",
    "   - For each identified use case:\n",
    "     - Prepare training dataset with appropriate features\n",
    "     - Train ML model with comprehensive evaluation metrics on training dataset\n",
    "     - Document model accuracy and feature importance\n",
    "     - Generate and store predictions on test dataset\n",
    "     - Capture DataScientist commentary on model performance\n",
    "\n",
    "### Output Requirements\n",
    "Format the final response in XML with the following structure:\n",
    "```xml\n",
    "<FinalResponse>\n",
    "<UseCases>\n",
    "    <UseCase>\n",
    "        <Name>Use Case Name</Name>\n",
    "        <Description>Detailed description</Description>\n",
    "        <ModelDetails>\n",
    "            <TargetColumn>\n",
    "                <Name>target_column_name</Name>\n",
    "                <Definition>Clear definition of what the target represents</Definition>\n",
    "            </TargetColumn>\n",
    "            <TrainingDataLocation>Full path to training data</TrainingDataLocation>\n",
    "            <TestDataLocation>Full path to test data</TestDataLocation>\n",
    "            <ModelLocation>Full path to stored model</ModelLocation>\n",
    "            <MLDatasetLocation>Full path to complete ML dataset</MLDatasetLocation>\n",
    "            <MLDatasetSQLQuery>Complete SQL query used</MLDatasetSQLQuery>\n",
    "            <Accuracy>Numerical accuracy metric</Accuracy>\n",
    "            <FeatureImportances>\n",
    "                <!-- Detailed feature importance list -->\n",
    "            </FeatureImportances>\n",
    "            <Sample-PredictedValues>\n",
    "                <!-- Representative sample of predictions -->\n",
    "            </Sample-PredictedValues>\n",
    "        </ModelDetails>\n",
    "        <PredictionDataLocation>Full path to predictions file</PredictionDataLocation>\n",
    "        <DataScientistCommentary>Expert analysis of model performance and limitations</DataScientistCommentary>\n",
    "    </UseCase>\n",
    "    <!-- Additional use cases as needed -->\n",
    "</UseCases>\n",
    "</FinalResponse>\n",
    "```\n",
    "\n",
    "If no viable use cases are identified, return: `<UseCases>None</UseCases>`\n",
    "\n",
    "Always return the complete XML structure with all required elements and nothing else.\"\"\"\n",
    "\n",
    "postfix = \"\".join(\n",
    "    random.choice(string.ascii_lowercase + \"0123456789\") for _ in range(8)\n",
    ")\n",
    "\n",
    "agent_name = agent_name + \"_\" + postfix\n",
    "\n",
    "IMAGE_URI = f'{AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_supervisor:latest'\n",
    "\n",
    "\n",
    "promptOverrideConfiguration = None\n",
    "\n",
    "agentCollaboration = 'SUPERVISOR' #|'SUPERVISOR_ROUTER'|'DISABLED'\n",
    "# The ARN should be in format: arn:aws:bedrock:{region}:{account}:agent-alias/{agent-id}/{alias-id}\n",
    "\n",
    "sub_agents_list = [\n",
    "    {\n",
    "        'sub_agent_alias_arn': data_engineer_agent_alias_arn,\n",
    "        'sub_agent_instruction': \"\"\"You can invoke the DataEngineerAgent agent when you have been given new datasets that have been staged in Amazon S3 and need to be made available in Amazon Athena as a table so that they can be queried by the business analyst agent. The DataEngineerAgent agent generates semantic type descriptions for each column in the table/file, creates a SQL table definition, and then creates a respective Amazon Athena table, which can be queried.\"\"\",\n",
    "        'sub_agent_association_name': 'DataEngineerAgent',\n",
    "        'relay_conversation_history': 'TO_COLLABORATOR'\n",
    "    },\n",
    "    {\n",
    "        'sub_agent_alias_arn': business_analyst_agent_alias_arn,\n",
    "        'sub_agent_instruction': \"\"\"You can invoke the BusinessAnalystAgent agent to review the data in the Amazon Athena database in order to identify AI/ML use cases that can be performed on the data. It returns any identified ML use cases with details such as the ML use case description, use case business value,  and the ML training dataset location and target column name which can be used by the DataScientistAgent agent.\"\"\",\n",
    "        'sub_agent_association_name': 'BusinessAnalystAgent',\n",
    "        'relay_conversation_history': 'TO_COLLABORATOR'\n",
    "    },\n",
    "    {\n",
    "        'sub_agent_alias_arn': data_scientist_agent_alias_arn,\n",
    "        'sub_agent_instruction': \"\"\"If you have a ML dataset and target column, then you can invoke the DataScientistAgent agent for machine learning tasks such as preparing the data to train a model, training an ML model or using a trained model to generate predictions for a given dataset and target column. The train method of the agent takes in a DataLocation from the BusinessAnalystAgent agent, and returns the trained ML model location, along with details on its accuracy and feature importance. The predict method takes a dataset and target, and generates predictions for it.\"\"\",\n",
    "        'sub_agent_association_name': 'DataScientistAgent',\n",
    "        'relay_conversation_history': 'TO_COLLABORATOR'\n",
    "    }\n",
    "]\n",
    "\n",
    "lambda_environment_variables = {\n",
    "    \"S3_BUCKET_NAME\": S3_BUCKET_NAME,\n",
    "    \"MODEL_ID\": MODEL_ID,\n",
    "    \"USER_FEEDBACK_TABLE_NAME\": USER_FEEDBACK_TABLE_NAME,\n",
    "    \"TRACE_TABLE_NAME\": TRACE_TABLE_NAME\n",
    "}\n",
    "\n",
    "scenario = BedrockAgentScenarioWrapper(\n",
    "    bedrock_agent_client=bedrock_agent_client,\n",
    "    runtime_client=bedrock_agent_runtime_client,\n",
    "    lambda_client=lambda_client,\n",
    "    iam_resource=iam_resource,\n",
    "    postfix=postfix,\n",
    "    agent_name=agent_name,\n",
    "    model_id=MODEL_ID,\n",
    "    sub_agents_list=sub_agents_list,\n",
    "    prompt=prompt,\n",
    "    action_group_schema_path=\"action_groups/supervisor_open_api_schema.yml\",\n",
    "    lambda_image_uri=IMAGE_URI,\n",
    "    lambda_environment_variables=lambda_environment_variables,\n",
    "    instruction=instruction,\n",
    "    agentCollaboration=agentCollaboration,\n",
    "    promptOverrideConfiguration=promptOverrideConfiguration\n",
    ")\n",
    "try:\n",
    "    response, trace_events = scenario.run_scenario()\n",
    "except Exception as e:\n",
    "    logging.exception(f\"Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"Inputs:\n",
    "# Here is the Athena database name: \"{S3_BUCKET_NAME.replace('-', '_')}\" \n",
    "# And here is the list of all datasets that need to be processed: \n",
    "# s3://{S3_BUCKET_NAME}/uploads/Orders.csv\n",
    "\n",
    "# Return the final response in XML format and nothing else.\"\"\"\n",
    "\n",
    "# scenario.prompt = prompt\n",
    "# print(scenario.prompt)\n",
    "# scenario.chat_with_agent()\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Orchestration Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "1) [Paper from Raphael Shu and Nilaksh Das and Michelle Yuan and Monica Sunkara and Yi Zhang: Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications](https://arxiv.org/abs/2412.05449)\n",
    "\n",
    "2) [AWS blog: Unlocking complex problem-solving with multi-agent collaboration on Amazon Bedrock](https://aws.amazon.com/blogs/machine-learning/unlocking-complex-problem-solving-with-multi-agent-collaboration-on-amazon-bedrock/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Definition Implementation\n",
    "\n",
    "#### Success Metrics\n",
    "\n",
    "- Overall GSR: Overall goal success rate covering both the user-side and the system-side. Implementation:  Use LLM to judge user-side and system-side assertions. For a conversation, score is 1 if all assertions are True; else 0.\n",
    "\n",
    "- Supervisor GSR: Goal success rate of the supervisor agent without any dependence on sub-agent and tool behavior. Implementation: If overall GSR is 1 or supervisor agent is reliable, then score for the conversation is 1; else 0.\n",
    "\n",
    "- User-side GSR: Goal success rate in the perspective of the user. Implementation: Use LLM to judge user-side assertions. For a conversation, score is 1 if all user-side assertions are True; else 0.\n",
    "\n",
    "- System-side GSR: Goal success rate in the perspective of the system developers. Implementation: Use LLM to judge system-side assertions. For a conversation, score is 1 if all system-side assertions are True; else 0.\n",
    "\n",
    "\n",
    "#### Latency Metrics (TBD as Bedrock Agents does not return timestamps/duration in traces yet)\n",
    "\n",
    "- Avg. communication overhead per turn: Average number of seconds that the supervisor agent spends communicating with other agents before getting back to the user. This time does not take into account the duration of agents other than the supervisor agent.       \n",
    "\n",
    "- Avg. latency per communication: Average number of seconds that the supervisor agent spends to deliver each message to communicate with other agents.\n",
    "\n",
    "- Avg. user-perceived turn latency per session: Average number of seconds it takes for the supervisor agent to get back to the user. This time does take into account the duration of all agents in the system.\n",
    "\n",
    "#### Operations Metrics\n",
    "\n",
    "- Avg. communications per session: Average count of messages sent by the supervisor agent over the entire session.\n",
    "\n",
    "- Avg. output tokens per communication: Average number of total output tokens from the supervisor agent for each message.\n",
    "\n",
    "#### Cost Metrics\n",
    "\n",
    "- Avg. cost per turn: Average cost of the supervisor agent for each turn. (TBD)\n",
    "\n",
    "- Avg. cost per session: Average cost of the supervisor agent for the entire session.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start mlflow server in seperate terminal \n",
    "\n",
    "(if you are not using SageMaker mlflow instance)\n",
    "\n",
    "mlflow server --host 127.0.0.1 --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use local mlflow server or when using SageMaker mlflow instance, set the tracking uri to the SageMaker mlflow instance\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "def get_presigned_url():\n",
    "    cmd = f\"\"\"aws sagemaker create-presigned-mlflow-tracking-server-url \\\n",
    "      --tracking-server-name {MLFLOW_SERVER_NAME} \\\n",
    "      --session-expiration-duration-in-seconds 1800 \\\n",
    "      --expires-in-seconds 300 \\\n",
    "      --region {REGION} --profile {SESSION_PROFILE}\"\"\"\n",
    "    \n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        response = json.loads(result.stdout)\n",
    "        return response[\"AuthorizedUrl\"]\n",
    "    else:\n",
    "        raise Exception(f\"Failed to get presigned URL: {result.stderr}\")\n",
    "\n",
    "# Get the authorized URL and set as MLflow tracking URI\n",
    "# MLFLOW_TRACKING_URI = get_presigned_url()\n",
    "# mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new experiment for multi-agent-colloboration if it does not exist yet\n",
    "import uuid\n",
    "experiment_description = (\n",
    "        \"multi-agent-colloboration.\"\n",
    "    )\n",
    "\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"multi-agent-colloboration\",\n",
    "    \"use_case\": \"multi-agent-colloboration\",\n",
    "    \"team\": \"aws-ai-ml-analytics\",\n",
    "    \"source\": \"multi-agent-colloboration\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "random_identifier = uuid.uuid4().hex\n",
    "experiment_name = f\"multi-agent-colloboration_{random_identifier}\"\n",
    "\n",
    "# Use search_experiments() to search on the project_name tag key\n",
    "mac_experiments = mlflow.MlflowClient().search_experiments(\n",
    "    filter_string=\"tags.`project_name` = 'multi-agent-colloboration'\"\n",
    ")\n",
    "print(mac_experiments)\n",
    "# check length of mlflow.store.entities.paged_list.PagedList\n",
    "# if experiment does not exist yet, create it\n",
    "if len(mac_experiments) == 0:\n",
    "    mac_experiment = mlflow.MlflowClient().create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "else:\n",
    "    mac_experiment = mac_experiments[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template\n",
    "PROMPT_TEMPLATE = \"\"\"Inputs:\n",
    "Here is the Athena database name: \"{ATHENA_DATABASE}\" \n",
    "And here is the list of all datasets that need to be processed: \n",
    "{FILE_NAME}\n",
    "\n",
    "Return the final response in XML format and nothing else.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if scenario exists and get agent ID\n",
    "if 'scenario' in locals() and scenario is not None and hasattr(scenario, 'agent'):\n",
    "    agent = scenario.agent\n",
    "    BEDROCK_AGENT_ID = agent.get('agentId')\n",
    "    # save the agent id to the environment variable and to the dev.env file\n",
    "    os.environ['BEDROCK_AGENT_ID'] = BEDROCK_AGENT_ID\n",
    "    set_key(local_env_filename, 'BEDROCK_AGENT_ID', BEDROCK_AGENT_ID)\n",
    "else:\n",
    "    # Fallback to hardcoded agent ID\n",
    "    print(\"Scenario not found, using fallback agent ID from environment variable\")\n",
    "\n",
    "print(f\"AGENT_ID: {BEDROCK_AGENT_ID}\")\n",
    "\n",
    "# get agent alias id\n",
    "agent_aliases = bedrock_agent_client.list_agent_aliases(agentId= BEDROCK_AGENT_ID)\n",
    "\n",
    "BEDROCK_AGENT_ALIAS_ID =  agent_aliases.get('agentAliasSummaries')[0].get('agentAliasId')\n",
    "print(f\"AGENT_ALIAS_ID: {BEDROCK_AGENT_ALIAS_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import uuid\n",
    "from typing import List, Optional\n",
    "\n",
    "import time\n",
    "import mlflow\n",
    "from botocore.config import Config\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ChatModel\n",
    "from mlflow.types.llm import ChatResponse, ChatMessage, ChatParams, ChatChoice\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from utils.bedrock_mlflow_agent import BedrockMultiAgentModel\n",
    "\n",
    "# get bedrock agent details with boto3 and extrace the MODEL_ID\n",
    "agent_response = bedrock_agent_client.get_agent( agentId = BEDROCK_AGENT_ID )\n",
    "MAC_MODEL_ID = agent_response.get('agent', {}).get('foundationModel')\n",
    "\n",
    "run_name = f\"mac_{BEDROCK_AGENT_ID}_{MAC_MODEL_ID}\"\n",
    "\n",
    "TEMPERATURE = 0\n",
    "MAXIMUM_LENGTH = 2000\n",
    "print(f'MAC_MODEL_ID: {MAC_MODEL_ID}')\n",
    "\n",
    "prompt = PROMPT_TEMPLATE.format(\n",
    "    ATHENA_DATABASE=S3_BUCKET_NAME.replace('-', '_'),\n",
    "    FILE_NAME=f\"s3://{S3_BUCKET_NAME}/uploads/Orders.csv\"\n",
    ")\n",
    "\n",
    "input_example = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "with mlflow.start_run(experiment_id=mac_experiment.experiment_id, run_name=run_name):\n",
    "\n",
    "    model_config = {\n",
    "        \"agents\": {\n",
    "            \"main\": {\n",
    "                \"model\": MAC_MODEL_ID,\n",
    "                \"aws_region\": REGION,\n",
    "                \"bedrock_agent_id\": BEDROCK_AGENT_ID,\n",
    "                \"bedrock_agent_alias_id\": BEDROCK_AGENT_ALIAS_ID,\n",
    "                \"instruction\": instruction,\n",
    "                \"inference_configuration\": {\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"maximumLength\": MAXIMUM_LENGTH,\n",
    "                },\n",
    "            },  \n",
    "        },\n",
    "        \"aws_profile\": SESSION_PROFILE\n",
    "    }\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": MAC_MODEL_ID,\n",
    "        \"bedrock_agent_id\": BEDROCK_AGENT_ID,\n",
    "        \"bedrock_agent_alias_id\": BEDROCK_AGENT_ALIAS_ID,\n",
    "        \"aws_region\": REGION,\n",
    "        \"instruction\": model_config[\"agents\"][\"main\"][\"instruction\"],\n",
    "        \"temperature\": model_config[\"agents\"][\"main\"][\"inference_configuration\"][\"temperature\"],\n",
    "        \"max_length\": model_config[\"agents\"][\"main\"][\"inference_configuration\"][\"maximumLength\"]\n",
    "    })\n",
    "\n",
    "    # Log any relevant tags\n",
    "    mlflow.set_tags({\n",
    "        \"model_type\": \"bedrock_multi_agent_colloboration\",\n",
    "        \"framework\": \"mlflow_pyfunc\",\n",
    "        \"environment\": \"development\"  # or \"production\", etc.\n",
    "    })\n",
    "\n",
    "    # Define the conda environment\n",
    "    conda_env = {\n",
    "        'name': 'bedrock_agent_env',\n",
    "        'channels': ['defaults', 'conda-forge'],\n",
    "        'dependencies': [\n",
    "            'python=3.11',\n",
    "            'pip',\n",
    "            {'pip': [\n",
    "                'boto3==1.36.2',\n",
    "                'mlflow==2.16.2',\n",
    "                'sagemaker-mlflow'                \n",
    "            ]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Log and load the model using MLflow\n",
    "    mac_chain_info = mlflow.pyfunc.log_model(\n",
    "        python_model=BedrockMultiAgentModel(),\n",
    "        model_config=model_config,\n",
    "        artifact_path=\"mac_agent\",  # This string is used as the path inside the MLflow model where artifacts are stored\n",
    "        input_example=None, # to skip validation #input_example,  # Must be a valid input to your chain\n",
    "        conda_env=conda_env,  # Add conda environment\n",
    "        code_path=[\"./utils/bedrock_mlflow_agent.py\"]\n",
    "\n",
    "    )\n",
    "\n",
    "    mac_agent = mlflow.pyfunc.load_model(mac_chain_info.model_uri)\n",
    "\n",
    "    # Define chat parameters\n",
    "    chat_params = ChatParams(\n",
    "        temperature=float(model_config[\"agents\"][\"main\"][\"inference_configuration\"][\"temperature\"]),\n",
    "        max_tokens=int(model_config[\"agents\"][\"main\"][\"inference_configuration\"][\"maximumLength\"]),\n",
    "        \n",
    "    )\n",
    "\n",
    "    # create a list of inputs by iterating through an S3 directory and creating an input for each file with input_example as a base \n",
    "\n",
    "    response =s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=\"uploads/\")\n",
    "    scenario = {\n",
    "                \"scenario\": \"Goals: - User needs to know if the given dataset has a AI/ML use case. Prepare the data in Amazon Athena. Then identify ML use cases for the data. Review the identified ML use cases and if you find any, use the returned DataLocation and ModelLocation to train a ML model and return its accuracy and respective feature importance. Lastly, use the trained model to generate predictions for the dataset and return the predictions.\",\n",
    "                \"input_problem\": \"PLACEHOLDER\",\n",
    "                \"assertions\": [\n",
    "                    \"agent: GetInformationForSemanticTypeDetection is executed to gather information about the data\",\n",
    "                    \"agent: SaveSQLTableDefinition is executed to save the SQL table definition to S3\",\n",
    "                    \"agent: CreateAthenaTable is executed to create the Athena table\",\n",
    "                    \"agent: QueryData is executed to verify the Athena table creation\",\n",
    "                    \"agent: GetDatabaseSchema is executed to get the latest database schema\",\n",
    "                    \"agent: SaveERM is executed to save the Entity Relationship Diagram in json format to S3\",\n",
    "                    \"agent: GetDatabaseSchema is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                    \"agent: GetUseCases is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                    \"agent: ExecuteQuery is executed to create a ML dataset\",\n",
    "                    \"agent: SaveDataset is executed to save the ML dataset to S3\",\n",
    "                    \"agent: The AI/ML use cases along with the respective S3 location of the ML dataset(s) and target column name are returned in the final response\",\n",
    "                    \"agent: Run exploratory data analysis on the data\",\n",
    "                    \"agent: Determine the right HoldoutFrac for the ml model training\",\n",
    "                    \"agent: Split the data into train and test sets\",\n",
    "                    \"agent: Train a new model with the specified details\",\n",
    "                    \"agent: The model is trained successfully\",\n",
    "                    \"agent: The model location and feature importance are returned in the final response\"\n",
    "                ]\n",
    "            }\n",
    "    scenarios_dict = {\"scenarios\": []}\n",
    "\n",
    "    random_identifier = uuid.uuid4().hex\n",
    "    \n",
    "    # iterate through the files and create an input for each file with input_example as a base \n",
    "    for item in response.get('Contents', []):\n",
    "        file_key = item['Key']\n",
    "        print(f\"file_key: {file_key}\")\n",
    "        # Skip if it's just the directory itself\n",
    "        if file_key.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        # only process two files Customers or Orders\n",
    "        if not file_key.startswith(('uploads/Customers.csv', 'uploads/Orders.csv')):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing file: {file_key}\")\n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "                    ATHENA_DATABASE=S3_BUCKET_NAME.replace('-', '_'),\n",
    "                    FILE_NAME=f\"s3://{S3_BUCKET_NAME}/{file_key}\"\n",
    "                )\n",
    "        print(f\"prompt: {prompt}\")\n",
    "\n",
    "        input_example[\"messages\"][0][\"content\"] = prompt\n",
    "        scenario[\"input_problem\"] = prompt\n",
    "        scenarios_dict[\"scenarios\"].append(scenario.copy())\n",
    "\n",
    "        response = mac_agent.predict(input_example,params=chat_params)\n",
    "        print(response)\n",
    "        \n",
    "        path = f\"../data/eval_dataset/conversations/{random_identifier}\"\n",
    "        print(f\"path: {path}\")\n",
    "        # check if path exists, if not create it\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        # read all files that start with conversation in this directory and determine the highest index\n",
    "        files = os.listdir(path)\n",
    "        highest_index = 0\n",
    "        for file in files:\n",
    "            if file.startswith(\"conversation\"):\n",
    "                index = int(file.split(\"_\")[1].replace(\".json\",\"\"))\n",
    "                if index > highest_index:\n",
    "                    highest_index = index\n",
    "        \n",
    "        # write metrics to file\n",
    "        with open(f\"{path}/metrics_{highest_index+1}.json\", \"w\") as f:\n",
    "            json.dump(mac_agent._model_impl.chat_model._metrics, f)\n",
    "        \n",
    "        # log metrics to mlflow\n",
    "        mlflow.log_table(mac_agent._model_impl.chat_model._metrics, f\"metrics_{highest_index+1}.json\", run_id=mlflow.active_run().info.run_id)\n",
    "\n",
    "        # write conversation_dict to file\n",
    "        with open(f\"{path}/conversation_{highest_index+1}.json\", \"w\") as f:\n",
    "            json.dump(mac_agent._model_impl.chat_model._conversation_json_dict, f)\n",
    "    \n",
    "        # overwrite scenarios to json file so that we always have the latest scenarios\n",
    "        with open(f\"{path}/scenarios.json\", \"w\") as f:\n",
    "            json.dump(scenarios_dict, f, indent=4)\n",
    "\n",
    "        # add a delay of 1.5 minutes to avoid 424 errors\n",
    "        time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ENGINEER_AGENT_ID = data_engineer_agent_alias_arn.split('/')[-2]\n",
    "BUSINESS_ANALYST_AGENT_ID = business_analyst_agent_alias_arn.split('/')[-2]\n",
    "DATA_SCIENTIST_AGENT_ID = data_scientist_agent_alias_arn.split('/')[-2]\n",
    "\n",
    "agent_config = {\n",
    "    \"agents\": [\n",
    "        {\n",
    "            \"agent_id\": BEDROCK_AGENT_ID,\n",
    "            \"agent_name\": \"AI_ML_Team_Supervisor\",\n",
    "            \"agent_instruction\": instruction,\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"tool_name\": \"SupervisorAPI\",\n",
    "                    \"name\": \"SupervisorAPI\",\n",
    "                    \"description\": \"Supervisor agent to analyze feedback and improve agent performance\",\n",
    "                    \"actions\": [\n",
    "                        {\n",
    "                            \"name\": \"GetLessonsLearnedFromPastRuns\",\n",
    "                            \"description\": \"Retrieve lessons learned from user feedback and trace table\",\n",
    "                            \"output_schema\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"body\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"description\": \"List of lessons learned in bullet point format\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"requires_confirmation\": False,\n",
    "                            \"meta\": {}\n",
    "                        }\n",
    "                    ],\n",
    "                    \"tool_type\": \"Module\",\n",
    "                    \"meta\": {}\n",
    "                }\n",
    "            ],\n",
    "            \"reachable_agents\": [\n",
    "                {\n",
    "                    \"scenario\": \"You can invoke the DataEngineerAgent agent when you have been given new datasets that have been staged in Amazon S3 and need to be made available in Amazon Athena as a table so that they can be queried by the business analyst agent. The DataEngineerAgent agent generates semantic type descriptions for each column in the table/file, creates a SQL table definition, and then creates a respective Amazon Athena table, which can be queried.\",\n",
    "                    \"agent_id\": DATA_ENGINEER_AGENT_ID,\n",
    "                    \"context_sharing\": True\n",
    "                },\n",
    "                {\n",
    "                    \"scenario\": \"You can invoke the BusinessAnalystAgent agent to review the data in the Amazon Athena database in order to identify AI/ML use cases that can be performed on the data. It returns any identified ML use cases with details such as the ML use case description, use case business value,  and the ML training dataset location and target column name which can be used by the DataScientistAgent agent.\",\n",
    "                    \"agent_id\": BUSINESS_ANALYST_AGENT_ID,\n",
    "                    \"context_sharing\": True\n",
    "                },\n",
    "                {\n",
    "                    \"scenario\": \"If you have a ML dataset and target column, then you can invoke the DataScientistAgent agent for machine learning tasks such as preparing the data to train a model, training an ML model or using a trained model to generate predictions for a given dataset and target column. The train method of the agent takes in a DataLocation from the BusinessAnalystAgent agent, and returns the trained ML model location, along with details on its accuracy and feature importance. The predict method takes a dataset and target, and generates predictions for it.\",\n",
    "                    \"agent_id\": DATA_SCIENTIST_AGENT_ID,\n",
    "                    \"context_sharing\": True\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": DATA_ENGINEER_AGENT_ID,\n",
    "            \"agent_name\": \"DataEngineer\",\n",
    "            \"agent_instruction\": \"\"\"Expert Data Engineer Agent\n",
    "You are an expert data engineer with access to a comprehensive set of data preparation and management tools. Your role is to help users process, analyze, and query data efficiently.\n",
    "\n",
    "Available Tools\n",
    "GetDatabaseSchema: Retrieves the current SQL database schema from S3.\n",
    "GetERM: Retrieves the Entity Relationship Diagram in JSON format.\n",
    "SaveERM: Saves an Entity Relationship Diagram in JSON format to S3.\n",
    "GetInformationForSemanticTypeDetection: Analyzes data to help detect semantic types.\n",
    "SaveSQLTableDefinition: Saves SQL table definition to S3.\n",
    "CreateAthenaTable: Creates an Athena table based on data and Athena table create definition.\n",
    "QueryData: Executes SQL queries against Athena databases.\n",
    "\n",
    "Workflow\n",
    "1) Data Analysis: When presented with new data, use GetInformationForSemanticTypeDetection to analyze the data structure and content. Identify semantic types for each column and 3 ML use cases where these semantic types could be used in.\n",
    "2) Table Schema Definition: Create appropriate SQL table definition with semantic column names based on the data analysis, considering semantic types and appropriate data types. \n",
    "3) Table Creation: Use the Table Schema Definition as a blueprint for the Athena table creation with the tool CreateAthenaTable to make the data available for querying.\n",
    "4) Data Querying: Verify successful table creation by querying the data using the QueryData tool.\n",
    "5) Entity RelationShip Diagram: Get the latest SQL schema with the GetDatabaseSchema tool.\n",
    "Then generate a new entity relationship diagram and save it with the tool SaveERM.\n",
    "\n",
    "Important Guidelines\n",
    "If you receive multiple datasets as input, process each one methodically and verify that all files have been processed before providing your final response.\n",
    "When creating table definitions, include SQL comments that explain the semantic type of each column and its potential use cases.\n",
    "For primary keys, add specific comments explaining the primary key constraint.\n",
    "Always ensure your SQL follows the appropriate format for the target system (ANSI SQL or Athena SQL).\"\"\",\n",
    "            \"tools\": [\n",
    "                        {\n",
    "                            \"tool_name\": \"DataEngineerAPI\",\n",
    "                            \"name\": \"DataEngineerAPI\",\n",
    "                            \"description\": \"Data Preparation to make data available for AI/ML and Analytics workloads\",\n",
    "                            \"actions\": [\n",
    "                                {\n",
    "                                    \"name\": \"GetDatabaseSchema\",\n",
    "                                    \"description\": \"Retrieve the SQL database schema from S3\",\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"array\",\n",
    "                                        \"items\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"SQL table definition statements\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                },\n",
    "                                {\n",
    "                                    \"name\": \"GetERM\",\n",
    "                                    \"description\": \"Retrieve the Entity Relationship Diagram in json format\",\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"array\",\n",
    "                                        \"items\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"Entity Relationship Diagram in json format\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                },\n",
    "                                {\n",
    "                                    \"name\": \"SaveERM\",\n",
    "                                    \"description\": \"Save the Entity Relationship Diagram in json format to S3\",\n",
    "                                    \"input_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"ERMData\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"Entity Relationship Diagram in json format\"\n",
    "                                            }\n",
    "                                        },\n",
    "                                        \"required\": [\n",
    "                                            \"ERMData\"\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"erm_file_location\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"S3 location where the ERM was saved\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                },\n",
    "                                {\n",
    "                                    \"name\": \"GetInformationForSemanticTypeDetection\",\n",
    "                                    \"description\": \"Get information for semantic type detection\",\n",
    "                                    \"input_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"DataLocation\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"S3 location of the data to analyze\"\n",
    "                                            }\n",
    "                                        },\n",
    "                                        \"required\": [\n",
    "                                            \"DataLocation\"\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"column_names\": {\n",
    "                                                \"data_type\": \"array\",\n",
    "                                                \"items\": {\n",
    "                                                    \"type\": \"string\"\n",
    "                                                }\n",
    "                                            },\n",
    "                                            \"data_sample\": {\n",
    "                                                \"data_type\": \"object\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                },\n",
    "                                {\n",
    "                                    \"name\": \"SaveSQLTableDefinition\",\n",
    "                                    \"description\": \"Save SQL table definition to S3\",\n",
    "                                    \"input_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"SQL_Table_Definition\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"SQL table definition\"\n",
    "                                            },\n",
    "                                            \"TableName\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"File or table name\"\n",
    "                                            }\n",
    "                                        },\n",
    "                                        \"required\": [\n",
    "                                            \"SQL_Table_Definition\"\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"sql_table_definition\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"sql_table_definition_file_location\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                },\n",
    "                                {\n",
    "                                    \"name\": \"CreateAthenaTable\",\n",
    "                                    \"description\": \"Create an Athena table based on data and semantic types\",\n",
    "                                    \"input_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"DataLocation\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"S3 location of the data\"\n",
    "                                            },\n",
    "                                            \"AthenaDatabase\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"The Athena database to create the table in\"\n",
    "                                            },\n",
    "                                            \"Athena_Table_Create_SQL_statement\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"Athena SQL table create statement with LOCATION DataLocation\"\n",
    "                                            },\n",
    "                                            \"TableName\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"Name of the table to create\"\n",
    "                                            }\n",
    "                                        },\n",
    "                                        \"required\": [\n",
    "                                            \"AthenaDatabase\",\n",
    "                                            \"TableName\",\n",
    "                                            \"Athena_Table_Create_SQL_statement\",\n",
    "                                            \"DataLocation\"\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"status\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"table_name\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"table_location\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"table_definition\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"query_results\": {\n",
    "                                                \"data_type\": \"object\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                },\n",
    "                                {\n",
    "                                    \"name\": \"QueryData\",\n",
    "                                    \"description\": \"Execute SQL query against Athena database\",\n",
    "                                    \"input_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"AthenaDatabase\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"The Athena database to query\"\n",
    "                                            },\n",
    "                                            \"SQLQuery\": {\n",
    "                                                \"data_type\": \"string\",\n",
    "                                                \"description\": \"SQL query to execute\"\n",
    "                                            }\n",
    "                                        },\n",
    "                                        \"required\": [\n",
    "                                            \"AthenaDatabase\",\n",
    "                                            \"SQLQuery\"\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"output_schema\": {\n",
    "                                        \"data_type\": \"object\",\n",
    "                                        \"properties\": {\n",
    "                                            \"status\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"query\": {\n",
    "                                                \"data_type\": \"string\"\n",
    "                                            },\n",
    "                                            \"data\": {\n",
    "                                                \"data_type\": \"object\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"requires_confirmation\": False,\n",
    "                                    \"meta\": {}\n",
    "                                }\n",
    "                            ],\n",
    "                            \"tool_type\": \"Module\",\n",
    "                            \"meta\": {}\n",
    "                        }\n",
    "                    ],\n",
    "            \"reachable_agents\": []\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": BUSINESS_ANALYST_AGENT_ID,\n",
    "            \"agent_name\": \"BusinessAnalyst\",\n",
    "            \"agent_instruction\": \"\"\"You are an expert business analyst. \n",
    "You have access to a set of tools to review SQL database schema, run SQL queries against the Athena database, and go through a list of ML use cases and determine whether the AI/ML use case can be performed on the data. \n",
    "Identify one high-value use cases focusing on classification and regression ML problem statements.\n",
    "For each viable AI/ML use case you have identified, provide a comprehensive analysis that includes:\n",
    "- ML Use Case Name\n",
    "- Description\n",
    "- Business Justification\n",
    "- Target Column Specification (e.g. for in customer churn example a customer that has been inactive for 6months)\n",
    "- ML dataset location\n",
    "- Athena SQL query that successfully generated the ML dataset that includes the target column\n",
    "\n",
    "Execute the generated Athena SQL query and ensure that it is valid SQL that can be executed in Amazon Athena. \n",
    "If you encounter any errors, review the Athena error message and correct the SQL query accordingly. \n",
    "If you cannot resolve the Athena error after 3 attempts, eliminate the ML use case and continue with the other use cases.\n",
    "\n",
    "Before your final response, pause and verify that your final response includes the ML dataset location.\"\"\",\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"tool_name\": \"BusinessAnalystAPI\",\n",
    "                    \"name\": \"BusinessAnalystAPI\",\n",
    "                    \"description\": \"API for database analysis and ML dataset preparation\",\n",
    "                    \"actions\": [\n",
    "                        {\n",
    "                            \"name\": \"GetDatabaseSchema\",\n",
    "                            \"description\": \"Retrieve the SQL database schema from S3\",\n",
    "                            \"input_schema\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"AthenaDatabase\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"description\": \"The Athena database name\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    \"AthenaDatabase\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"output_schema\": {\n",
    "                                \"data_type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"table_name\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"columns\": {\n",
    "                                            \"type\": \"array\",\n",
    "                                            \"items\": {\n",
    "                                                \"type\": \"object\",\n",
    "                                                \"properties\": {\n",
    "                                                    \"name\": {\n",
    "                                                        \"type\": \"string\"\n",
    "                                                    },\n",
    "                                                    \"type\": {\n",
    "                                                        \"type\": \"string\"\n",
    "                                                    }\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"requires_confirmation\": False,\n",
    "                            \"meta\": {}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"GetUseCases\",\n",
    "                            \"description\": \"Retrieve available AI/ML use cases from S3\",\n",
    "                            \"output_schema\": {\n",
    "                                \"data_type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"name\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"Name of the use case\"\n",
    "                                        },\n",
    "                                        \"description\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"Detailed description of the use case\"\n",
    "                                        },\n",
    "                                        \"required_columns\": {\n",
    "                                            \"type\": \"array\",\n",
    "                                            \"items\": {\n",
    "                                                \"type\": \"string\"\n",
    "                                            },\n",
    "                                            \"description\": \"Required columns for this use case\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"requires_confirmation\": False,\n",
    "                            \"meta\": {}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"ExecuteQuery\",\n",
    "                            \"description\": \"Execute an Athena query, save results, and return samples\",\n",
    "                            \"input_schema\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"AthenaDatabase\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"description\": \"The Athena database name\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"Query\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"description\": \"The SQL query to execute\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"UseCaseName\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"description\": \"Optional use case name to save full results\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    \"AthenaDatabase\",\n",
    "                                    \"Query\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"output_schema\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"status\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"enum\": [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\", \"ERROR\"]\n",
    "                                    },\n",
    "                                    \"total_rows\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Total number of rows in the full result\"\n",
    "                                    },\n",
    "                                    \"total_columns\": {\n",
    "                                        \"type\": \"integer\",\n",
    "                                        \"description\": \"Total number of columns\"\n",
    "                                    },\n",
    "                                    \"columns\": {\n",
    "                                        \"type\": \"array\",\n",
    "                                        \"items\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"description\": \"List of column names\"\n",
    "                                    },\n",
    "                                    \"sample_data\": {\n",
    "                                        \"type\": \"array\",\n",
    "                                        \"items\": {\n",
    "                                            \"type\": \"object\",\n",
    "                                            \"additionalProperties\": True\n",
    "                                        },\n",
    "                                        \"description\": \"Sample rows from the query results\"\n",
    "                                    },\n",
    "                                    \"dataset_location\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"S3 location of saved dataset (if use case provided)\"\n",
    "                                    },\n",
    "                                    \"error\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Detailed error message if query failed\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"requires_confirmation\": False,\n",
    "                            \"meta\": {}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"SaveDataset\",\n",
    "                            \"description\": \"Save a dataset to S3\",\n",
    "                            \"input_schema\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"UseCaseName\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"description\": \"Name of the use case for the dataset\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"Data\": {\n",
    "                                        \"data_type\": \"array\",\n",
    "                                        \"items\": {\n",
    "                                            \"type\": \"object\",\n",
    "                                            \"additionalProperties\": True\n",
    "                                        },\n",
    "                                        \"description\": \"Dataset to save as array of records\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    \"UseCaseName\",\n",
    "                                    \"Data\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"output_schema\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"location\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"S3 location where the dataset was saved\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"requires_confirmation\": False,\n",
    "                            \"meta\": {}\n",
    "                        }\n",
    "                    ],\n",
    "                    \"tool_type\": \"Module\",\n",
    "                    \"meta\": {}\n",
    "                }\n",
    "            ],\n",
    "            \"reachable_agents\": []\n",
    "        },\n",
    "        {\n",
    "            \"agent_id\": DATA_SCIENTIST_AGENT_ID,\n",
    "            \"agent_name\": \"DataScientist\",\n",
    "            \"agent_instruction\": \"\"\"ROLE: Expert Data Scientist with AutoML\n",
    "CAPABILITIES:\n",
    "1) Exploratory data analysis with the ExploratoryDataAnalysis function  \n",
    "ExploratoryDataAnalysis(\n",
    "APIPath: \"ExploratoryDataAnalysis\"\n",
    "DataLocation: [path_to_data]) Returns: { summary: string, description: string, visualizations: object }\n",
    "\n",
    "2) Create a train test split with the TrainTestSplit function\n",
    "TrainTestSplit(\n",
    "APIPath: \"TrainTestSplit\"\n",
    "DataLocation: [path_to_data]\n",
    "HoldoutFrac: [fraction_of_data_to_holdout_for_testing]\n",
    ") Returns: { train_data_location: string, test_data_location: string }\n",
    "\n",
    "3) Machine learning model training with the Train function\n",
    "Train(\n",
    "APIPath: \"Train\"\n",
    "Target: [target_variable]\n",
    "TrainDataLocation: [train_data_location]\n",
    "TestDataLocation: [test_data_location]\n",
    "ModelLocation: [path_to_store_model]\n",
    " ) Returns: { model_location: string, accuracy_metrics: object, feature_importance: object }\n",
    "\n",
    "3) Prediction generation with the Predict function\n",
    "Predict(\n",
    "APIPath: \"Predict\"\n",
    "DataLocation: [path_to_data]\n",
    "ModelLocation: [path_to_model] \n",
    "ResultDataLocation: [path_to_store_predictions_data]) Returns: sample predictions and location of generated predictions\n",
    "\n",
    "WORKFLOW:\n",
    "First verify you have received a ML dataset location and target column name. If you only got a SQL query respond that you do not have access to Athena and that you need the dataset to be stored in S3.\n",
    "Then use exploratory data analysis to gather more details on the ML dataset\n",
    "Use this information to determine the right HoldoutFrac for the ml model training.\n",
    "Then use the TrainTestSplit function to split the data into train and test sets.\n",
    "\n",
    "If training:\n",
    "Ensure that the ModelLocation is constructed in the \"models/\" subdirectory and has a subfolder that indicates the ml use-case, and that the actual filename is \"models.zip\".\n",
    "Example ModelLocation: /models/customer_churn/model.zip\n",
    "Execute Train() function\n",
    "Return ML model location and ALL feature importance values.\n",
    "\n",
    "If predicting:\n",
    "Ensure that the ResultDataLocation is constructed in the \"results/\" subdirectory and has a filename that indicates the ml use-case, and has a filename with CSV file extension.\n",
    "Example ModelLocation: /models/customer_churn/model.zip\n",
    "Execute Predict() function\n",
    "Return sample predictions and predictions dataset location.\n",
    "\n",
    "ERROR HANDLING:\n",
    "Report missing or invalid parameters\n",
    "Alert insufficient data quality/quantity\n",
    "Notify of model compatibility issues\n",
    "\n",
    "Ensure that any created ML model is stored in the \"models/\" subdirectory, and any generated predictions are stored in the \"results/\" subdirectory.\"\"\",\n",
    "            \"tools\": [\n",
    "        {\n",
    "            \"tool_name\": \"DataScientistAPI\",\n",
    "            \"name\": \"DataScientistAPI\",\n",
    "            \"description\": \"AutoML to train a ML model, use a ML model to make predictions, or get feature importance\",\n",
    "            \"actions\": [\n",
    "                {\n",
    "                    \"name\": \"Train\",\n",
    "                    \"description\": \"Train a machine learning model\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"Target\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"Target\",\n",
    "                                \"description\": \"The target column to predict\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"DataLocation\",\n",
    "                                \"description\": \"S3 location of the data to train the model on\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"ModelLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"ModelLocation\",\n",
    "                                \"description\": \"S3 location to store the model\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"Target\",\n",
    "                            \"DataLocation\",\n",
    "                            \"ModelLocation\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"title\": \"200\",\n",
    "                        \"description\": \"Successful operation\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"content\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"message\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"results\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": []\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Predict\",\n",
    "                    \"description\": \"Make predictions using a trained model\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"Target\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"Target\",\n",
    "                                \"description\": \"The target column to predict\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"DataLocation\",\n",
    "                                \"description\": \"S3 location of the input data for the prediction\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"ResultDataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"ResultDataLocation\",\n",
    "                                \"description\": \"S3 location of the output data for the prediction\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"ModelLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"ModelLocation\",\n",
    "                                \"description\": \"S3 location of the trained model that is used for the predictions\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"Target\",\n",
    "                            \"DataLocation\",\n",
    "                            \"ModelLocation\",\n",
    "                            \"ResultDataLocation\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"title\": \"200\",\n",
    "                        \"description\": \"Successful operation\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"content\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"message\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"results\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": []\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"FeatureImportance\",\n",
    "                    \"description\": \"Get feature importance from a trained model\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"Target\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"Target\",\n",
    "                                \"description\": \"The target column to predict\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"DataLocation\",\n",
    "                                \"description\": \"S3 location of the data\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"ModelLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"ModelLocation\",\n",
    "                                \"description\": \"S3 location of the trained model that is used to get the feature importance\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"Target\",\n",
    "                            \"DataLocation\",\n",
    "                            \"ModelLocation\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"title\": \"200\",\n",
    "                        \"description\": \"Successful operation\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"content\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"message\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"results\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": []\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"ExploratoryDataAnalysis\",\n",
    "                    \"description\": \"Perform exploratory data analysis on a dataset\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"DataLocation\",\n",
    "                                \"description\": \"S3 location of the data\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"DataLocation\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"title\": \"200\",\n",
    "                        \"description\": \"Successful operation\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"content\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"message\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"results\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": []\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"TrainTestSplit\",\n",
    "                    \"description\": \"Perform train test split on a dataset\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"title\": \"DataLocation\",\n",
    "                                \"description\": \"S3 location of the data\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"HoldoutFrac\": {\n",
    "                                \"data_type\": \"number\",\n",
    "                                \"title\": \"HoldoutFrac\",\n",
    "                                \"description\": \"Fraction of data to hold out for testing\",\n",
    "                                \"required\": []\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"DataLocation\",\n",
    "                            \"HoldoutFrac\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"title\": \"200\",\n",
    "                        \"description\": \"Successful operation\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"required\": []\n",
    "                            },\n",
    "                            \"content\": {\n",
    "                                \"data_type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"message\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    },\n",
    "                                    \"results\": {\n",
    "                                        \"data_type\": \"string\",\n",
    "                                        \"required\": []\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": []\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                }\n",
    "            ],\n",
    "            \"tool_type\": \"Module\",\n",
    "            \"meta\": {}\n",
    "        }\n",
    "    ],\n",
    "            \"reachable_agents\": []\n",
    "        }\n",
    "    ],\n",
    "    \"primary_agent_id\": BEDROCK_AGENT_ID,\n",
    "    \"human_id\": \"User\"\n",
    "}\n",
    "# save agent config to json file\n",
    "with open(f'../data/eval_dataset/conversations/{random_identifier}/agents.json', 'w') as f:\n",
    "    json.dump(agent_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define different evaluation scenarios\n",
    "evaluation_scenarios = {\n",
    "    \"scenarios\": [\n",
    "        {\n",
    "            \"scenario\": \"Goals: - User needs to know if their data supports any AI/ML use cases. Prepare the data in Amazon Athena. Then identify ML use cases for the data. Review the identified ML use cases and if you find any, train a ML model generate predictions.\",\n",
    "            \"input_problem\": ( 'Inputs: AthenaDatabase: {ATHENA_DATABASE} SourceDataLocation: s3://{S3_BUCKET_NAME}/uploads/Customers.csv'\n",
    "                \n",
    "            ),     \n",
    "            \"assertions\": [\n",
    "                \"agent: GetInformationForSemanticTypeDetection is executed to gather information about the data\",\n",
    "                \"agent: SaveSQLTableDefinition is executed to save the SQL table definition to S3\",\n",
    "                \"agent: CreateAthenaTable is executed to create the Athena table\",\n",
    "                \"agent: QueryData is executed to verify the Athena table creation\",\n",
    "                \"agent: GetDatabaseSchema is executed to get the latest database schema\",\n",
    "                \"agent: SaveERM is executed to save the Entity Relationship Diagram in json format to S3\",\n",
    "                \"agent: GetDatabaseSchema is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                \"agent: GetUseCases is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                \"agent: ExecuteQuery is executed to create a ML dataset\",\n",
    "                \"agent: SaveDataset is executed to save the ML dataset to S3\",\n",
    "                \"agent: The AI/ML use cases along with the respective S3 location of the ML dataset(s) and target column name are returned in the final response\",\n",
    "                \"agent: Run exploratory data analysis on the data\",\n",
    "                \"agent: Determine the right HoldoutFrac for the ml model training\",\n",
    "                \"agent: Split the data into train and test sets\",\n",
    "                \"agent: Train a new model with the specified details\",\n",
    "                \"agent: The model is trained successfully\",\n",
    "                \"agent: The model location and feature importance are returned in the final response\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Goals: - User needs to know if their data supports any AI/ML use cases. Identify ML use cases for the data. Review the identified ML use cases and if you find any, train a ML model generate predictions.\",\n",
    "            \"input_problem\": (\n",
    "                'Inputs: Here is the Athena database name: \"{ATHENA_DATABASE}\"'\n",
    "                'No additional datasets need to be processed.'\n",
    "                'Return the final response in XML format and nothing else.'\n",
    "               ),     \n",
    "            \"assertions\": [\n",
    "                \"agent: GetDatabaseSchema is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                \"agent: GetUseCases is executed to detect AI/ML use cases that can be performed on the data in the Athena database\",\n",
    "                \"agent: ExecuteQuery is executed to create a ML dataset\",\n",
    "                \"agent: SaveDataset is executed to save the ML dataset to S3\",\n",
    "                \"agent: The AI/ML use cases along with the respective S3 location of the ML dataset(s) and target column name are returned in the final response\",\n",
    "                \"agent: Run exploratory data analysis on the data\",\n",
    "                \"agent: Determine the right HoldoutFrac for the ml model training\",\n",
    "                \"agent: Split the data into train and test sets\",\n",
    "                \"agent: Train a new model with the specified details\",\n",
    "                \"agent: The model is trained successfully\",\n",
    "                \"agent: The model location and feature importance are returned in the final response\"\n",
    "            ]\n",
    "        },   \n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "# save evaluation scenarios to json file\n",
    "with open(f'../data/eval_dataset/conversations/{random_identifier}/scenarios.json', 'w') as f:\n",
    "    json.dump(evaluation_scenarios, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark\n",
    "from utils.benchmark import run_benchmark\n",
    "\n",
    "results = run_benchmark(\n",
    "    dataset_dir=path,\n",
    "    scenario_filename=\"scenarios.json\",\n",
    "    conversations_dir=path,\n",
    "    llm_judge_id=MODEL_ID,\n",
    "    region=REGION,\n",
    "    session=session\n",
    ")\n",
    "\n",
    "# Check if results is not None before proceeding\n",
    "if results is not None:\n",
    "    # Create high-level metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'user_gsr': [results['user_gsr']],\n",
    "        'system_gsr': [results['system_gsr']],\n",
    "        'overall_gsr': [results['overall_gsr']],\n",
    "        'partial_gsr': [results['partial_gsr']],\n",
    "        'scenario_count': [results['scenario_count']],\n",
    "        'conversation_count': [results['conversation_count']]\n",
    "    })\n",
    "\n",
    "    # Create detailed assertions DataFrame\n",
    "    assertions_list = []\n",
    "    for eval_result in results['conversation_evals']:\n",
    "        trajectory_index = eval_result['trajectory_index']\n",
    "        for assertion in eval_result['report']:\n",
    "            assertions_list.append({\n",
    "                'trajectory_index': trajectory_index,\n",
    "                'assertion_type': assertion['assertion_type'],\n",
    "                'assertion': assertion['assertion'],\n",
    "                'answer': assertion['answer'],\n",
    "                'evidence': assertion['evidence']\n",
    "            })\n",
    "\n",
    "    assertions_df = pd.DataFrame(assertions_list)\n",
    "\n",
    "    # Display results\n",
    "    print(\"High-level Metrics:\")\n",
    "    display(metrics_df)\n",
    "\n",
    "    print(\"\\nDetailed Assertions:\")\n",
    "    display(assertions_df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: run_benchmark returned None. Please check for errors in the benchmark execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log multi-agent-colloboration results to mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new experiment for multi-agent-colloboration if it does not exist yet\n",
    "import uuid\n",
    "experiment_description = (\n",
    "        \"multi-agent-colloboration.\"\n",
    "    )\n",
    "\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"multi-agent-colloboration\",\n",
    "    \"use_case\": \"multi-agent-colloboration\",\n",
    "    \"team\": \"aws-ai-ml-analytics\",\n",
    "    \"source\": \"multi-agent-colloboration\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "experiment_name = f\"multi-agent-colloboration_{random_identifier}\"\n",
    "\n",
    "# Use search_experiments() to search on the project_name tag key\n",
    "mac_experiments = mlflow.MlflowClient().search_experiments(\n",
    "    filter_string=\"tags.`project_name` = 'multi-agent-colloboration'\"\n",
    ")\n",
    "# if experiment does not exist yet, create it\n",
    "if len(mac_experiments) == 0:\n",
    "    mac_experiment = mlflow.MlflowClient().create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "else:\n",
    "    mac_experiment = mac_experiments[0]\n",
    "\n",
    "# get the first run id of the mlflow experiment\n",
    "runs = mlflow.search_runs(experiment_ids=[mac_experiment.experiment_id])\n",
    "if len(runs) > 0:\n",
    "    run_id = runs.iloc[0].run_id\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "\n",
    "\n",
    "# log metrics to mlflow experiment\n",
    "\n",
    "mlflow.log_metrics(metrics_df.iloc[0].to_dict(), run_id=run_id)\n",
    "\n",
    "# log assertions to mlflow experiment\n",
    "mlflow.log_table(assertions_df, \"assertions.json\", run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all metrics json files from the \"{dataset_dir}/conversations\" directory into a pandas dataframe\n",
    "\n",
    "# get number of files in the \"{dataset_dir}/conversations\" directory that contain metrics in the filename\n",
    "print(f\"path: {path}\")\n",
    "num_files = len([f for f in os.listdir(path) if f.endswith('.json') and 'metrics' in f])\n",
    "print(f\"num_files: {num_files}\")\n",
    "\n",
    "# read all metrics json files from the path directory into a pandas dataframe\n",
    "metrics_run_df = pd.DataFrame()\n",
    "for i in range(num_files):\n",
    "    metrics_file = os.path.join(path, f\"metrics_{i}.json\")\n",
    "    print(f\"metrics_file: {metrics_file}\")\n",
    "    if not os.path.exists(metrics_file):\n",
    "        continue\n",
    "    with open(metrics_file) as f:\n",
    "        # load json file into a pandas dataframe\n",
    "        print(f\"reading metrics file: {metrics_file}\")\n",
    "        metrics_data = json.load(f)\n",
    "        metrics_run_tmp_df = pd.DataFrame([metrics_data])\n",
    "        print(f\"metrics_run_tmp_df: {metrics_run_tmp_df}\")\n",
    "        metrics_run_df = pd.concat([metrics_run_df, metrics_run_tmp_df], ignore_index=True)\n",
    "\n",
    "# name the index column \"conversation_index\"\n",
    "metrics_run_df.index.name = \"conversation_index\"\n",
    "\n",
    "# get avg total_tokens, avg num_agent_calls, avg num tool_calls, avg num_kb_lookups, and total sum of total_tokens\n",
    "# Calculate average and total metrics\n",
    "if not metrics_run_df.empty:\n",
    "    metrics_summary = {\n",
    "        'avg_total_tokens': metrics_run_df['total_tokens'].mean() if 'total_tokens' in metrics_run_df.columns else 0,\n",
    "        'avg_num_agent_calls': metrics_run_df['num_agent_calls'].mean() if 'num_agent_calls' in metrics_run_df.columns else 0,\n",
    "        'avg_num_tool_calls': metrics_run_df['num_tool_calls'].mean() if 'num_tool_calls' in metrics_run_df.columns else 0,\n",
    "        'avg_num_kb_lookups': metrics_run_df['num_kb_lookups'].mean() if 'num_kb_lookups' in metrics_run_df.columns else 0,\n",
    "        'total_tokens': metrics_run_df['total_tokens'].sum() if 'total_tokens' in metrics_run_df.columns else 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    for metric, value in metrics_summary.items():\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "    \n",
    "    # Log metrics to mlflow if run_id exists\n",
    "    if 'run_id' in locals() and run_id:\n",
    "        mlflow.log_metrics(metrics_summary, run_id=run_id)\n",
    "else:\n",
    "    print(\"No metrics data available for calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Supervisor Agent was able to orchestrate the multi-agent collaboration by using the reachable agents to perform the tasks of identifying ML use cases, training a ML model, and then generating predictions with the trained model.\n",
    "\n",
    "We successfully validated the Supervisor Agent results by using the assertions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "- We focused on the multi-agent collaboration evaluation framework. Thus we only used a relatively simple scenario and only two sample datasets. For an actual production use case, this would need to be significantly expanded.\n",
    "\n",
    "- Here we used \"anthropic.claude-3-5-sonnet-20240620-v1:0\" in all our agents, but you can use this framework in a metric-driven development process to evaluate the performance of your agents and improve efficiency. As a next step, we could evaluate the performance of smaller/faster LLMs in the supervisor or any of the sub-agents and assess if the performance is still good enough.\n",
    "\n",
    "- In its current form this use case is a sequential workflow: first uploading a dataset, then identifying ML use cases, then training a model, and finally generating predictions. Therefore, an agentic workflow would have been a more efficient approach. Instead of relying on the supervisor agent to determine the order of execution and handle the orchestration of the agents, we could have used Amazon Bedrock Flows or any other orchestration tool to execute these agents in the predefined, deterministic order (see also [Amazon Bedrock Flows](https://aws.amazon.com/bedrock/flows/) & [Anthropic's blog on building effective agents](https://www.anthropic.com/research/building-effective-agents) ).\n",
    "\n",
    "- To properly implement the use case of identifying ML use cases and subsequently training a model and generating predictions, we would need to expand on the covered AI/ML use cases and algorithms. In addition, the current implementation is only meant for relatively small datasets as it runs synchronously in Lambda, and as such is bound by the Lambda execution time. In an actual production implementation, we would need to implement an asynchronous execution of the agents and their respective tools.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
