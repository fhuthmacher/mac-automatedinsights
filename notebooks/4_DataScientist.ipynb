{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Scientist Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will first create a data scientist agent with Amazon Bedrock Agents that will be able to train an AutoML model with AutoGluon and generate predictions as well as generate feature importance.\n",
    "\n",
    "Then we will evaluate the agents's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from utils.bedrock import BedrockLLMWrapper\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['AWS_ACCOUNT'] = os.getenv('AWS_ACCOUNT')\n",
    "os.environ['DATASCIENTIST_AGENT_PROFILE_ARN'] = os.getenv('DATASCIENTIST_AGENT_PROFILE_ARN')\n",
    "os.environ['DATASCIENTIST_AGENT_EVAL_PROFILE_ARN'] = os.getenv('DATASCIENTIST_AGENT_EVAL_PROFILE_ARN')\n",
    "REGION = os.environ['REGION']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "AWS_ACCOUNT = os.environ['AWS_ACCOUNT']\n",
    "DATASCIENTIST_AGENT_PROFILE_ARN = os.environ['DATASCIENTIST_AGENT_PROFILE_ARN']\n",
    "DATASCIENTIST_AGENT_EVAL_PROFILE_ARN = os.environ['DATASCIENTIST_AGENT_EVAL_PROFILE_ARN']\n",
    "\n",
    "# Bedrock Agents does not yet support application inference profiles\n",
    "MODEL_ID =  \"anthropic.claude-3-5-sonnet-20240620-v1:0\" #\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\" # DATAENGINEER_AGENT_PROFILE_ARN #\"us.amazon.nova-pro-v1:0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore.config\n",
    "config = botocore.config.Config(\n",
    "    connect_timeout=600,  # 10 minutes\n",
    "    read_timeout=600,     # 10 minutes\n",
    "    retries={'max_attempts': 3}\n",
    ")\n",
    "\n",
    "session = boto3.Session(region_name=REGION)\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "\n",
    "bedrock_agent_client = session.client('bedrock-agent', config=config)\n",
    "bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', config=config)\n",
    "bedrock_runtime_client = session.client('bedrock-runtime', config=config)\n",
    "bedrock_client = session.client('bedrock', config=config)\n",
    "lambda_client = session.client('lambda', config=config)\n",
    "iam_resource = session.resource('iam')\n",
    "iam_client = session.client('iam')\n",
    "athena_client = session.client('athena')\n",
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Scientist agent lambda function with autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../datascientist/bedrock_data_scientist_agent.py\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import zipfile\n",
    "from urllib.parse import urlparse\n",
    "import base64\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional, Dict, Any, List, Union, Tuple\n",
    "import time\n",
    "import botocore\n",
    "import traceback\n",
    "import tempfile\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# get the environment variables\n",
    "S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
    "MODEL_ID = os.getenv('MODEL_ID')\n",
    "\n",
    "ATHENA_QUERY_EXECUTION_LOCATION = f's3://{S3_BUCKET_NAME}/athena_results/'\n",
    "\n",
    "class Parameters(BaseModel):\n",
    "    target: Optional[str] = None\n",
    "    model_location: Optional[str] = None\n",
    "    data_location: Optional[str] = None\n",
    "    train_data_location: Optional[str] = None\n",
    "    test_data_location: Optional[str] = None\n",
    "    result_data_location: Optional[str] = None\n",
    "    hyperparameters: Optional[Dict[str, Dict[str, int]]] = None\n",
    "    holdout_frac: Optional[float] = None\n",
    "    api_path: str\n",
    "\n",
    "    @validator('target')\n",
    "    def validate_target(cls, v, values):\n",
    "        # Only validate target if the API path requires it\n",
    "        if values.get('api_path') in ['Train', 'Predict'] and not v:\n",
    "            raise ValueError(\"Target is required for TrainModel and Predict operations\")\n",
    "        return v\n",
    "    \n",
    "    @validator('model_location')\n",
    "    def validate_model_location(cls, v, values):\n",
    "        # Only validate model_location if the API path requires it\n",
    "        if values.get('api_path') in ['Train', 'Predict'] and not v:\n",
    "            raise ValueError(\"Model location is required for TrainModel and Predict operations\")\n",
    "        return v\n",
    "\n",
    "class APIResponse(BaseModel):\n",
    "    message: str\n",
    "    results: Dict[str, Any]\n",
    "\n",
    "def normalize_s3_path(path: str, default_bucket: str) -> str:\n",
    "    \"\"\"Normalize S3 path to full path with bucket name\"\"\"\n",
    "    if not path:\n",
    "        return None\n",
    "        \n",
    "    # If path already starts with s3://, return as is\n",
    "    if path.startswith('s3://'):\n",
    "        return path\n",
    "        \n",
    "    # Remove any leading slashes\n",
    "    clean_path = path.lstrip('/')\n",
    "    \n",
    "    # Construct full S3 path\n",
    "    return f's3://{default_bucket}/{clean_path}'\n",
    "\n",
    "def parse_s3_path(s3_path: str) -> Tuple[str, str]:\n",
    "    \"\"\"Parse S3 path into bucket and key\"\"\"\n",
    "    if not s3_path:\n",
    "        return None, None\n",
    "        \n",
    "    parsed = urlparse(s3_path)\n",
    "    bucket = parsed.netloc\n",
    "    key = parsed.path.lstrip('/')\n",
    "    return bucket, key\n",
    "\n",
    "def parse_parameters(event: Dict[str, Any]) -> Parameters:\n",
    "    parameters = {}\n",
    "    \n",
    "    # Extract APIPath from event\n",
    "    api_path = event.get('apiPath', '').strip('/')\n",
    "    \n",
    "    # Try to extract parameters from inputText if it contains JSON\n",
    "    if 'inputText' in event:\n",
    "        try:\n",
    "            json_start = event['inputText'].find('{')\n",
    "            if json_start != -1:\n",
    "                json_str = event['inputText'][json_start:]\n",
    "                input_json = json.loads(json_str)\n",
    "                if 'parameters' in input_json:\n",
    "                    for param in input_json['parameters']:\n",
    "                        parameters[param['name']] = param['value']\n",
    "        except json.JSONDecodeError:\n",
    "            logger.warning(\"Could not parse JSON from inputText\")\n",
    "\n",
    "    # Extract parameters from requestBody if present\n",
    "    if 'requestBody' in event and 'content' in event['requestBody']:\n",
    "        content = event['requestBody']['content']\n",
    "        if 'application/json' in content and 'properties' in content['application/json']:\n",
    "            for prop in content['application/json']['properties']:\n",
    "                parameters[prop['name']] = prop['value']\n",
    "\n",
    "    return Parameters(\n",
    "        api_path=api_path,\n",
    "        target=parameters.get('Target'),\n",
    "        model_location=parameters.get('ModelLocation'),\n",
    "        data_location=parameters.get('DataLocation'),\n",
    "        train_data_location=parameters.get('TrainDataLocation'),\n",
    "        test_data_location=parameters.get('TestDataLocation'),\n",
    "        result_data_location=parameters.get('ResultDataLocation'),\n",
    "        hyperparameters=parameters.get('Hyperparameters'),\n",
    "        holdout_frac=parameters.get('HoldoutFrac')\n",
    "    )\n",
    "\n",
    "def train(train_df, test_df, target, model_location, hyperparameters) -> APIResponse:\n",
    "    logger.info(\"Training model...\")\n",
    "\n",
    "    # Create a writable directory for AutoGluon\n",
    "    temp_dir = \"/tmp/model\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Set the current working directory to the temp directory\n",
    "    original_dir = os.getcwd()\n",
    "    os.chdir(temp_dir)\n",
    "\n",
    "    try:\n",
    "        if hyperparameters is None:\n",
    "            hyperparameters = {\n",
    "                'GBM': {'num_boost_round': 10000},\n",
    "                'RF': {'n_estimators': 300},\n",
    "                'XT': {'n_estimators': 300},\n",
    "            }\n",
    "        \n",
    "        num_trials = 3  # try at most 3 different hyperparameter configurations for each type of model\n",
    "        search_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n",
    "\n",
    "        hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "            'num_trials': num_trials,\n",
    "            'scheduler' : 'local',\n",
    "            'searcher': search_strategy,\n",
    "        }\n",
    "        \n",
    "        # Train the model\n",
    "        time_limit = 3*60  # train various models for ~3 min\n",
    "\n",
    "        predictor = TabularPredictor(label=target).fit(train_df, presets='medium', time_limit=time_limit, hyperparameters=hyperparameters, hyperparameter_tune_kwargs=hyperparameter_tune_kwargs)\n",
    "\n",
    "        logger.info(\"Model trained successfully\")\n",
    "\n",
    "        # Create absolute path and ensure the model directory exists\n",
    "        model_path = os.path.abspath(\"/tmp/model\")\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        logger.info(f\"Created model directory at: {model_path}\")\n",
    "\n",
    "        # Save the model with absolute path\n",
    "        predictor.save(model_path)\n",
    "        logger.info(f\"Model saved to: {model_path}\")\n",
    "\n",
    "        # Compile the model\n",
    "        predictor.compile()\n",
    "        logger.info(\"Model compiled successfully\")\n",
    "\n",
    "        # Get model metrics and convert to serializable format\n",
    "        model_accuracy = predictor.evaluate(test_df)\n",
    "        model_names = predictor.model_names()\n",
    "        \n",
    "        # Convert feature importance DataFrame to dict\n",
    "        feature_importance = predictor.feature_importance(train_df)\n",
    "        feature_importance_dict = feature_importance.to_dict() if isinstance(feature_importance, pd.DataFrame) else {}\n",
    "        \n",
    "        # Get leaderboard and convert to dict\n",
    "        leaderboard = predictor.leaderboard(test_df, extra_info=True)\n",
    "        leaderboard_dict = leaderboard.to_dict() if isinstance(leaderboard, pd.DataFrame) else {}\n",
    "\n",
    "        # Verify the files were saved\n",
    "        saved_files = os.listdir(model_path)\n",
    "        logger.info(f\"Files in model directory: {saved_files}\")\n",
    "\n",
    "        # zip contents of /tmp/model/   \n",
    "        with zipfile.ZipFile(\"/tmp/model.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(\"/tmp/model/\"):\n",
    "                for file in files:\n",
    "                    if file != \"model.zip\":  # Skip the zip file itself\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(file_path, \"/tmp/model/\")\n",
    "                        zipf.write(file_path, arcname)\n",
    "\n",
    "        # Parse S3 location\n",
    "        s3_url = urlparse(model_location)\n",
    "        bucket = s3_url.netloc\n",
    "        key = s3_url.path.lstrip('/')  # Remove leading slash\n",
    "\n",
    "        # Upload model.zip to s3\n",
    "        s3_client.upload_file(\n",
    "            \"/tmp/model.zip\",\n",
    "            Bucket=bucket,\n",
    "            Key=key\n",
    "        )\n",
    "\n",
    "        return APIResponse(\n",
    "            message=\"Model trained successfully\",\n",
    "            results={\n",
    "                'model_location': model_location,\n",
    "                'model_names': model_names,\n",
    "                'model_accuracy': {k: float(v) if isinstance(v, (int, float)) else v \n",
    "                                 for k, v in model_accuracy.items()},\n",
    "                'feature_importance': feature_importance_dict,\n",
    "                'leaderboard': leaderboard_dict\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in train function: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        # Restore original working directory\n",
    "        os.chdir(original_dir)\n",
    "\n",
    "def predict(df, model_location, result_data_location) -> APIResponse:\n",
    "    logger.info(\"Predicting...\")\n",
    "    # Load the model\n",
    "    logger.info(\"Loading model...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure the model directory exists\n",
    "        os.makedirs(\"/tmp/model\", exist_ok=True)\n",
    "        \n",
    "        # download model.zip from s3\n",
    "        s3_url = urlparse(model_location)\n",
    "        bucket = s3_url.netloc\n",
    "        key = s3_url.path.lstrip('/')  # Remove leading slash\n",
    "        s3_client.download_file(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Filename=\"/tmp/model/model.zip\"\n",
    "        )\n",
    "\n",
    "        # Clear the model directory first\n",
    "        for root, dirs, files in os.walk(\"/tmp/model\", topdown=False):\n",
    "            for name in files:\n",
    "                if name != \"model.zip\":  # Don't delete the zip we just downloaded\n",
    "                    try:\n",
    "                        os.remove(os.path.join(root, name))\n",
    "                    except OSError as e:\n",
    "                        logger.warning(f\"Error removing file {name}: {e}\")\n",
    "            for name in dirs:\n",
    "                try:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "                except OSError as e:\n",
    "                    logger.warning(f\"Error removing directory {name}: {e}\")\n",
    "\n",
    "        # unzip model.zip\n",
    "        with zipfile.ZipFile(\"/tmp/model/model.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"/tmp/model\")\n",
    "        \n",
    "        # Find the AutogluonModels directory\n",
    "        model_dir = \"/tmp/model\"\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            if \"predictor.pkl\" in files:\n",
    "                model_dir = root\n",
    "                break\n",
    "        \n",
    "        # Load the model from the correct path\n",
    "        predictor = TabularPredictor.load(model_dir)\n",
    "        logger.info(f\"Model loaded successfully from {model_dir}\")\n",
    "\n",
    "        # Make prediction\n",
    "        logger.info(\"Making prediction...\")\n",
    "        prediction = predictor.predict(df)\n",
    "        logger.info(f\"Prediction made: {prediction}\")\n",
    "\n",
    "        # join the predictions with the original dataframe by adding a new column 'prediction'\n",
    "        df['prediction'] = prediction\n",
    "        logger.info(f\"Joined predictions with original dataframe: {df}\")\n",
    "\n",
    "        # save the predictions to a new csv file\n",
    "        df.to_csv(\"/tmp/predictions.csv\", index=False)\n",
    "        logger.info(f\"Predictions saved to /tmp/predictions.csv\")\n",
    "\n",
    "        # upload the predictions to s3\n",
    "        results_s3_url = urlparse(result_data_location)\n",
    "        results_bucket = results_s3_url.netloc\n",
    "        results_key = results_s3_url.path.lstrip('/')  # Remove leading slash\n",
    "        s3_client.upload_file(\n",
    "            \"/tmp/predictions.csv\",\n",
    "            Bucket=results_bucket,\n",
    "            Key=results_key\n",
    "        )\n",
    "        logger.info(f\"Predictions uploaded to s3: {result_data_location}\")\n",
    "\n",
    "        sample_predictions = prediction[:10].tolist() if len(prediction) > 10 else prediction.tolist()\n",
    "        \n",
    "        return APIResponse(\n",
    "            message=\"Prediction made successfully\",\n",
    "            results={\n",
    "                'prediction_results_location': result_data_location,\n",
    "                'predictions': sample_predictions,\n",
    "                'total_predictions': len(prediction),\n",
    "                'model_info': str(predictor.model_names())\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict function: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def feature_importance(df: pd.DataFrame, model_location: str) -> APIResponse:\n",
    "    \"\"\"Calculate feature importance for a trained model\"\"\"\n",
    "    logger.info(\"Calculating feature importance...\")\n",
    "    \n",
    "    if df is None:\n",
    "        raise ValueError(\"Data is required to calculate feature importance\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure the model directory exists\n",
    "        os.makedirs(\"/tmp/model\", exist_ok=True)\n",
    "        \n",
    "        # download model.zip from s3\n",
    "        s3_url = urlparse(model_location)\n",
    "        bucket = s3_url.netloc\n",
    "        key = s3_url.path.lstrip('/')  # Remove leading slash\n",
    "        \n",
    "        try:\n",
    "            s3_client.download_file(\n",
    "                Bucket=bucket,\n",
    "                Key=key,\n",
    "                Filename=\"/tmp/model/model.zip\"\n",
    "            )\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                raise ValueError(f\"Model not found at location: {model_location}\")\n",
    "            raise\n",
    "\n",
    "        # unzip model.zip\n",
    "        with zipfile.ZipFile(\"/tmp/model/model.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"/tmp/model\")\n",
    "        \n",
    "        # Find the AutogluonModels directory\n",
    "        model_dir = \"/tmp/model\"\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            if \"predictor.pkl\" in files:\n",
    "                model_dir = root\n",
    "                break\n",
    "        \n",
    "        # Load the model from the correct path\n",
    "        predictor = TabularPredictor.load(model_dir)\n",
    "        logger.info(f\"Model loaded successfully from {model_dir}\")\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        feature_importance = predictor.feature_importance(df)\n",
    "        logger.info(f\"Feature importance calculated successfully\")\n",
    "        feature_importance_dict = feature_importance.to_dict() if isinstance(feature_importance, pd.DataFrame) else {}\n",
    "        \n",
    "            \n",
    "        return APIResponse(\n",
    "            message=\"Feature importance calculated successfully\",\n",
    "            results={\n",
    "                'feature_importance': feature_importance_dict\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating feature importance: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def save_dataset(self, df: pd.DataFrame, use_case_name: str) -> str:\n",
    "    \"\"\"Save a dataset to S3 and return its location\"\"\"\n",
    "    logger.info(f\"Saving dataset for use case: {use_case_name}\")\n",
    "    logger.debug(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.csv') as tmp:\n",
    "            df.to_csv(tmp.name, index=False)\n",
    "            s3_path = f'ml_datasets/{use_case_name}_dataset.csv'\n",
    "            \n",
    "            logger.info(f\"Uploading dataset to s3://{self.s3_bucket_name}/{s3_path}\")\n",
    "            self.s3_client.upload_file(\n",
    "                tmp.name,\n",
    "                self.s3_bucket_name,\n",
    "                s3_path\n",
    "            )\n",
    "            \n",
    "            location = f's3://{self.s3_bucket_name}/{s3_path}'\n",
    "            logger.info(f\"Dataset successfully saved to {location}\")\n",
    "            return location\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in save_dataset: {str(e)}\", exc_info=True)\n",
    "        return ''\n",
    "        \n",
    "def load_data(data_location: str) -> pd.DataFrame:\n",
    "    \"\"\"Load data from S3 with proper error handling\"\"\"\n",
    "    if not data_location:\n",
    "        raise ValueError(\"DataLocation is required\")\n",
    "        \n",
    "    logger.info(f\"Loading data from {data_location}\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure data directory exists\n",
    "        os.makedirs(\"/tmp/data\", exist_ok=True)\n",
    "        \n",
    "        # Parse and validate data location\n",
    "        data_bucket, data_key = parse_s3_path(data_location)\n",
    "        if not data_bucket or not data_key:\n",
    "            raise ValueError(f\"Invalid data location format: {data_location}\")\n",
    "            \n",
    "        # Get filename and local path\n",
    "        filename = os.path.basename(data_key)\n",
    "        local_file_path = os.path.join(\"/tmp/data\", filename)\n",
    "        \n",
    "        logger.info(f\"Downloading from bucket: {data_bucket}, key: {data_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if file exists\n",
    "            s3_client.head_object(Bucket=data_bucket, Key=data_key)\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                raise ValueError(f\"Data file not found: {data_location}\")\n",
    "            else:\n",
    "                raise\n",
    "                \n",
    "        # Download file\n",
    "        s3_client.download_file(Bucket=data_bucket, Key=data_key, Filename=local_file_path)\n",
    "        logger.info(f\"Downloaded file to: {local_file_path}\")\n",
    "        \n",
    "        # Handle zip files\n",
    "        if data_location.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(local_file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(\"/tmp/data\")\n",
    "                \n",
    "        # Load the specific file we just downloaded\n",
    "        try:\n",
    "            ext = os.path.splitext(filename)[1].lower()\n",
    "            if ext == '.csv':\n",
    "                df = pd.read_csv(local_file_path)\n",
    "            elif ext == '.json':\n",
    "                df = pd.read_json(local_file_path)\n",
    "            elif ext == '.parquet':\n",
    "                df = pd.read_parquet(local_file_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "                \n",
    "            if df is None or df.empty:\n",
    "                raise ValueError(f\"No data found in file: {filename}\")\n",
    "                \n",
    "            logger.info(f\"Successfully loaded data from {filename}\")\n",
    "            logger.info(f\"Data shape: {df.shape}\")\n",
    "            logger.info(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load data from {filename}: {str(e)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, ValueError):\n",
    "            raise\n",
    "        logger.error(f\"Error loading data: {str(e)}\", exc_info=True)\n",
    "        raise ValueError(f\"Failed to load data: {str(e)}\")\n",
    "\n",
    "def validate_data_location(data_location: str):\n",
    "    \"\"\"Validate that data_location is a valid S3 path and the file exists\"\"\"\n",
    "    if not data_location:\n",
    "        raise ValueError(\"DataLocation is required\")\n",
    "    \n",
    "    bucket, key = parse_s3_path(data_location)\n",
    "    if not bucket or not key:\n",
    "        raise ValueError(f\"Invalid data location format: {data_location}\")\n",
    "    \n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            raise ValueError(f\"Data file not found at location: {data_location}\")\n",
    "        raise\n",
    "\n",
    "def validate_model_location(model_location: str):\n",
    "    \"\"\"Validate that model_location is a valid S3 path, ends with model.zip, and is in the 'models' subdirectory\"\"\"\n",
    "    if not model_location:\n",
    "        raise ValueError(\"ModelLocation is required\")\n",
    "    \n",
    "    bucket, key = parse_s3_path(model_location)\n",
    "    if not bucket or not key:\n",
    "        raise ValueError(f\"Invalid model location format: {model_location}\")\n",
    "    \n",
    "    if not key.endswith('model.zip'):\n",
    "        raise ValueError(\"ModelLocation must end with 'model.zip'\")\n",
    "    \n",
    "    if 'models/' not in key:\n",
    "        raise ValueError(\"ModelLocation must be in the 'models' subdirectory\")\n",
    "\n",
    "def validate_result_data_location(result_data_location: str):\n",
    "    \"\"\"Validate that result_data_location ends with a file extension and is in the 'results' subdirectory\"\"\"\n",
    "    if not result_data_location:\n",
    "        raise ValueError(\"ResultDataLocation is required\")\n",
    "    \n",
    "    bucket, key = parse_s3_path(result_data_location)\n",
    "    if not bucket or not key:\n",
    "        raise ValueError(f\"Invalid result data location format: {result_data_location}\")\n",
    "    \n",
    "    # Check if the key has a file extension\n",
    "    if '.' not in os.path.basename(key):\n",
    "        raise ValueError(\"ResultDataLocation must end with a file extension\")\n",
    "    \n",
    "    # Check if the key is in the 'results' subdirectory\n",
    "    if 'results/' not in key:\n",
    "        raise ValueError(\"ResultDataLocation must be in the 'results' subdirectory\")\n",
    "\n",
    "def validate_model_exists(model_location: str):\n",
    "    \"\"\"Validate that model exists in S3\"\"\"\n",
    "    try:\n",
    "        bucket, key = parse_s3_path(model_location)\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            raise ValueError(f\"Model not found at location: {model_location}\")\n",
    "        raise\n",
    "\n",
    "def validate_target_column(df: pd.DataFrame, target: str):\n",
    "    \"\"\"Validate target column exists in dataframe\"\"\"\n",
    "    if target not in df.columns:\n",
    "        available_columns = list(df.columns)\n",
    "        raise ValueError(\n",
    "            f\"Target column '{target}' not found in data. \"\n",
    "            f\"Available columns: {available_columns}\"\n",
    "        )\n",
    "\n",
    "def train_test_split_dataset(df: pd.DataFrame, holdout_frac: float, data_location: str, target: str = None) -> APIResponse:\n",
    "    \"\"\"\n",
    "    Splits a dataset into training and testing sets and saves them to S3.\n",
    "    Ensures balanced distribution of target classes for classification or representative \n",
    "    distribution of target values for regression.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame to split\n",
    "        holdout_frac: Fraction of data to use for testing (between 0 and 1)\n",
    "        data_location: Original S3 location of the data\n",
    "        target: The target column to predict (optional)\n",
    "    Returns:\n",
    "        APIResponse: Object containing split results\n",
    "    \"\"\"\n",
    "    logger.info(f\"Splitting dataset with holdout fraction: {holdout_frac}\")\n",
    "    \n",
    "    try:\n",
    "        # Validate holdout fraction\n",
    "        if not 0 < holdout_frac < 1:\n",
    "            raise ValueError(f\"Holdout fraction must be between 0 and 1, got {holdout_frac}\")\n",
    "        \n",
    "        # If target column is provided, determine if it's classification or regression\n",
    "        if target and target in df.columns:\n",
    "            # Check data type and unique values to determine problem type\n",
    "            is_numeric = pd.api.types.is_numeric_dtype(df[target])\n",
    "            unique_count = df[target].nunique()\n",
    "            \n",
    "            # Heuristic: If numeric with many unique values (>10% of dataset size), \n",
    "            # likely regression, otherwise classification\n",
    "            is_regression = is_numeric and unique_count > max(10, len(df) * 0.1)\n",
    "            \n",
    "            if is_regression:\n",
    "                logger.info(f\"Target column '{target}' appears to be for a regression problem\")\n",
    "                logger.info(f\"Using binned stratification for regression target\")\n",
    "                \n",
    "                # For regression, bin the target into quantiles for stratified sampling\n",
    "                num_bins = min(10, unique_count)  # Use at most 10 bins\n",
    "                \n",
    "                # Create a temporary binned column for stratification\n",
    "                df['_temp_bin'] = pd.qcut(df[target], q=num_bins, duplicates='drop')\n",
    "                \n",
    "                # Log the bin distribution\n",
    "                bin_counts = df['_temp_bin'].value_counts().to_dict()\n",
    "                logger.info(f\"Target bin distribution: {bin_counts}\")\n",
    "                \n",
    "                # Perform stratified split on the bins\n",
    "                train_df = pd.DataFrame()\n",
    "                test_df = pd.DataFrame()\n",
    "                \n",
    "                # For each bin\n",
    "                for bin_value in df['_temp_bin'].unique():\n",
    "                    # Get all rows for this bin\n",
    "                    bin_df = df[df['_temp_bin'] == bin_value]\n",
    "                    \n",
    "                    # Shuffle the bin dataframe\n",
    "                    bin_df = bin_df.sample(frac=1.0, random_state=42)\n",
    "                    \n",
    "                    # Calculate split index for this bin\n",
    "                    bin_test_size = int(len(bin_df) * holdout_frac)\n",
    "                    \n",
    "                    # Split the bin dataframe\n",
    "                    bin_test_df = bin_df.iloc[:bin_test_size].copy()\n",
    "                    bin_train_df = bin_df.iloc[bin_test_size:].copy()\n",
    "                    \n",
    "                    # Append to the main dataframes\n",
    "                    test_df = pd.concat([test_df, bin_test_df])\n",
    "                    train_df = pd.concat([train_df, bin_train_df])\n",
    "                \n",
    "                # Remove the temporary bin column\n",
    "                train_df = train_df.drop(columns=['_temp_bin'])\n",
    "                test_df = test_df.drop(columns=['_temp_bin'])\n",
    "                \n",
    "                # Shuffle the final dataframes\n",
    "                train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "                test_df = test_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "                \n",
    "                # Log statistics about the target distribution in both sets\n",
    "                train_target_mean = train_df[target].mean()\n",
    "                test_target_mean = test_df[target].mean()\n",
    "                train_target_std = train_df[target].std()\n",
    "                test_target_std = test_df[target].std()\n",
    "                \n",
    "                logger.info(f\"Training set target mean: {train_target_mean}, std: {train_target_std}\")\n",
    "                logger.info(f\"Testing set target mean: {test_target_mean}, std: {test_target_std}\")\n",
    "                \n",
    "                # Calculate and log the distribution similarity\n",
    "                mean_diff_pct = abs((train_target_mean - test_target_mean) / train_target_mean) * 100 if train_target_mean != 0 else 0\n",
    "                std_diff_pct = abs((train_target_std - test_target_std) / train_target_std) * 100 if train_target_std != 0 else 0\n",
    "                \n",
    "                logger.info(f\"Target distribution difference: mean {mean_diff_pct:.2f}%, std {std_diff_pct:.2f}%\")\n",
    "                \n",
    "                # Store regression-specific metrics\n",
    "                regression_metrics = {\n",
    "                    'train_mean': float(train_target_mean),\n",
    "                    'test_mean': float(test_target_mean),\n",
    "                    'train_std': float(train_target_std),\n",
    "                    'test_std': float(test_target_std),\n",
    "                    'mean_difference_percent': float(mean_diff_pct),\n",
    "                    'std_difference_percent': float(std_diff_pct)\n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                # Classification problem\n",
    "                logger.info(f\"Target column '{target}' appears to be for a classification problem\")\n",
    "                logger.info(f\"Using stratified split for classification target\")\n",
    "                \n",
    "                # Check if target has sufficient classes\n",
    "                class_counts = df[target].value_counts()\n",
    "                \n",
    "                if unique_count < 2:\n",
    "                    logger.warning(f\"Target column '{target}' has only {unique_count} unique value. \"\n",
    "                                  f\"This may not be suitable for classification tasks.\")\n",
    "                \n",
    "                # Log class distribution\n",
    "                logger.info(f\"Target class distribution: {class_counts.to_dict()}\")\n",
    "                \n",
    "                # Check for rare classes (less than 10 samples)\n",
    "                rare_classes = class_counts[class_counts < 10].index.tolist()\n",
    "                if rare_classes:\n",
    "                    logger.warning(f\"Found rare classes with fewer than 10 samples: {rare_classes}\")\n",
    "                    logger.warning(\"Consider data augmentation or different sampling strategies\")\n",
    "                \n",
    "                # Custom implementation of stratified split\n",
    "                train_df = pd.DataFrame()\n",
    "                test_df = pd.DataFrame()\n",
    "                \n",
    "                # For each class in the target column\n",
    "                for class_value in df[target].unique():\n",
    "                    # Get all rows for this class\n",
    "                    class_df = df[df[target] == class_value]\n",
    "                    \n",
    "                    # Shuffle the class dataframe\n",
    "                    class_df = class_df.sample(frac=1.0, random_state=42)\n",
    "                    \n",
    "                    # Calculate split index for this class\n",
    "                    class_test_size = int(len(class_df) * holdout_frac)\n",
    "                    \n",
    "                    # Split the class dataframe\n",
    "                    class_test_df = class_df.iloc[:class_test_size].copy()\n",
    "                    class_train_df = class_df.iloc[class_test_size:].copy()\n",
    "                    \n",
    "                    # Append to the main dataframes\n",
    "                    test_df = pd.concat([test_df, class_test_df])\n",
    "                    train_df = pd.concat([train_df, class_train_df])\n",
    "                \n",
    "                # Shuffle the final dataframes\n",
    "                train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "                test_df = test_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "                \n",
    "                # Verify class distribution in splits\n",
    "                train_class_dist = train_df[target].value_counts().to_dict()\n",
    "                test_class_dist = test_df[target].value_counts().to_dict()\n",
    "                \n",
    "                logger.info(f\"Training set class distribution: {train_class_dist}\")\n",
    "                logger.info(f\"Testing set class distribution: {test_class_dist}\")\n",
    "                \n",
    "                # Check if all classes are represented in both splits\n",
    "                train_classes = set(train_df[target].unique())\n",
    "                test_classes = set(test_df[target].unique())\n",
    "                all_classes = set(df[target].unique())\n",
    "                \n",
    "                if train_classes != all_classes or test_classes != all_classes:\n",
    "                    logger.warning(\"Not all classes are represented in both splits!\")\n",
    "                    logger.warning(f\"Missing in train: {all_classes - train_classes}\")\n",
    "                    logger.warning(f\"Missing in test: {all_classes - test_classes}\")\n",
    "                \n",
    "                # Store classification-specific metrics\n",
    "                regression_metrics = None\n",
    "        else:\n",
    "            # If no target or target not in columns, use random split\n",
    "            logger.info(\"Using random split (no target column specified)\")\n",
    "            \n",
    "            # Shuffle the DataFrame\n",
    "            shuffled_df = df.sample(frac=1.0, random_state=42)\n",
    "            \n",
    "            # Calculate the split point\n",
    "            split_idx = int(len(shuffled_df) * (1 - holdout_frac))\n",
    "            \n",
    "            # Split the dataset\n",
    "            train_df = shuffled_df.iloc[:split_idx].copy()\n",
    "            test_df = shuffled_df.iloc[split_idx:].copy()\n",
    "            \n",
    "            # No regression metrics for random split\n",
    "            regression_metrics = None\n",
    "        \n",
    "        logger.info(f\"Split dataset into training set ({len(train_df)} rows) and testing set ({len(test_df)} rows)\")\n",
    "        \n",
    "        # Create temporary directory if it doesn't exist\n",
    "        os.makedirs(\"/tmp/data\", exist_ok=True)\n",
    "        \n",
    "        # Save the split datasets to temporary files\n",
    "        train_tmp_path = os.path.join(\"/tmp/data\", \"train_data.csv\")\n",
    "        test_tmp_path = os.path.join(\"/tmp/data\", \"test_data.csv\")\n",
    "        \n",
    "        # Save the split datasets to temporary files\n",
    "        train_df.to_csv(train_tmp_path, index=False)\n",
    "        test_df.to_csv(test_tmp_path, index=False)\n",
    "        \n",
    "        # Generate S3 paths for the split datasets\n",
    "        s3_base_path = os.path.dirname(data_location)\n",
    "        train_s3_path = f\"{s3_base_path}/train_data.csv\"\n",
    "        test_s3_path = f\"{s3_base_path}/test_data.csv\"\n",
    "        \n",
    "        # Upload the split datasets to S3\n",
    "        train_bucket, train_key = parse_s3_path(train_s3_path)\n",
    "        test_bucket, test_key = parse_s3_path(test_s3_path)\n",
    "        \n",
    "        s3_client.upload_file(train_tmp_path, train_bucket, train_key)\n",
    "        s3_client.upload_file(test_tmp_path, test_bucket, test_key)\n",
    "        \n",
    "        logger.info(f\"Uploaded training data to {train_s3_path}\")\n",
    "        logger.info(f\"Uploaded testing data to {test_s3_path}\")\n",
    "        \n",
    "        # Prepare results based on problem type\n",
    "        results = {\n",
    "            'training_data_location': train_s3_path,\n",
    "            'testing_data_location': test_s3_path,\n",
    "            'training_rows': len(train_df),\n",
    "            'testing_rows': len(test_df),\n",
    "            'holdout_fraction': holdout_frac,\n",
    "            'target_column': target if target and target in df.columns else None,\n",
    "        }\n",
    "        \n",
    "        # Add problem-specific metrics\n",
    "        if target and target in df.columns:\n",
    "            if is_regression:\n",
    "                results['problem_type'] = 'regression'\n",
    "                results['target_distribution'] = regression_metrics\n",
    "            else:\n",
    "                results['problem_type'] = 'classification'\n",
    "                results['target_distribution'] = {\n",
    "                    'training': train_df[target].value_counts().to_dict(),\n",
    "                    'testing': test_df[target].value_counts().to_dict()\n",
    "                }\n",
    "        \n",
    "        # Return the results using the correct APIResponse format\n",
    "        return APIResponse(\n",
    "            message=\"Dataset split successfully\",\n",
    "            results=results\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in train_test_split_dataset: {str(e)}\", exc_info=True)\n",
    "        raise ValueError(f\"Failed to split dataset: {str(e)}\")\n",
    "\n",
    "def exploratory_data_analysis(df: pd.DataFrame) -> APIResponse:\n",
    "    \"\"\"\n",
    "    Performs basic exploratory data analysis on a dataset from S3.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        APIResponse: Object containing EDA results\n",
    "    \"\"\"\n",
    "    logger.info(f\"Performing exploratory data analysis on dataset\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            raise ValueError(\"No data found or empty dataset\")\n",
    "            \n",
    "        # Get basic dataset information\n",
    "        num_rows, num_cols = df.shape\n",
    "        logger.info(f\"Dataset shape: {num_rows} rows, {num_cols} columns\")\n",
    "        memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)  # in MB\n",
    "        logger.info(f\"Memory usage: {memory_usage} MB\")\n",
    "        \n",
    "        # Data types analysis\n",
    "        dtypes_count = df.dtypes.value_counts().to_dict()\n",
    "        logger.info(f\"Data types count: {dtypes_count}\")\n",
    "        dtypes_by_column = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "        logger.info(f\"Data types by column: {dtypes_by_column}\")\n",
    "        \n",
    "        # Missing values analysis\n",
    "        missing_values = df.isnull().sum().to_dict()\n",
    "        missing_percentage = {col: round((count / num_rows) * 100, 2) \n",
    "                             for col, count in missing_values.items() if count > 0}\n",
    "        logger.info(f\"Missing values: {missing_values}\")\n",
    "        logger.info(f\"Missing percentage: {missing_percentage}\")\n",
    "        \n",
    "        # Numeric columns analysis\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        numeric_stats = {}\n",
    "        logger.info(f\"Numeric columns: {numeric_cols}\")\n",
    "        \n",
    "        if numeric_cols:\n",
    "            # Basic statistics for numeric columns\n",
    "            numeric_stats = {\n",
    "                'summary': df[numeric_cols].describe().to_dict(),\n",
    "                'skewness': {col: float(df[col].skew()) \n",
    "                            for col in numeric_cols if not df[col].isnull().all()},\n",
    "                'kurtosis': {col: float(df[col].kurtosis()) \n",
    "                            for col in numeric_cols if not df[col].isnull().all()}\n",
    "            }\n",
    "            logger.info(f\"Numeric stats: {numeric_stats}\")\n",
    "        # Categorical columns analysis\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        categorical_stats = {}\n",
    "        \n",
    "        if categorical_cols:\n",
    "            # For each categorical column, get value counts (limited to top 10)\n",
    "            categorical_stats = {\n",
    "                col: {\n",
    "                    'unique_count': df[col].nunique(),\n",
    "                    'top_values': df[col].value_counts().head(10).to_dict()\n",
    "                } for col in categorical_cols\n",
    "            }\n",
    "            logger.info(f\"Categorical stats: {categorical_stats}\")\n",
    "        \n",
    "        # Correlation analysis for numeric columns (if at least 2 exist)\n",
    "        correlation_matrix = {}\n",
    "        if len(numeric_cols) >= 2:\n",
    "            corr_matrix = df[numeric_cols].corr().round(2)\n",
    "            \n",
    "            # Find highly correlated features (absolute correlation > 0.7)\n",
    "            high_correlations = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i+1, len(corr_matrix.columns)):\n",
    "                    if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                        high_correlations.append({\n",
    "                            'feature1': corr_matrix.columns[i],\n",
    "                            'feature2': corr_matrix.columns[j],\n",
    "                            'correlation': float(corr_matrix.iloc[i, j])\n",
    "                        })\n",
    "            \n",
    "            # Convert NaN values to None for JSON serialization\n",
    "            corr_dict = corr_matrix.to_dict()\n",
    "            for col1 in corr_dict:\n",
    "                for col2 in corr_dict[col1]:\n",
    "                    if pd.isna(corr_dict[col1][col2]):\n",
    "                        corr_dict[col1][col2] = None\n",
    "            \n",
    "            correlation_matrix = {\n",
    "                'high_correlations': high_correlations,\n",
    "                'matrix': corr_dict\n",
    "            }\n",
    "            logger.info(f\"Correlation matrix: {correlation_matrix}\")\n",
    "        # Potential target column detection\n",
    "        potential_targets = []\n",
    "        for col in df.columns:\n",
    "            # Check if column name contains keywords often used for target variables\n",
    "            target_keywords = ['target', 'label', 'class', 'outcome', 'result', 'y']\n",
    "            if any(keyword in col.lower() for keyword in target_keywords):\n",
    "                potential_targets.append(col)\n",
    "            \n",
    "            # For categorical columns with few unique values\n",
    "            if col in categorical_cols and df[col].nunique() < 10:\n",
    "                potential_targets.append(col)\n",
    "        \n",
    "        # Compile all results\n",
    "        eda_results = {\n",
    "            'dataset_info': {\n",
    "                'rows': num_rows,\n",
    "                'columns': num_cols,\n",
    "                'memory_usage_mb': round(memory_usage, 2),\n",
    "                'column_names': df.columns.tolist()\n",
    "            },\n",
    "            'data_types': {\n",
    "                'summary': {str(k): int(v) for k, v in dtypes_count.items()},\n",
    "                'by_column': dtypes_by_column\n",
    "            },\n",
    "            'missing_values': {\n",
    "                'columns_with_missing': {k: int(v) for k, v in missing_values.items() if v > 0},\n",
    "                'missing_percentage': missing_percentage\n",
    "            },\n",
    "            'numeric_analysis': numeric_stats,\n",
    "            'categorical_analysis': categorical_stats,\n",
    "            'correlation_analysis': correlation_matrix,\n",
    "            'potential_target_columns': potential_targets,\n",
    "            'data_quality_issues': []\n",
    "        }\n",
    "        \n",
    "        # Identify potential data quality issues\n",
    "        data_quality_issues = []\n",
    "        \n",
    "        # Check for columns with high missing values (>20%)\n",
    "        high_missing = [col for col, pct in missing_percentage.items() if pct > 20]\n",
    "        if high_missing:\n",
    "            data_quality_issues.append({\n",
    "                'issue_type': 'high_missing_values',\n",
    "                'description': f\"Columns with >20% missing values: {', '.join(high_missing)}\"\n",
    "            })\n",
    "        \n",
    "        # Check for highly skewed numeric columns (skewness > 3 or < -3)\n",
    "        if numeric_cols:\n",
    "            high_skew = [col for col, skew in numeric_stats.get('skewness', {}).items() \n",
    "                        if abs(skew) > 3]\n",
    "            if high_skew:\n",
    "                data_quality_issues.append({\n",
    "                    'issue_type': 'high_skewness',\n",
    "                    'description': f\"Highly skewed columns: {', '.join(high_skew)}\"\n",
    "                })\n",
    "            logger.info(f\"Data quality issues: {data_quality_issues}\")\n",
    "        \n",
    "        # Check for highly correlated features\n",
    "        if len(correlation_matrix.get('high_correlations', [])) > 0:\n",
    "            corr_pairs = [f\"{item['feature1']}/{item['feature2']}\" \n",
    "                         for item in correlation_matrix.get('high_correlations', [])]\n",
    "            data_quality_issues.append({\n",
    "                'issue_type': 'high_correlation',\n",
    "                'description': f\"Highly correlated feature pairs: {', '.join(corr_pairs)}\"\n",
    "            })\n",
    "            logger.info(f\"Data quality issues: {data_quality_issues}\")\n",
    "        # Add data quality issues to results\n",
    "        eda_results['data_quality_issues'] = data_quality_issues\n",
    "        \n",
    "        # Log the EDA results\n",
    "        logger.info(f\"EDA results: {eda_results}\")\n",
    "        logger.info(f\"Data quality issues: {data_quality_issues}\")\n",
    "        \n",
    "        return APIResponse(\n",
    "            message=\"Exploratory data analysis completed successfully\",\n",
    "            results=eda_results\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in exploratory_data_analysis: {str(e)}\", exc_info=True)\n",
    "        raise ValueError(f\"Failed to perform exploratory data analysis: {str(e)}\")\n",
    "    \n",
    "def format_response(response_data: APIResponse, request_type: str, event: Dict) -> Dict:\n",
    "    \"\"\"Format and truncate response for the lambda\"\"\"\n",
    "    try:\n",
    "        # Convert response data to dict format\n",
    "        response_body = {\n",
    "            'application/json': {\n",
    "                'body': {\n",
    "                    'message': response_data.message,\n",
    "                    'results': response_data.results\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Convert to JSON string to check size\n",
    "        response_json = json.dumps(response_body)\n",
    "        response_size = len(response_json.encode('utf-8'))\n",
    "        MAX_RESPONSE_SIZE = 22 * 1024  # 22KB in bytes\n",
    "        \n",
    "        if response_size > MAX_RESPONSE_SIZE:\n",
    "            logger.warning(f\"Response size {response_size} bytes exceeds limit. Truncating content...\")\n",
    "            \n",
    "            # Create truncated response structure\n",
    "            truncated_response = {\n",
    "                'message': f\"Successfully completed {request_type} operation (results truncated)\",\n",
    "                'results': {\n",
    "                    'truncated': True,\n",
    "                    'original_size': response_size\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add essential information based on request type\n",
    "            if request_type == 'Train':\n",
    "                truncated_response['results'].update({\n",
    "                    'model_location': response_data.results.get('model_location'),\n",
    "                    'model_accuracy': response_data.results.get('model_accuracy')\n",
    "                })\n",
    "            elif request_type == 'Predict':\n",
    "                truncated_response['results'].update({\n",
    "                    'result_location': response_data.results.get('result_location'),\n",
    "                    'total_predictions': response_data.results.get('total_predictions')\n",
    "                })\n",
    "            \n",
    "            response_body['application/json']['body'] = truncated_response\n",
    "\n",
    "        return {\n",
    "            'messageVersion': '1.0',\n",
    "            'response': {\n",
    "                'actionGroup': event['actionGroup'],\n",
    "                'apiPath': event['apiPath'],\n",
    "                'httpMethod': event['httpMethod'],\n",
    "                'httpStatusCode': 200,\n",
    "                'responseBody': response_body\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error formatting response: {str(e)}\", exc_info=True)\n",
    "        return error_response(500, f\"Error formatting response: {str(e)}\", event)\n",
    "\n",
    "def error_response(status_code: int, error_message: str, event: Dict) -> Dict:\n",
    "    \"\"\"Generate standardized error response\"\"\"\n",
    "    return {\n",
    "        'messageVersion': '1.0',\n",
    "        'response': {\n",
    "            'actionGroup': event.get('actionGroup'),\n",
    "            'apiPath': event.get('apiPath'),\n",
    "            'httpMethod': event.get('httpMethod', 'POST'),\n",
    "            'httpStatusCode': status_code,\n",
    "            'responseBody': {\n",
    "                'application/json': {\n",
    "                    'body': {\n",
    "                        'error': error_message,\n",
    "                        'status': 'ERROR'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def clean_tmp_directories():\n",
    "    \"\"\"Clean up temporary directories\"\"\"\n",
    "    try:\n",
    "        # Clean up model directory\n",
    "        if os.path.exists('/tmp/model'):\n",
    "            for root, dirs, files in os.walk('/tmp/model', topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "                    \n",
    "        # Clean up data directory\n",
    "        if os.path.exists('/tmp/data'):\n",
    "            for root, dirs, files in os.walk('/tmp/data', topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error cleaning temporary directories: {str(e)}\")\n",
    "\n",
    "def normalize_all_paths(parameters):\n",
    "    \"\"\"Normalize all S3 paths in the parameters\"\"\"\n",
    "    model_location = parameters.model_location\n",
    "    data_location = parameters.data_location\n",
    "    train_data_location = parameters.train_data_location\n",
    "    test_data_location = parameters.test_data_location\n",
    "    result_data_location = parameters.result_data_location\n",
    "    \n",
    "    # Normalize paths\n",
    "    if model_location:\n",
    "        model_location = normalize_s3_path(model_location, S3_BUCKET_NAME)\n",
    "    if data_location:\n",
    "        data_location = normalize_s3_path(data_location, S3_BUCKET_NAME)\n",
    "    if train_data_location:\n",
    "        train_data_location = normalize_s3_path(train_data_location, S3_BUCKET_NAME)\n",
    "    if test_data_location:\n",
    "        test_data_location = normalize_s3_path(test_data_location, S3_BUCKET_NAME)\n",
    "    if result_data_location:\n",
    "        result_data_location = normalize_s3_path(result_data_location, S3_BUCKET_NAME)\n",
    "    \n",
    "    return {\n",
    "        'model_location': model_location,\n",
    "        'data_location': data_location,\n",
    "        'train_data_location': train_data_location,\n",
    "        'test_data_location': test_data_location,\n",
    "        'result_data_location': result_data_location\n",
    "    }\n",
    "\n",
    "def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:\n",
    "    try:\n",
    "        logger.info(f\"Received event: {json.dumps(event)}\")\n",
    "        \n",
    "        # Clean up temporary directories at start\n",
    "        clean_tmp_directories()\n",
    "        \n",
    "        # Parse parameters\n",
    "        parameters = parse_parameters(event)\n",
    "        target = parameters.target\n",
    "        api_path = parameters.api_path\n",
    "        holdout_frac = parameters.holdout_frac\n",
    "        hyperparameters = parameters.hyperparameters\n",
    "        \n",
    "        # Log the original parameters\n",
    "        logger.info(f\"Original parameters: {parameters}\")\n",
    "        \n",
    "        # Normalize all paths\n",
    "        normalized_paths = normalize_all_paths(parameters)\n",
    "        model_location = normalized_paths['model_location']\n",
    "        data_location = normalized_paths['data_location']\n",
    "        train_data_location = normalized_paths['train_data_location']\n",
    "        test_data_location = normalized_paths['test_data_location']\n",
    "        result_data_location = normalized_paths['result_data_location']\n",
    "        \n",
    "        # Log the normalized paths\n",
    "        logger.info(f\"Normalized paths: {normalized_paths}\")\n",
    "        \n",
    "        if api_path == 'Train':\n",
    "            \n",
    "            if not all([target, model_location, train_data_location, test_data_location]):\n",
    "                raise ValueError(\"Missing required parameters. Need Target, ModelLocation, TrainDataLocation, TestDataLocation.\")\n",
    "            \n",
    "            # Validate paths\n",
    "            validate_model_location(model_location)\n",
    "            validate_data_location(train_data_location)\n",
    "            validate_data_location(test_data_location)\n",
    "            \n",
    "            # Load data\n",
    "            train_df = load_data(train_data_location)\n",
    "            test_df = load_data(test_data_location)\n",
    "\n",
    "            # Validate target column exists\n",
    "            validate_target_column(train_df, target)\n",
    "            validate_target_column(test_df, target)\n",
    "                \n",
    "            # Validate target column has enough classes\n",
    "            unique_classes = train_df[target].nunique()\n",
    "            if unique_classes < 2:\n",
    "                raise ValueError(\n",
    "                    f\"Target column '{target}' has only {unique_classes} unique value(s). \"\n",
    "                    \"For classification, at least 2 different classes are required. \"\n",
    "                    f\"Unique values found: {train_df[target].unique().tolist()}\"\n",
    "                )\n",
    "\n",
    "            # Train model\n",
    "            try:\n",
    "                result = train(train_df, test_df, target, model_location, hyperparameters)\n",
    "                # Use the format_response function to properly serialize the APIResponse object\n",
    "                return format_response(result, \"Train\", event)\n",
    "            except AssertionError as e:\n",
    "                # Handle AutoGluon specific errors\n",
    "                if \"num_classes must be\" in str(e):\n",
    "                    raise ValueError(\n",
    "                        f\"Training failed: Target column has insufficient classes. {str(e)}. \"\n",
    "                        f\"Check your target column '{target}' for data quality issues.\"\n",
    "                    )\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Training failed: {str(e)}\")\n",
    "        elif api_path == 'Predict':\n",
    "            if not all([model_location, data_location, result_data_location]):\n",
    "                raise ValueError(\"Missing required parameters. Need ModelLocation, DataLocation, and ResultDataLocation.\")\n",
    "            \n",
    "            # Validate paths\n",
    "            validate_model_location(model_location)\n",
    "            validate_data_location(data_location)\n",
    "            validate_result_data_location(result_data_location)\n",
    "            \n",
    "            # Load data\n",
    "            df = load_data(data_location)\n",
    "\n",
    "            # Remove target column from dataframe\n",
    "            df = df.drop(columns=[target])\n",
    "\n",
    "            # Predict\n",
    "            try:\n",
    "                result = predict(df, model_location, result_data_location)\n",
    "                # Use the format_response function to properly serialize the APIResponse object\n",
    "                return format_response(result, \"Predict\", event)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Prediction failed: {str(e)}\")\n",
    "            \n",
    "        elif api_path == 'FeatureImportance':\n",
    "            if not all([model_location, data_location]):\n",
    "                raise ValueError(\"Missing required parameters. Need ModelLocation and DataLocation.\")\n",
    "                \n",
    "            # Validate paths\n",
    "            validate_model_location(model_location)\n",
    "            validate_data_location(data_location)\n",
    "            \n",
    "            # Load data\n",
    "            df = load_data(data_location)\n",
    "\n",
    "            # Get feature importance\n",
    "            try:\n",
    "                result = feature_importance(df, model_location)\n",
    "                # Use the format_response function to properly serialize the APIResponse object\n",
    "                return format_response(result, \"FeatureImportance\", event)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Feature importance calculation failed: {str(e)}\")\n",
    "        \n",
    "        elif api_path == 'ExploratoryDataAnalysis':\n",
    "            # For EDA, we only need the data location\n",
    "            if not data_location:\n",
    "                raise ValueError(\"Missing required parameter: DataLocation\")\n",
    "\n",
    "            # Validate data location\n",
    "            validate_data_location(data_location)\n",
    "\n",
    "            # Load data\n",
    "            df = load_data(data_location)\n",
    "            \n",
    "            # Perform exploratory data analysis\n",
    "            result = exploratory_data_analysis(df)\n",
    "\n",
    "            # Return the results\n",
    "            return format_response(result, \"ExploratoryDataAnalysis\", event)\n",
    "            \n",
    "        elif api_path == 'TrainTestSplit':\n",
    "            \n",
    "            if not all([holdout_frac]):\n",
    "                raise ValueError(\"Missing required parameter: HoldoutFrac\")\n",
    "            \n",
    "            if not data_location:\n",
    "                raise ValueError(\"Missing required parameter: DataLocation\")\n",
    "\n",
    "            # Validate data location\n",
    "            validate_data_location(data_location)\n",
    "\n",
    "            # Load data\n",
    "            df = load_data(data_location)\n",
    "            \n",
    "            # Train test split\n",
    "            result = train_test_split_dataset(df, holdout_frac, data_location, target)\n",
    "\n",
    "            # Return the results\n",
    "            return format_response(result, \"TrainTestSplit\", event)\n",
    "            \n",
    "            \n",
    "    except ValueError as e:\n",
    "        # Handle validation errors (400)\n",
    "        error_details = {\n",
    "            \"error\": str(e),\n",
    "            \"error_type\": \"ValidationError\",\n",
    "            \"parameters\": parameters.dict() if 'parameters' in locals() else None,  # Convert Pydantic model to dict\n",
    "            \"data_info\": {\n",
    "                \"shape\": df.shape if 'df' in locals() else None,\n",
    "                \"columns\": df.columns.tolist() if 'df' in locals() else None,\n",
    "                \"target_info\": {\n",
    "                    \"unique_values\": df[target].unique().tolist() if all(v in locals() for v in ['df', 'target']) else None,\n",
    "                    \"value_counts\": df[target].value_counts().to_dict() if all(v in locals() for v in ['df', 'target']) else None\n",
    "                } if all(v in locals() for v in ['df', 'target']) else None\n",
    "            } if 'df' in locals() else None\n",
    "        }\n",
    "        \n",
    "        logger.error(f\"Validation error: {json.dumps(error_details, indent=2)}\")\n",
    "        \n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event.get('actionGroup'),\n",
    "                \"apiPath\": event.get('apiPath'),\n",
    "                \"httpMethod\": event.get('httpMethod', 'POST'),\n",
    "                \"httpStatusCode\": 400,\n",
    "                \"responseBody\": {\n",
    "                    \"application/json\": {\n",
    "                        \"body\": error_details\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle unexpected errors (500)\n",
    "        error_details = {\n",
    "            \"error\": str(e),\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"traceback\": traceback.format_exc()\n",
    "        }\n",
    "        \n",
    "        logger.error(f\"Internal error: {json.dumps(error_details, indent=2)}\")\n",
    "        \n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event.get('actionGroup'),\n",
    "                \"apiPath\": event.get('apiPath'),\n",
    "                \"httpMethod\": event.get('httpMethod', 'POST'),\n",
    "                \"httpStatusCode\": 500,\n",
    "                \"responseBody\": {\n",
    "                    \"application/json\": {\n",
    "                        \"body\": error_details\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    finally:\n",
    "        # Clean up temporary directories at end\n",
    "        clean_tmp_directories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../datascientist/datascientist.Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.11\n",
    "\n",
    "# Install build dependencies first\n",
    "RUN yum install libgomp git gcc gcc-c++ make -y \\\n",
    " && yum clean all -y && rm -rf /var/cache/yum\n",
    "\n",
    "ARG TORCH_VER=2.1.2\n",
    "ARG TORCH_VISION_VER=0.16.2\n",
    "ARG NUMPY_VER=1.24.3\n",
    "RUN python3 -m pip --no-cache-dir install --upgrade --trusted-host pypi.org --trusted-host files.pythonhosted.org pip \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade wheel setuptools \\\n",
    " && python3 -m pip uninstall -y dataclasses \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade torch==\"${TORCH_VER}\" torchvision==\"${TORCH_VISION_VER}\" -f https://download.pytorch.org/whl/torch_stable.html \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade numpy==${NUMPY_VER} \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pandas \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade autogluon.tabular[all] \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade boto3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pydantic\n",
    "\n",
    "# Copy function code\n",
    "WORKDIR /var/task\n",
    "COPY ../datascientist/bedrock_data_scientist_agent.py .\n",
    "COPY ../notebooks/utils/ utils/\n",
    "\n",
    "# Set handler environment variable\n",
    "ENV _HANDLER=\"bedrock_data_scientist_agent.lambda_handler\"\n",
    "\n",
    "# Let's go back to using the default entrypoint\n",
    "ENTRYPOINT [ \"/lambda-entrypoint.sh\" ]\n",
    "CMD [ \"bedrock_data_scientist_agent.lambda_handler\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run local docker container to test the dataengineer-lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run local docker container\n",
    "!docker build -t datascientist-lambda -f ../datascientist/datascientist.Dockerfile .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run with tailing log\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "!docker run -d \\\n",
    "-e AWS_ACCESS_KEY_ID={credentials.access_key} \\\n",
    "-e AWS_SECRET_ACCESS_KEY={credentials.secret_key} \\\n",
    "-e AWS_SESSION_TOKEN={credentials.token} \\\n",
    "-e AWS_DEFAULT_REGION={REGION} \\\n",
    "-e REGION={REGION} \\\n",
    "-e AWS_LAMBDA_FUNCTION_TIMEOUT=900 \\\n",
    "-e S3_BUCKET_NAME={S3_BUCKET_NAME} \\\n",
    "-p 9000:8080 datascientist-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --filter ancestor=datascientist-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model in container\n",
    "\n",
    "# sample request structure:\n",
    "request_body = {\n",
    "    \"apiPath\": \"/Train\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"ModelLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"/models/model.zip\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Target\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": \"is_churned\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/customer_churn_prediction_dataset.csv\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"HoldoutFrac\",\n",
    "                        \"type\": \"number\",\n",
    "                        \"value\": 0.2\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataScientistActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with model in container\n",
    "request_body = {\n",
    "    \"apiPath\": \"/Predict\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"ModelLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"/models/customer_churn/model.zip\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Target\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": \"is_churned\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/ml_datasets/customer_churn_prediction_dataset.csv\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"ResultDataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"/results/churn_predictions.csv\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataScientistActions\",\n",
    "}\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance from model in container\n",
    "request_body = {\n",
    "    \"apiPath\": \"/FeatureImportance\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"ModelLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"/models/customer_churn/model.zip\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Target\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": \"is_churned\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/ml_datasets/customer_churn_prediction_dataset.csv\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataScientistActions\"\n",
    "}\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exploratory data analysis in container\n",
    "request_body = {\n",
    "    \"apiPath\": \"/ExploratoryDataAnalysis\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/ml_datasets/customer_churn_prediction_dataset.csv\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataScientistActions\"\n",
    "}\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the container\n",
    "!docker stop $(docker ps -q --filter ancestor=datascientist-lambda)\n",
    "!docker ps --filter ancestor=datascientist-lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload docker image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ECR repository for datascientist-lambda (if not already created in 1_environmentSetup.ipynb)\n",
    "#!aws ecr create-repository --repository-name automatedinsights/lambda_datascientist --region {REGION} --profile {SESSION_PROFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload docker image to ECR\n",
    "!aws ecr get-login-password --region {REGION} --profile {SESSION_PROFILE} | docker login --username AWS --password-stdin {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com\n",
    "!docker tag datascientist-lambda:latest {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_datascientist:latest\n",
    "!docker push {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_datascientist:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Test Bedrock Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "from utils.bedrock_agent import BedrockAgentScenarioWrapper\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "agent_name = \"DataScientist\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "instruction = \"\"\"## ROLE\n",
    "Expert Data Scientist specializing in AutoML workflows for data analysis, model training, and prediction generation.\n",
    "\n",
    "## CAPABILITIES\n",
    "\n",
    "### 1. Exploratory Data Analysis\n",
    "```\n",
    "ExploratoryDataAnalysis(\n",
    "    APIPath: \"ExploratoryDataAnalysis\",\n",
    "    DataLocation: [path_to_data]\n",
    ") \n",
    "Returns: {\n",
    "    summary: string,\n",
    "    description: string\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Data Splitting\n",
    "```\n",
    "TrainTestSplit(\n",
    "    APIPath: \"TrainTestSplit\",\n",
    "    DataLocation: [path_to_data],\n",
    "    Target: [target_variable],\n",
    "    HoldoutFrac: [fraction_of_data_to_holdout_for_testing]\n",
    ") \n",
    "Returns: {\n",
    "    train_data_location: string,\n",
    "    test_data_location: string\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Model Training\n",
    "```\n",
    "Train(\n",
    "    APIPath: \"Train\",\n",
    "    Target: [target_variable],\n",
    "    TrainDataLocation: [train_data_location],\n",
    "    TestDataLocation: [test_data_location],\n",
    "    ModelLocation: [path_to_store_model]\n",
    ") \n",
    "Returns: {\n",
    "    model_location: string,\n",
    "    accuracy_metrics: object,\n",
    "    feature_importance: object\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. Prediction Generation\n",
    "```\n",
    "Predict(\n",
    "    APIPath: \"Predict\",\n",
    "    DataLocation: [path_to_data],\n",
    "    ModelLocation: [path_to_model],\n",
    "    ResultDataLocation: [path_to_store_predictions_data]\n",
    ") \n",
    "Returns: {\n",
    "    sample_predictions: object,\n",
    "    predictions_location: string\n",
    "}\n",
    "```\n",
    "\n",
    "## WORKFLOW\n",
    "\n",
    "### Initial Validation\n",
    "1. Verify receipt of ML dataset location and target column name\n",
    "2. If only SQL query is provided, inform user: \"I don't have access to Athena. Please provide the dataset stored in S3.\"\n",
    "\n",
    "### Analysis Phase\n",
    "1. Run `ExploratoryDataAnalysis()` on the provided dataset\n",
    "2. Analyze results to determine:\n",
    "   - Data quality and quantity\n",
    "   - Determine appropriate test split ratio (HoldoutFrac)\n",
    "\n",
    "### Training Workflow\n",
    "1. Execute `TrainTestSplit()` with the determined HoldoutFrac\n",
    "2. Construct ModelLocation following the pattern: `/models/{use_case}/model.zip`\n",
    "3. Execute `Train()` function with required parameters\n",
    "4. Return comprehensive results:\n",
    "   - Model location\n",
    "   - Training dataset location\n",
    "   - Test dataset location\n",
    "   - Complete feature importance values\n",
    "   - Key accuracy metrics\n",
    "\n",
    "### Prediction Workflow\n",
    "1. Verify model exists at specified ModelLocation\n",
    "2. Construct ResultDataLocation following the pattern: `/results/{use_case}_{timestamp}.csv`\n",
    "3. Execute `Predict()` function\n",
    "4. Return:\n",
    "   - Sample predictions (first 5-10 rows)\n",
    "   - Full predictions dataset location\n",
    "\n",
    "## ERROR HANDLING\n",
    "\n",
    "### Parameter Validation\n",
    "- Verify all required parameters before function execution\n",
    "- Validate path formats and file extensions\n",
    "\n",
    "### Data Quality Checks\n",
    "- Minimum data quantity thresholds for reliable model training\n",
    "- Missing value percentage warnings\n",
    "- Feature cardinality and distribution alerts\n",
    "\n",
    "### Model Validation\n",
    "- Compatibility checks between model and prediction data\n",
    "- Performance threshold warnings\n",
    "- Feature drift detection\n",
    "\n",
    "## STORAGE CONVENTIONS\n",
    "- All models must be stored in `/models/{use_case}/` with filename `model.zip`\n",
    "- All predictions must be stored in `/results/` with descriptive filenames and `.csv` extension\n",
    "- Maintain consistent naming conventions across all artifacts (lower case, no spaces)\n",
    "\n",
    "### FINAL RESPONSE\n",
    "- Ensure the final response contains the following information:\n",
    "  - Model location\n",
    "  - Training dataset location\n",
    "  - Test dataset location\n",
    "  - Complete feature importance values\n",
    "  - All accuracy metrics\n",
    "  - Summary of the analysis\n",
    "- Format the final response in XML as shown in the example response below:\n",
    "    <ModelDetails>\n",
    "        <TargetColumn>\n",
    "            <Name>target_column_name</Name>\n",
    "            <Definition>Clear definition of what the target represents</Definition>\n",
    "        </TargetColumn>\n",
    "        <TrainingDataLocation>Full path to training data</TrainingDataLocation>\n",
    "        <TestDataLocation>Full path to test data</TestDataLocation>\n",
    "        <ModelLocation>Full path to stored model</ModelLocation>\n",
    "        <MLDatasetLocation>Full path to complete ML dataset</MLDatasetLocation>\n",
    "        <MLDatasetSQLQuery>Complete SQL query used</MLDatasetSQLQuery>\n",
    "        <Accuracy>Numerical accuracy metric</Accuracy>\n",
    "        <FeatureImportances>\n",
    "            <!-- Detailed feature importance list -->\n",
    "        </FeatureImportances>\n",
    "        <Sample-PredictedValues>\n",
    "            <!-- Representative sample of predictions -->\n",
    "        </Sample-PredictedValues>\n",
    "    </ModelDetails>\n",
    "    <PredictionDataLocation>Full path to predictions file</PredictionDataLocation>\n",
    "    <DataScientistCommentary>Expert analysis of model performance and limitations</DataScientistCommentary>\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Train a new model with the below details.\"\n",
    "\n",
    "request_body = json.dumps({\n",
    "                    \"parameters\": [\n",
    "                                    {\n",
    "                                        \"name\": \"Target\",\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"value\": \"is_churned\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"DataLocation\",\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/customer_churn_prediction_dataset.csv\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"ModelLocation\",\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"value\": f\"s3://{S3_BUCKET_NAME}/models/model.zip\"\n",
    "                                    }\n",
    "                                    \n",
    "                    ]\n",
    "  \n",
    "}, ensure_ascii=False)\n",
    "\n",
    "prompt = prompt + str(request_body)\n",
    "\n",
    "\n",
    "postfix = \"\".join(\n",
    "    random.choice(string.ascii_lowercase + \"0123456789\") for _ in range(8)\n",
    ")\n",
    "\n",
    "agent_name = agent_name + \"_\" + postfix\n",
    "\n",
    "IMAGE_URI = f'{AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_datascientist:latest'\n",
    "\n",
    "agentCollaboration = 'DISABLED' #'SUPERVISOR' #|'SUPERVISOR_ROUTER'|'DISABLED'\n",
    "\n",
    "sub_agents_list = []\n",
    "promptOverrideConfiguration = None\n",
    "\n",
    "\n",
    "lambda_environment_variables = {\n",
    "    \"S3_BUCKET_NAME\": S3_BUCKET_NAME,\n",
    "    \"MODEL_ID\": MODEL_ID\n",
    "}\n",
    "\n",
    "scenario = BedrockAgentScenarioWrapper(\n",
    "    bedrock_agent_client=bedrock_agent_client,\n",
    "    runtime_client=bedrock_agent_runtime_client,\n",
    "    lambda_client=lambda_client,\n",
    "    iam_resource=iam_resource,\n",
    "    postfix=postfix,\n",
    "    agent_name=agent_name,\n",
    "    model_id=MODEL_ID,\n",
    "    sub_agents_list=sub_agents_list,\n",
    "    prompt=prompt,\n",
    "    lambda_image_uri=IMAGE_URI,\n",
    "    lambda_environment_variables=lambda_environment_variables,\n",
    "    action_group_schema_path=\"./action_groups/datascientist_open_api_schema.yml\",\n",
    "    instruction=instruction,\n",
    "    agentCollaboration=agentCollaboration,\n",
    "    promptOverrideConfiguration=promptOverrideConfiguration\n",
    ")\n",
    "try:\n",
    "    scenario.run_scenario()\n",
    "except Exception as e:\n",
    "    logging.exception(f\"Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Train a new model with the below details.\"\n",
    "\n",
    "# request_body = json.dumps({\n",
    "#                     \"parameters\": [\n",
    "#                                     {\n",
    "#                                         \"name\": \"Target\",\n",
    "#                                         \"type\": \"string\",\n",
    "#                                         \"value\": \"is_churned\"\n",
    "#                                     },\n",
    "#                                     {\n",
    "#                                         \"name\": \"DataLocation\",\n",
    "#                                         \"type\": \"string\",\n",
    "#                                         \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/customer_churn_prediction_dataset.csv\"\n",
    "#                                     },\n",
    "#                                     {\n",
    "#                                         \"name\": \"ModelLocation\",\n",
    "#                                         \"type\": \"string\",\n",
    "#                                         \"value\": f\"s3://{S3_BUCKET_NAME}/models/model.zip\"\n",
    "#                                     },\n",
    "                                    \n",
    "#                     ]\n",
    "  \n",
    "# }, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# prompt = prompt + str(request_body)\n",
    "# prompt\n",
    "# # scenario.prompt = prompt \n",
    "# # scenario.chat_with_agent()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =scenario.agent\n",
    "\n",
    "AGENT_ID = agent.get('agentId')\n",
    "\n",
    "# get agent alias id\n",
    "agent_aliases = bedrock_agent_client.list_agent_aliases(agentId= AGENT_ID)\n",
    "\n",
    "AGENT_ALIAS_ID =  agent_aliases.get('agentAliasSummaries')[0].get('agentAliasId')\n",
    "print(f\"AGENT_ID: {AGENT_ID}\")\n",
    "print(f\"AGENT_ALIAS_ID: {AGENT_ALIAS_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save agent config to json file for evaluation\n",
    "agent_config = {\n",
    "    \"agent_id\": AGENT_ID,\n",
    "    \"agent_alias_id\": AGENT_ALIAS_ID,\n",
    "    \"human_id\": \"User\",\n",
    "    \"agent_name\": \"DataScientist\",\n",
    "    \"agent_instruction\": instruction,\n",
    "    \"tools\": [\n",
    "    {\n",
    "        \"tool_name\": \"DataScientistAPI\",\n",
    "        \"name\": \"DataScientistAPI\",\n",
    "        \"description\": \"AutoML to train a ML model, use a ML model to make predictions, or get feature importance\",\n",
    "        \"actions\": [\n",
    "            {\n",
    "                \"name\": \"Train\",\n",
    "                \"description\": \"Train a machine learning model\",\n",
    "                \"input_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Target\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"Target\",\n",
    "                            \"description\": \"The target column to predict\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"DataLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"DataLocation\",\n",
    "                            \"description\": \"S3 location of the data to train the model on\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"ModelLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"ModelLocation\",\n",
    "                            \"description\": \"S3 location to store the model\",\n",
    "                            \"required\": []\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"Target\",\n",
    "                        \"DataLocation\",\n",
    "                        \"ModelLocation\"\n",
    "                    ]\n",
    "                },\n",
    "                \"output_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"title\": \"200\",\n",
    "                    \"description\": \"Successful operation\",\n",
    "                    \"properties\": {\n",
    "                        \"description\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"message\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                },\n",
    "                                \"results\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                },\n",
    "                \"requires_confirmation\": False,\n",
    "                \"meta\": {}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Predict\",\n",
    "                \"description\": \"Make predictions using a trained model\",\n",
    "                \"input_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Target\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"Target\",\n",
    "                            \"description\": \"The target column to predict\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"DataLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"DataLocation\",\n",
    "                            \"description\": \"S3 location of the input data for the prediction\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"ResultDataLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"ResultDataLocation\",\n",
    "                            \"description\": \"S3 location of the output data for the prediction\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"ModelLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"ModelLocation\",\n",
    "                            \"description\": \"S3 location of the trained model that is used for the predictions\",\n",
    "                            \"required\": []\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"Target\",\n",
    "                        \"DataLocation\",\n",
    "                        \"ModelLocation\",\n",
    "                        \"ResultDataLocation\"\n",
    "                    ]\n",
    "                },\n",
    "                \"output_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"title\": \"200\",\n",
    "                    \"description\": \"Successful operation\",\n",
    "                    \"properties\": {\n",
    "                        \"description\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"message\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                },\n",
    "                                \"results\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                },\n",
    "                \"requires_confirmation\": False,\n",
    "                \"meta\": {}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FeatureImportance\",\n",
    "                \"description\": \"Get feature importance from a trained model\",\n",
    "                \"input_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Target\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"Target\",\n",
    "                            \"description\": \"The target column to predict\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"DataLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"DataLocation\",\n",
    "                            \"description\": \"S3 location of the data\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"ModelLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"ModelLocation\",\n",
    "                            \"description\": \"S3 location of the trained model that is used to get the feature importance\",\n",
    "                            \"required\": []\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"Target\",\n",
    "                        \"DataLocation\",\n",
    "                        \"ModelLocation\"\n",
    "                    ]\n",
    "                },\n",
    "                \"output_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"title\": \"200\",\n",
    "                    \"description\": \"Successful operation\",\n",
    "                    \"properties\": {\n",
    "                        \"description\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"message\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                },\n",
    "                                \"results\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                },\n",
    "                \"requires_confirmation\": False,\n",
    "                \"meta\": {}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ExploratoryDataAnalysis\",\n",
    "                \"description\": \"Perform exploratory data analysis on a dataset\",\n",
    "                \"input_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"DataLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"DataLocation\",\n",
    "                            \"description\": \"S3 location of the data\",\n",
    "                            \"required\": []\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"DataLocation\"\n",
    "                    ]\n",
    "                },\n",
    "                \"output_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"title\": \"200\",\n",
    "                    \"description\": \"Successful operation\",\n",
    "                    \"properties\": {\n",
    "                        \"description\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"message\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                },\n",
    "                                \"results\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                },\n",
    "                \"requires_confirmation\": False,\n",
    "                \"meta\": {}\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"TrainTestSplit\",\n",
    "                \"description\": \"Perform train test split on a dataset\",\n",
    "                \"input_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"DataLocation\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"title\": \"DataLocation\",\n",
    "                            \"description\": \"S3 location of the data\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"HoldoutFrac\": {\n",
    "                            \"data_type\": \"number\",\n",
    "                            \"title\": \"HoldoutFrac\",\n",
    "                            \"description\": \"Fraction of data to hold out for testing\",\n",
    "                            \"required\": []\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"DataLocation\",\n",
    "                        \"HoldoutFrac\"\n",
    "                    ]\n",
    "                },\n",
    "                \"output_schema\": {\n",
    "                    \"data_type\": \"object\",\n",
    "                    \"title\": \"200\",\n",
    "                    \"description\": \"Successful operation\",\n",
    "                    \"properties\": {\n",
    "                        \"description\": {\n",
    "                            \"data_type\": \"string\",\n",
    "                            \"required\": []\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"data_type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"message\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                },\n",
    "                                \"results\": {\n",
    "                                    \"data_type\": \"string\",\n",
    "                                    \"required\": []\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                },\n",
    "                \"requires_confirmation\": False,\n",
    "                \"meta\": {}\n",
    "            }\n",
    "        ],\n",
    "        \"tool_type\": \"Module\",\n",
    "        \"meta\": {}\n",
    "    }\n",
    "],\n",
    "            \"reachable_agents\": []\n",
    "}\n",
    "# save agent config to json file\n",
    "with open('../datascientist/agent.json', 'w') as f:\n",
    "    json.dump(agent_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define different evaluation scenarios\n",
    "evaluation_scenarios = {\n",
    "    \"scenarios\": [\n",
    "        {\n",
    "            \"scenario\": \"Train\",\n",
    "            \"input_problem\": (\n",
    "                'Train a new model with the following details. Return the final model location and the feature importance in the final response. '\n",
    "                f'[{{\"name\": \"DataLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/uploads/customer_churn_prediction_dataset.csv\"}},'\n",
    "                f'[{{\"name\": \"ModelLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/models/model.zip\"}},'\n",
    "                f'{{\"name\": \"Target\", \"type\": \"string\", \"value\": \"is_churned\"}}]'\n",
    "            ),     \n",
    "            \"assertions\": [\n",
    "                \"agent: Run exploratory data analysis on the data\",\n",
    "                \"agent: Determine the right HoldoutFrac for the ml model training\",\n",
    "                \"agent: Split the data into train and test sets\",\n",
    "                \"agent: Train a new model with the specified details\",\n",
    "                \"agent: The model is trained successfully\",\n",
    "                \"agent: The model location and feature importance are returned in the final response\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Predict\", \n",
    "            \"input_problem\": (\n",
    "                'Generate predictions based on the following details. Return the prediction results and the location of the full results in the final response. '\n",
    "                f'[{{\"name\": \"DataLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/uploads/customer_churn_prediction_dataset.csv\"}},'\n",
    "                f'[{{\"name\": \"ModelLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/models/model.zip\"}},'\n",
    "                f'{{\"name\": \"ResultDataLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/results/churn_predictions.csv\"}},'\n",
    "                f'{{\"name\": \"Target\", \"type\": \"string\", \"value\": \"is_churned\"}}]'\n",
    "            ),\n",
    "            \"assertions\": [\n",
    "                \"agent: Generate predictions based on the specified details\",\n",
    "                \"agent: A sample of the prediction results along with the location of the full results is returned in the final response\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"FeatureImportance\",\n",
    "            \"input_problem\": (\n",
    "                'Get the feature importance of the trained model based on the following details. Return the feature importance in the final response. '\n",
    "                f'[{{\"name\": \"DataLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/uploads/customer_churn_prediction_dataset.csv\"}},'\n",
    "                f'[{{\"name\": \"ModelLocation\", \"type\": \"string\", \"value\": \"s3://{S3_BUCKET_NAME}/models/model.zip\"}},'\n",
    "                f'{{\"name\": \"Target\", \"type\": \"string\", \"value\": \"is_churned\"}}]'\n",
    "            ),\n",
    "            \"assertions\": [\n",
    "                \"agent: Get the feature importance of the trained model based on the specified details\",\n",
    "                \"agent: The feature importance is returned in the final response\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "# save evaluation scenarios to json file\n",
    "with open('../datascientist/scenarios.json', 'w') as f:\n",
    "    json.dump(evaluation_scenarios, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent evaluation\n",
    "from utils.benchmark import run_agent_evaluation\n",
    "\n",
    "dataset_dir = \"../datascientist\"\n",
    "results = run_agent_evaluation(\n",
    "    scenario_filepath = f\"{dataset_dir}/scenarios.json\",\n",
    "    agent_filepath = f\"{dataset_dir}/agent.json\",\n",
    "    llm_judge_id = DATASCIENTIST_AGENT_EVAL_PROFILE_ARN,\n",
    "    region = REGION,\n",
    "    session = session\n",
    ")\n",
    "\n",
    "# Check if results is not None before proceeding\n",
    "if results is not None:\n",
    "    # Create high-level metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'user_gsr': [results['user_gsr']],\n",
    "        'system_gsr': [results['system_gsr']],\n",
    "        'overall_gsr': [results['overall_gsr']],\n",
    "        'partial_gsr': [results['partial_gsr']],\n",
    "        'scenario_count': [results['scenario_count']],\n",
    "        'conversation_count': [results['conversation_count']]\n",
    "    })\n",
    "\n",
    "    # Create detailed assertions DataFrame\n",
    "    assertions_list = []\n",
    "    for eval_result in results['conversation_evals']:\n",
    "        trajectory_index = eval_result['trajectory_index']\n",
    "        for assertion in eval_result['report']:\n",
    "            assertions_list.append({\n",
    "                'trajectory_index': trajectory_index,\n",
    "                'assertion_type': assertion['assertion_type'],\n",
    "                'assertion': assertion['assertion'],\n",
    "                'answer': assertion['answer'],\n",
    "                'evidence': assertion['evidence']\n",
    "            })\n",
    "\n",
    "    assertions_df = pd.DataFrame(assertions_list)\n",
    "\n",
    "    # Display results\n",
    "    print(\"High-level Metrics:\")\n",
    "    display(metrics_df)\n",
    "\n",
    "    print(\"\\nDetailed Assertions:\")\n",
    "    display(assertions_df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: Please check for errors in the evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We first created a Docker container that contains all of the available tools/functions that the agent can use.\n",
    "- We then created a Bedrock Agent with an Action Group that uses the Docker container in a Lambda function as the execution environment.\n",
    "- We then created a set of evaluation scenarios that cover different aspects of the agent's behavior.\n",
    "- We then ran the agent evaluation and the results are displayed above.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
