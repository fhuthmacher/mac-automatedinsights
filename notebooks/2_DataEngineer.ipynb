{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Engineer Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create a data engineer agent with Amazon Bedrock Agents that will be able to:\n",
    "- read data from a file\n",
    "- do a semantic type detection on each column of the file\n",
    "- create a SQL table definition\n",
    "- create a table in Amazon Athena\n",
    "- create data in a specified s3 directory\n",
    "\n",
    "\n",
    "And then we will evaluate the agents's performance in different scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huthmac/Documents/AWS/00_workspace/mac-automatedinsights/venv/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/26/25 15:52:30] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///Users/huthmac/Documents/AWS/00_workspace/mac-automatedinsights/venv/lib/python3.11/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/huthmac/Documents/AWS/00_workspace/mac-automatedinsights/venv/lib/python3.11/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/26/25 15:52:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=268517;file:///Users/huthmac/Documents/AWS/00_workspace/mac-automatedinsights/venv/lib/python3.11/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=777975;file:///Users/huthmac/Documents/AWS/00_workspace/mac-automatedinsights/venv/lib/python3.11/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/huthmac/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['AWS_ACCOUNT'] = os.getenv('AWS_ACCOUNT')\n",
    "os.environ['DATAENGINEER_AGENT_PROFILE_ARN'] = os.getenv('DATAENGINEER_AGENT_PROFILE_ARN')\n",
    "os.environ['DATAENGINEER_AGENT_EVAL_PROFILE_ARN'] = os.getenv('DATAENGINEER_AGENT_EVAL_PROFILE_ARN')\n",
    "\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "AWS_ACCOUNT = os.environ['AWS_ACCOUNT']\n",
    "DATAENGINEER_AGENT_PROFILE_ARN = os.environ['DATAENGINEER_AGENT_PROFILE_ARN']\n",
    "DATAENGINEER_AGENT_EVAL_PROFILE_ARN = os.environ['DATAENGINEER_AGENT_EVAL_PROFILE_ARN']\n",
    "\n",
    "# Bedrock Agents does not yet support application inference profiles\n",
    "MODEL_ID =  \"anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore.config\n",
    "config = botocore.config.Config(\n",
    "    connect_timeout=600,  # 10 minutes\n",
    "    read_timeout=600,     # 10 minutes\n",
    "    retries={'max_attempts': 3}\n",
    ")\n",
    "\n",
    "session = boto3.Session(region_name=REGION)\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "bedrock_agent_client = session.client('bedrock-agent', config=config)\n",
    "bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', config=config)\n",
    "bedrock_runtime_client = session.client('bedrock-runtime', config=config)\n",
    "bedrock_client = session.client('bedrock', config=config)\n",
    "lambda_client = session.client('lambda', config=config)\n",
    "iam_resource = session.resource('iam')\n",
    "iam_client = session.client('iam')\n",
    "athena_client = session.client('athena')\n",
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Engineer agent lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../dataengineer/bedrock_data_engineer_agent.py\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "import zipfile\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from io import StringIO\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "class RequestType(str, Enum):\n",
    "    GET_INFORMATION_FOR_SEMANTIC_TYPE_DETECTION = \"/GetInformationForSemanticTypeDetection\"\n",
    "    SAVE_SQL_TABLE_DEFINITION = \"/SaveSQLTableDefinition\"\n",
    "    CREATE_ATHENA_TABLE = \"/CreateAthenaTable\"\n",
    "    QUERY_DATA = \"/QueryData\"\n",
    "    GET_DATABASE_SCHEMA = \"/GetDatabaseSchema\"\n",
    "    GET_ERM = \"/GetERM\"\n",
    "    SAVE_ERM = \"/SaveERM\"\n",
    "\n",
    "\n",
    "\n",
    "class APIResponse(BaseModel):\n",
    "    message: str\n",
    "    results: Dict[str, Any]\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# get the environment variables\n",
    "if 'S3_BUCKET_NAME' not in globals():\n",
    "    S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\n",
    "    logger.info(f\"S3_BUCKET_NAME: {S3_BUCKET_NAME}\")\n",
    "\n",
    "if 'MODEL_ID' not in globals():\n",
    "    MODEL_ID = os.getenv('MODEL_ID')\n",
    "    logger.info(f\"MODEL_ID: {MODEL_ID}\")\n",
    "\n",
    "if 'REGION' not in globals():\n",
    "    REGION = os.getenv('REGION')\n",
    "    logger.info(f\"REGION: {REGION}\")\n",
    "\n",
    "if 'ATHENA_QUERY_EXECUTION_LOCATION' not in globals():\n",
    "    ATHENA_QUERY_EXECUTION_LOCATION = f's3://{S3_BUCKET_NAME}/athena_results/'\n",
    "    logger.info(f\"ATHENA_QUERY_EXECUTION_LOCATION: {ATHENA_QUERY_EXECUTION_LOCATION}\")\n",
    "\n",
    "session = boto3.Session(region_name=REGION)\n",
    "\n",
    "s3_client = session.client('s3')\n",
    "athena_client = session.client('athena')\n",
    "\n",
    "try:\n",
    "    response = athena_client.get_work_group(WorkGroup='primary')\n",
    "    ConfigurationUpdates={}\n",
    "    ConfigurationUpdates['EnforceWorkGroupConfiguration']= True\n",
    "    ResultConfigurationUpdates= {}\n",
    "    athena_location = \"s3://\"+ S3_BUCKET_NAME +\"/athena_results/\"\n",
    "    ResultConfigurationUpdates['OutputLocation']=athena_location\n",
    "    EngineVersion = response['WorkGroup']['Configuration']['EngineVersion']\n",
    "    ConfigurationUpdates['ResultConfigurationUpdates']=ResultConfigurationUpdates\n",
    "    ConfigurationUpdates['PublishCloudWatchMetricsEnabled']= response['WorkGroup']['Configuration']['PublishCloudWatchMetricsEnabled']\n",
    "    ConfigurationUpdates['EngineVersion']=EngineVersion\n",
    "    ConfigurationUpdates['RequesterPaysEnabled']= response['WorkGroup']['Configuration']['RequesterPaysEnabled']\n",
    "    response2 = athena_client.update_work_group(WorkGroup='primary',ConfigurationUpdates=ConfigurationUpdates,State='ENABLED')\n",
    "    logger.info(f\"athena output location updated to s3://{S3_BUCKET_NAME}/athena_results/\")  \n",
    "except Exception as e:\n",
    "    logger.error(str(e))\n",
    "\n",
    "\n",
    "\n",
    "def parse_json(json_string):\n",
    "    if not json_string:  # Handle None or empty string\n",
    "        logger.warning(\"Received empty JSON string\")\n",
    "        return {\n",
    "            'semantic_column_name': 'unknown',\n",
    "            'column_description': 'No response from LLM',\n",
    "            'data_type': 'unknown',\n",
    "            'usecases': []\n",
    "        }\n",
    "    try:\n",
    "        # First try to clean up any leading/trailing whitespace\n",
    "        json_string = json_string.strip()\n",
    "        \n",
    "        # Remove any text before the first '{'\n",
    "        if '{' in json_string:\n",
    "            json_string = json_string[json_string.find('{'):]\n",
    "            \n",
    "        # Remove any text after the last '}'\n",
    "        if '}' in json_string:\n",
    "            json_string = json_string[:json_string.rfind('}')+1]\n",
    "            \n",
    "        # Try to parse the cleaned JSON string\n",
    "        parsed = json.loads(json_string)\n",
    "        \n",
    "        # Convert old semantic_type key to semantic_column_name if needed\n",
    "        if 'semantic_type' in parsed and 'semantic_column_name' not in parsed:\n",
    "            parsed['semantic_column_name'] = parsed.pop('semantic_type')\n",
    "            \n",
    "        return parsed\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON: {e}\")\n",
    "        logger.error(f\"Problematic JSON string: {json_string}\")\n",
    "        return {\n",
    "            'semantic_column_name': 'unknown',\n",
    "            'column_description': 'No response from LLM',\n",
    "            'data_type': 'unknown',\n",
    "            'usecases': []\n",
    "        }\n",
    "\n",
    "def execute_athena_query(database, query):\n",
    "    logger.info(\"Executing Athena query...\")\n",
    "    # Start query execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': ATHENA_QUERY_EXECUTION_LOCATION\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get query execution ID\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    print(f\"Query Execution ID: {query_execution_id}\")\n",
    "\n",
    "    # Wait for the query to complete\n",
    "    response_wait = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    while response_wait['QueryExecution']['Status']['State'] in ['QUEUED', 'RUNNING']:\n",
    "        print(\"Query is still running...\")\n",
    "        response_wait = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    print(f'response_wait {response_wait}')\n",
    "\n",
    "    # Check if the query completed successfully\n",
    "    if response_wait['QueryExecution']['Status']['State'] == 'SUCCEEDED':\n",
    "        print(\"Query succeeded!\")\n",
    "\n",
    "        # Get query results\n",
    "        query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "        # Extract and return the result data\n",
    "        code = 'SUCCEEDED'\n",
    "        return code, extract_result_data(query_results)\n",
    "\n",
    "    else:\n",
    "        print(\"Query failed!\")\n",
    "        code = response_wait['QueryExecution']['Status']['State']\n",
    "        message = response_wait['QueryExecution']['Status']['StateChangeReason']\n",
    "    \n",
    "        return code, message\n",
    "\n",
    "def extract_result_data(query_results):\n",
    "    # Return a cleaned response to the agent\n",
    "    result_data = []\n",
    "\n",
    "    # Extract column names\n",
    "    column_info = query_results['ResultSet']['ResultSetMetadata']['ColumnInfo']\n",
    "    column_names = [column['Name'] for column in column_info]\n",
    "\n",
    "    # Extract data rows\n",
    "    for row in query_results['ResultSet']['Rows']:\n",
    "        # Handle different data types in the response\n",
    "        data = []\n",
    "        for item in row['Data']:\n",
    "            # Each item may contain different field based on the data type\n",
    "            value = None\n",
    "            if 'VarCharValue' in item:\n",
    "                value = item['VarCharValue']\n",
    "            elif 'NumberValue' in item:\n",
    "                value = str(item['NumberValue'])  # Convert to string for consistency\n",
    "            else:\n",
    "                value = str(item)  # Fallback to string representation\n",
    "            data.append(value)\n",
    "            \n",
    "        result_data.append(dict(zip(column_names, data)))\n",
    "\n",
    "    return result_data\n",
    "\n",
    "\n",
    "def create_sql_table_definition(sql_table_definition, s3_file_location) -> APIResponse:\n",
    "    logger.info(\"Creating SQL table definition...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a writable directory\n",
    "        temp_dir = \"/tmp/sql_table_definition\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        # Set the current working directory to the temp directory\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(temp_dir)\n",
    "\n",
    "        # save sql_table_definition to s3 based on s3_file_location\n",
    "        s3_url = urlparse(s3_file_location)\n",
    "        bucket = s3_url.netloc\n",
    "        key = s3_url.path.lstrip('/')  # Remove leading slash\n",
    "        # extract the filename from the key\n",
    "        filename = key.split('/')[-1]\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(sql_table_definition)\n",
    "\n",
    "        s3_client.upload_file(\n",
    "            filename,\n",
    "            Bucket=bucket,\n",
    "            Key=key\n",
    "        )\n",
    "        \n",
    "        return APIResponse(\n",
    "            message=\"SQL table definition successful\",\n",
    "            results={\n",
    "                'sql_table_definition': sql_table_definition,\n",
    "                'sql_table_definition_file_location': s3_file_location\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in create_sql_table_definition function: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_database_schema() -> str:\n",
    "        \"\"\"Retrieve the SQL database schema from S3\"\"\"\n",
    "        schema_prefix = 'metadata/sql_table_definition'\n",
    "        logger.info(f\"Retrieving database schema from s3://{S3_BUCKET_NAME}/{schema_prefix}\")\n",
    "        \n",
    "        sql_database_schema = []\n",
    "        try:\n",
    "            response = s3_client.list_objects_v2(\n",
    "                Bucket=S3_BUCKET_NAME, \n",
    "                Prefix=schema_prefix\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                logger.warning(f\"No schema files found in s3://{S3_BUCKET_NAME}/{schema_prefix}\")\n",
    "                return \"[]\"\n",
    "            \n",
    "            logger.info(f\"Found {len(response['Contents'])} schema files\")\n",
    "            \n",
    "            for item in response['Contents']:\n",
    "                if item['Key'].endswith('/'):\n",
    "                    continue\n",
    "                    \n",
    "                logger.info(f\"Reading schema file: {item['Key']}\")\n",
    "                try:\n",
    "                    content = s3_client.get_object(\n",
    "                        Bucket=S3_BUCKET_NAME, \n",
    "                        Key=item['Key']\n",
    "                    )['Body'].read().decode('utf-8')\n",
    "                    sql_database_schema.append(content)\n",
    "                    logger.debug(f\"Successfully read schema from {item['Key']}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading schema file {item['Key']}: {str(e)}\")\n",
    "            \n",
    "            logger.info(f\"Successfully retrieved {len(sql_database_schema)} schema definitions\")\n",
    "            return json.dumps(sql_database_schema)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in get_database_schema: {str(e)}\", exc_info=True)\n",
    "            return \"[]\"\n",
    "\n",
    "def get_erm_schema() -> str:\n",
    "        \"\"\"Retrieve the Entity Relationship Model (ERM) from S3\"\"\"\n",
    "        erm_prefix = 'metadata/er_diagram'\n",
    "        logger.info(f\"Retrieving ERM from s3://{S3_BUCKET_NAME}/{erm_prefix}\")\n",
    "        \n",
    "        erm_schemas = []\n",
    "        try:\n",
    "            response = s3_client.list_objects_v2(\n",
    "                Bucket=S3_BUCKET_NAME, \n",
    "                Prefix=erm_prefix\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                logger.warning(f\"No ERM files found in s3://{S3_BUCKET_NAME}/{erm_prefix}\")\n",
    "                return \"[]\"\n",
    "            \n",
    "            logger.info(f\"Found {len(response['Contents'])} ERM files\")\n",
    "            \n",
    "            for item in response['Contents']:\n",
    "                if item['Key'].endswith('/'):\n",
    "                    continue\n",
    "                    \n",
    "                logger.info(f\"Reading ERM file: {item['Key']}\")\n",
    "                try:\n",
    "                    content = s3_client.get_object(\n",
    "                        Bucket=S3_BUCKET_NAME, \n",
    "                        Key=item['Key']\n",
    "                    )['Body'].read().decode('utf-8')\n",
    "                    # Verify it's valid JSON before adding\n",
    "                    json_content = json.loads(content)\n",
    "                    erm_schemas.append(json_content)\n",
    "                    logger.debug(f\"Successfully read ERM from {item['Key']}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.error(f\"File {item['Key']} contains invalid JSON\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error reading ERM file {item['Key']}: {str(e)}\")\n",
    "            \n",
    "            logger.info(f\"Successfully retrieved {len(erm_schemas)} ERM schemas\")\n",
    "            return json.dumps(erm_schemas)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in get_erm_schema: {str(e)}\", exc_info=True)\n",
    "            return \"[]\"\n",
    "\n",
    "def save_erm_schema(erm_data) -> APIResponse:\n",
    "    \"\"\"Save the Entity Relationship Model (ERM) to S3\n",
    "    \n",
    "    Args:\n",
    "        erm_data: The ERM data to save\n",
    "        \n",
    "    Returns:\n",
    "        APIResponse: Response with status and saved ERM location\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a writable directory\n",
    "        temp_dir = \"/tmp/erm\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        # Set the current working directory to the temp directory\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(temp_dir)\n",
    "        \n",
    "\n",
    "        filename = f'erm.json'\n",
    "        filepath = os.path.join(temp_dir, filename)\n",
    "        \n",
    "        # Save the ERM data to a file\n",
    "        with open(filepath, 'w') as f:\n",
    "            if isinstance(erm_data, str):\n",
    "                f.write(erm_data)\n",
    "            else:\n",
    "                json.dump(erm_data, f, indent=2)\n",
    "        \n",
    "        # Upload the file to S3\n",
    "        s3_key = f'metadata/er_diagram/{filename}'\n",
    "        s3_client.upload_file(\n",
    "            filepath,\n",
    "            Bucket=S3_BUCKET_NAME,\n",
    "            Key=s3_key\n",
    "        )\n",
    "        \n",
    "        s3_location = f's3://{S3_BUCKET_NAME}/{s3_key}'\n",
    "        logger.info(f\"Saved ERM to {s3_location}\")\n",
    "        \n",
    "        # Change back to original directory\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "        return APIResponse(\n",
    "            message=\"ERM saved successfully\",\n",
    "            results={\n",
    "                'erm_file_location': s3_location\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in save_erm_schema: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def query_athena_table(athena_database, sql_query) -> APIResponse:\n",
    "    logger.info(\"Querying Athena table...\")\n",
    "    try:\n",
    "\n",
    "        # execute sql query\n",
    "        status_code, response_data = execute_athena_query(athena_database, sql_query)\n",
    "        logger.info(f\"Athena query execution response: {response_data}\")\n",
    "        logger.info(f\"status_code: {status_code}\")\n",
    "        if status_code == 'SUCCEEDED':\n",
    "            return APIResponse(\n",
    "                message=f\"Query execution {status_code}\",\n",
    "                results={\n",
    "                    'status': status_code,\n",
    "                    'query': sql_query,\n",
    "                    'data': response_data if isinstance(response_data, dict) else {'results': response_data}\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return APIResponse(\n",
    "                message=f\"Query execution failed with status: {status_code}\",\n",
    "                results={\n",
    "                    'status': status_code,\n",
    "                    'query': sql_query,\n",
    "                    'error': response_data if isinstance(response_data, str) else str(response_data)\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in query_athena_table function: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def check_athena_table_exists(database, table_name):\n",
    "    \"\"\"\n",
    "    Check if a table exists in Athena\n",
    "    \n",
    "    Args:\n",
    "        database (str): The name of the database\n",
    "        table_name (str): The name of the table to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if table exists, False otherwise\n",
    "    \"\"\"\n",
    "    logger.info(f\"Checking if table {table_name} exists in database {database}\")\n",
    "    try:\n",
    "        # Try to get table metadata\n",
    "        response = athena_client.get_table_metadata(\n",
    "            CatalogName='AwsDataCatalog',\n",
    "            DatabaseName=database,\n",
    "            TableName=table_name\n",
    "        )\n",
    "        logger.info(f\"get_table_metadata response: {response}\")\n",
    "        logger.info(f\"Table {table_name} exists in database {database}\")\n",
    "        return True\n",
    "    except athena_client.exceptions.MetadataException:\n",
    "        logger.info(f\"Table {table_name} does not exist in database {database}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking if table exists: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "def parse_request_parameters(event):\n",
    "    \"\"\"Parse request parameters from the Lambda event\"\"\"\n",
    "    parameters = {}\n",
    "    \n",
    "    # Extract parameters from requestBody if present\n",
    "    if event.get('requestBody') and event['requestBody'].get('content'):\n",
    "        content = event['requestBody']['content']\n",
    "        if 'application/json' in content and 'properties' in content['application/json']:\n",
    "            for prop in content['application/json']['properties']:\n",
    "                parameters[prop['name']] = prop['value']\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Create base temp directories at the start\n",
    "        os.makedirs(\"/tmp/data\", exist_ok=True)\n",
    "        os.makedirs(\"/tmp/metadata\", exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Received event: {json.dumps(event)}\")\n",
    "        \n",
    "        parameters = parse_request_parameters(event)\n",
    "        logger.info(f\"parameters: {parameters}\")\n",
    "\n",
    "        request_type = event.get('apiPath')\n",
    "        logger.info(f\"request_type: {request_type}\")\n",
    "        \n",
    "        \n",
    "        if parameters.get('DataLocation') is not None and parameters.get('DataLocation') != \"\":\n",
    "            logger.info(f\"Downloading data from {parameters.get('DataLocation')}\")\n",
    "\n",
    "            # Ensure data directory exists\n",
    "            os.makedirs(\"/tmp/data\", exist_ok=True)\n",
    "\n",
    "            # Clear the /tmp/data directory before processing new file\n",
    "            for file in os.listdir(\"/tmp/data\"):\n",
    "                os.remove(os.path.join(\"/tmp/data\", file))\n",
    "\n",
    "            # split data_location into bucket and key\n",
    "            s3_url = urlparse(parameters.get('DataLocation'))\n",
    "            data_bucket = s3_url.netloc\n",
    "            logger.info(f\"data_bucket: {data_bucket}\")\n",
    "            data_key = s3_url.path.lstrip('/')  # Remove leading slash\n",
    "            logger.info(f\"data_key: {data_key}\")\n",
    "\n",
    "            # Get just the filename from the path\n",
    "            filename = os.path.basename(data_key)\n",
    "            logger.info(f\"filename: {filename}\")\n",
    "            # remove the extension from the filename\n",
    "            file_or_table_name = os.path.splitext(filename)[0]\n",
    "            logger.info(f\"file_or_table_name: {file_or_table_name}\")\n",
    "\n",
    "            local_file_path = os.path.join(\"/tmp/data\", filename)\n",
    "            logger.info(f\"local_file_path: {local_file_path}\")\n",
    "            # download the data from s3\n",
    "            s3_client.download_file(\n",
    "                Bucket=data_bucket,\n",
    "                Key=data_key,\n",
    "                Filename=local_file_path\n",
    "            )\n",
    "\n",
    "            # check if the data is zipped then unzip it\n",
    "            if data_key.endswith(\".zip\"):\n",
    "                logger.info(f\"Unzipping data\")\n",
    "                with zipfile.ZipFile(local_file_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(\"/tmp/data\")\n",
    "                logger.info(f\"Unzipped data to /tmp/data\")\n",
    "            \n",
    "            logger.info(f\"Files in /tmp/data: {os.listdir('/tmp/data')}\")\n",
    "\n",
    "            # Load data from the first compatible file found\n",
    "            data_loaded = False\n",
    "            for file in os.listdir(\"/tmp/data\"):\n",
    "                logger.info(f\"file: {file}\")\n",
    "                file_path = os.path.join(\"/tmp/data\", file)\n",
    "                logger.info(f\"Processing file: {file_path}\")\n",
    "                ext = os.path.splitext(file)[1].lower()\n",
    "                logger.info(f\"File extension: {ext}\")\n",
    "                # set filename\n",
    "                try:\n",
    "                    if ext == '.csv':\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        # remove the extension from the filename\n",
    "                        file_or_table_name = os.path.splitext(file)[0]\n",
    "                        \n",
    "                    elif ext == '.json':\n",
    "                        df = pd.read_json(file_path)\n",
    "                        # remove the extension from the filename\n",
    "                        file_or_table_name = os.path.splitext(file)[0]\n",
    "                    elif ext == '.parquet':\n",
    "                        df = pd.read_parquet(file_path)\n",
    "                        # remove the extension from the filename\n",
    "                        file_or_table_name = os.path.splitext(file)[0]\n",
    "                    \n",
    "                    if not df.empty:\n",
    "                        logger.info(f\"Successfully loaded data from {file_path}\")\n",
    "                        data_loaded = True\n",
    "                        logger.info(f\"file_or_table_name: {file_or_table_name}\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if not data_loaded:\n",
    "                raise ValueError(\"Could not load data from any files in the specified location\")\n",
    "            \n",
    "\n",
    "        response_data = None\n",
    "        s3_base_path = f's3://{S3_BUCKET_NAME}/'\n",
    "\n",
    "        if request_type == RequestType.GET_DATABASE_SCHEMA:\n",
    "            # Get database schema\n",
    "            schema = get_database_schema()\n",
    "            response_data = json.loads(schema)  # Convert string to JSON array\n",
    "            response_body = {\n",
    "                'application/json': {\n",
    "                    'body': response_data\n",
    "                }\n",
    "            }\n",
    "            response_data = APIResponse(\n",
    "                message=\"Database schema retrieved successfully\",\n",
    "                results=response_body\n",
    "            )\n",
    "\n",
    "\n",
    "        if request_type == RequestType.GET_INFORMATION_FOR_SEMANTIC_TYPE_DETECTION:\n",
    "            # Export sample data to make available for the ReAct agent\n",
    "            sample_df = df.head(100)  # Use only a sample\n",
    "            \n",
    "            # Return information needed for the ReAct agent to perform semantic detection\n",
    "            response_data = APIResponse(\n",
    "                message=\"Data sample prepared for semantic type detection\",\n",
    "                results={\n",
    "                    'column_names': list(df.columns),\n",
    "                    'data_sample': sample_df.head(10).to_dict('records')\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        if request_type == RequestType.SAVE_SQL_TABLE_DEFINITION:\n",
    "            # Get the SQL table definition\n",
    "            sql_table_definition = parameters.get('SQL_Table_Definition')\n",
    "            table_name = parameters.get('TableName')\n",
    "            \n",
    "            if not table_name:\n",
    "                # Try to extract table name from SQL definition\n",
    "                table_match = re.search(r'CREATE\\s+TABLE\\s+(\\w+)', sql_table_definition, re.IGNORECASE)\n",
    "                if table_match:\n",
    "                    table_name = table_match.group(1)\n",
    "                else:\n",
    "                    # Fallback to a default name if we can't extract it\n",
    "                    table_name = f\"table_{int(time.time())}\"\n",
    "                logger.info(f\"Extracted or generated table name: {table_name}\")\n",
    "            \n",
    "            if sql_table_definition:\n",
    "                # Save SQL definition to S3\n",
    "                s3_file_location = f'{s3_base_path}metadata/sql_table_definition/{table_name}_sql_table_definition.sql'\n",
    "                s3_file_location = s3_file_location.lower()\n",
    "                response_data = create_sql_table_definition(sql_table_definition, s3_file_location)\n",
    "                \n",
    "            else:\n",
    "                response_data = APIResponse(\n",
    "                    message=\"Missing SQL table definition\",\n",
    "                    results={\n",
    "                        'error': \"Missing SQL table definition\"\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if request_type == RequestType.CREATE_ATHENA_TABLE:\n",
    "            table_name = parameters.get('TableName')\n",
    "            athena_database = parameters.get('AthenaDatabase')\n",
    "            athena_table_create_definition = parameters.get('Athena_Table_Create_SQL_statement')\n",
    "            \n",
    "            # For backward compatibility, check other possible parameter names\n",
    "            if athena_table_create_definition is None:\n",
    "                athena_table_create_definition = parameters.get('TableDefinition')\n",
    "                if athena_table_create_definition is None:\n",
    "                    athena_table_create_definition = parameters.get('Table_Definition')\n",
    "            \n",
    "            if athena_table_create_definition is None:\n",
    "                logger.error(\"Missing table definition parameter\")\n",
    "                raise ValueError(\"Missing required parameter: Athena_Table_Create_SQL_statement\")\n",
    "                \n",
    "            data_location = parameters.get('DataLocation')\n",
    "            if not data_location:\n",
    "                logger.error(\"Missing DataLocation parameter\")\n",
    "                raise ValueError(\"Missing required parameter: DataLocation\")\n",
    "                \n",
    "            s3_target_file_location = f'{s3_base_path}raw/{table_name}/'\n",
    "            \n",
    "            # Get the filename from the data_location\n",
    "            filename = os.path.basename(data_location)\n",
    "            \n",
    "            logger.info(f\"Original data location: {data_location}\")\n",
    "            s3_target_file_location = s3_target_file_location.lower()\n",
    "            logger.info(f\"Target location: {s3_target_file_location}\")\n",
    "            \n",
    "            # Update the LOCATION with the s3_target_file_location if it exists in the SQL\n",
    "            if 'LOCATION' in athena_table_create_definition:\n",
    "                # Extract the current location\n",
    "                location_match = re.search(r\"LOCATION\\s+'([^']+)'\", athena_table_create_definition)\n",
    "                if location_match:\n",
    "                    current_location = location_match.group(1)\n",
    "                    logger.info(f\"Current location in SQL: {current_location}\")\n",
    "                    # Replace with the new location\n",
    "                    athena_table_create_definition = athena_table_create_definition.replace(\n",
    "                        f\"LOCATION '{current_location}'\", \n",
    "                        f\"LOCATION '{s3_target_file_location}'\"\n",
    "                    )\n",
    "                    logger.info(f\"Updated SQL with new location: {s3_target_file_location}\")\n",
    "            \n",
    "            # Prepare and upload the data\n",
    "            prepare_and_upload_data(df, athena_table_create_definition, s3_target_file_location)\n",
    "            \n",
    "            # Execute the provided table definition in Athena\n",
    "            status_code, response_data = execute_athena_query(athena_database, athena_table_create_definition)\n",
    "            \n",
    "            response_data = APIResponse(\n",
    "                message=f\"Athena table creation {status_code}\",\n",
    "                results={\n",
    "                    'status': status_code,\n",
    "                    'query_results': response_data if isinstance(response_data, dict) else {'data': response_data}\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if request_type == RequestType.QUERY_DATA:\n",
    "            sql_query = parameters.get('SQLQuery')\n",
    "            athena_database = parameters.get('AthenaDatabase')\n",
    "            response_data = query_athena_table(athena_database, sql_query)\n",
    "\n",
    "        if request_type == RequestType.GET_ERM:\n",
    "            # Get the ERM schema\n",
    "            erm_schema = get_erm_schema()\n",
    "            response_data = json.loads(erm_schema)  # Convert string to JSON array\n",
    "            response_body = {\n",
    "                'application/json': {\n",
    "                    'body': response_data\n",
    "                }\n",
    "            }\n",
    "            response_data = APIResponse(\n",
    "                message=\"ERM schema retrieved successfully\",\n",
    "                results=response_body\n",
    "            )\n",
    "        \n",
    "        if request_type == RequestType.SAVE_ERM:\n",
    "            # Extract ERM data from request body\n",
    "            erm_data = parameters.get('ERMData')\n",
    "            \n",
    "            if erm_data:\n",
    "                response_data = save_erm_schema(erm_data)\n",
    "            elif event.get('requestBody') and event['requestBody'].get('content'):\n",
    "                content = event['requestBody']['content']\n",
    "                if 'application/json' in content and 'body' in content['application/json']:\n",
    "                    erm_data = content['application/json']['body']\n",
    "                    response_data = save_erm_schema(erm_data)\n",
    "                else:\n",
    "                    response_data = APIResponse(\n",
    "                        message=\"Missing ERM data in request body\",\n",
    "                        results={\n",
    "                            'error': \"Missing ERM data in request body\"\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                response_data = APIResponse(\n",
    "                    message=\"Missing ERM data\",\n",
    "                    results={\n",
    "                        'error': \"Missing ERM data parameter or request body\"\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        # Format successful response\n",
    "        response_body = {\n",
    "            'application/json': {\n",
    "                'body': {\n",
    "                    'message': str(response_data.message),\n",
    "                    'results': str(response_data.results)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response_size = sys.getsizeof(json.dumps(response_body))\n",
    "        MAX_RESPONSE_SIZE = 22000  # 22KB limit\n",
    "        if response_size > MAX_RESPONSE_SIZE:\n",
    "            logger.error(f\"Response size {response_size} exceeds limit. Truncating content...\")\n",
    "\n",
    "        response_code = 200\n",
    "        action_response = {\n",
    "            'actionGroup': event['actionGroup'],\n",
    "            'apiPath': event['apiPath'],\n",
    "            'httpMethod': event['httpMethod'],\n",
    "            'httpStatusCode': response_code,\n",
    "            'responseBody': response_body\n",
    "        }\n",
    "\n",
    "        api_response = {'messageVersion': '1.0', 'response': action_response}\n",
    "        logger.info(f\"action_response: {api_response}\")\n",
    "        return api_response\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in lambda_handler: {str(e)}\")\n",
    "        logger.error(f\"Exception type: {type(e)}\")\n",
    "        logger.error(f\"Stack trace: {sys.exc_info()}\")\n",
    "            \n",
    "        return {\n",
    "            \"messageVersion\": \"1.0\",\n",
    "            \"response\": {\n",
    "                \"actionGroup\": event['actionGroup'],\n",
    "                \"apiPath\": event['apiPath'],\n",
    "                \"httpMethod\": event['httpMethod'],\n",
    "                \"httpStatusCode\": 400 if isinstance(e, ValueError) else 500,\n",
    "                \"responseBody\": {\n",
    "                    'application/json': {\n",
    "                        'body': {\n",
    "                            \"error\": str(e),\n",
    "                            \"errorCode\": \"400\" if isinstance(e, ValueError) else \"500\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "def prepare_and_upload_data(df, sql_table_definition, s3_target_file_location):\n",
    "    \"\"\"\n",
    "    Updates DataFrame columns to match SQL table definition and uploads to S3\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to process\n",
    "        sql_table_definition (str): SQL CREATE TABLE statement\n",
    "        s3_target_file_location (str): S3 location to upload the processed file\n",
    "        \n",
    "    Returns:\n",
    "        str: The S3 location where the file was uploaded\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        logger.info(f\"Preparing data for upload to {s3_target_file_location}\")\n",
    "        \n",
    "        # Extract column names from SQL table definition\n",
    "        # This regex looks for column definitions in a CREATE TABLE statement\n",
    "        # Updated to handle both regular and EXTERNAL tables\n",
    "        column_pattern = r'CREATE\\s+(EXTERNAL\\s+)?TABLE\\s+\\w+\\s*\\((.*?)\\).*?(?:LOCATION|$)'\n",
    "        match = re.search(column_pattern, sql_table_definition, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if not match:\n",
    "            logger.error(\"Could not extract column definitions from SQL statement\")\n",
    "            logger.error(f\"SQL statement: {sql_table_definition}\")\n",
    "            raise ValueError(\"Invalid SQL table definition format\")\n",
    "            \n",
    "        # Group 2 contains the column definitions if EXTERNAL is present, otherwise group 1\n",
    "        column_section = match.group(2)\n",
    "        \n",
    "        # Extract individual column names\n",
    "        columns = []\n",
    "        for line in column_section.split(','):\n",
    "            # Extract the column name (first word in the line)\n",
    "            column_match = re.search(r'^\\s*(\\w+)', line.strip())\n",
    "            if column_match:\n",
    "                columns.append(column_match.group(1))\n",
    "        \n",
    "        logger.info(f\"Extracted columns from SQL definition: {columns}\")\n",
    "        \n",
    "        if len(columns) == 0:\n",
    "            logger.error(\"No columns extracted from SQL definition\")\n",
    "            raise ValueError(\"No columns found in SQL table definition\")\n",
    "            \n",
    "        # Check if number of columns matches\n",
    "        if len(columns) != len(df.columns):\n",
    "            logger.warning(f\"Column count mismatch: SQL has {len(columns)}, DataFrame has {len(df.columns)}\")\n",
    "            # We'll proceed anyway and rename the columns we have\n",
    "        \n",
    "        # Create a mapping of old column names to new column names\n",
    "        # Use min length to avoid index errors if counts don't match\n",
    "        column_mapping = {}\n",
    "        for i in range(min(len(df.columns), len(columns))):\n",
    "            column_mapping[df.columns[i]] = columns[i]\n",
    "        \n",
    "        # Rename the DataFrame columns\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        logger.info(f\"Renamed DataFrame columns to: {list(df.columns)}\")\n",
    "        \n",
    "        # Parse S3 URL\n",
    "        s3_url = urlparse(s3_target_file_location)\n",
    "        bucket = s3_url.netloc\n",
    "        key = s3_url.path.lstrip('/')\n",
    "        \n",
    "        # Create a temporary file\n",
    "        temp_dir = \"/tmp/processed_data\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        # If s3_target_file_location is a directory (ends with /), append filename\n",
    "        if s3_target_file_location.endswith('/'):\n",
    "            filename = f\"{os.path.basename(key.rstrip('/'))}_{int(time.time())}.csv\"\n",
    "            key = f\"{key}{filename}\"\n",
    "            local_file_path = os.path.join(temp_dir, filename)\n",
    "        else:\n",
    "            local_file_path = os.path.join(temp_dir, os.path.basename(key))\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(local_file_path, index=False)\n",
    "        logger.info(f\"Saved processed data to {local_file_path}\")\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_client.upload_file(\n",
    "            local_file_path,\n",
    "            Bucket=bucket,\n",
    "            Key=key\n",
    "        )\n",
    "        logger.info(f\"Uploaded processed data to s3://{bucket}/{key}\")\n",
    "        \n",
    "        return f\"s3://{bucket}/{key}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in prepare_and_upload_data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../dataengineer/dataengineer.Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.11\n",
    "\n",
    "# Install build dependencies first\n",
    "RUN yum install libgomp git gcc gcc-c++ make -y \\\n",
    " && yum clean all -y && rm -rf /var/cache/yum\n",
    "\n",
    "\n",
    "RUN python3 -m pip --no-cache-dir install --upgrade --trusted-host pypi.org --trusted-host files.pythonhosted.org pip \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade wheel setuptools \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pandas \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade boto3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade opensearch-py \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade Pillow \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pyarrow \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade fastparquet \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade urllib3 \\\n",
    " && python3 -m pip --no-cache-dir install --upgrade pydantic\n",
    "\n",
    "# Copy function code\n",
    "WORKDIR /var/task\n",
    "COPY ../dataengineer/bedrock_data_engineer_agent.py .\n",
    "COPY ../notebooks/utils/ utils/\n",
    "\n",
    "# Set handler environment variable\n",
    "ENV _HANDLER=\"bedrock_data_engineer_agent.lambda_handler\"\n",
    "\n",
    "# Let's go back to using the default entrypoint\n",
    "ENTRYPOINT [ \"/lambda-entrypoint.sh\" ]\n",
    "CMD [ \"bedrock_data_engineer_agent.lambda_handler\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run local docker container to test the dataengineer-lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run local docker container\n",
    "!docker build -t dataengineer-lambda -f ../dataengineer/dataengineer.Dockerfile .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run with tailing log\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "!docker run -d \\\n",
    "-e AWS_ACCESS_KEY_ID={credentials.access_key} \\\n",
    "-e AWS_SECRET_ACCESS_KEY={credentials.secret_key} \\\n",
    "-e AWS_SESSION_TOKEN={credentials.token} \\\n",
    "-e AWS_DEFAULT_REGION={REGION} \\\n",
    "-e REGION={REGION} \\\n",
    "-e AWS_LAMBDA_FUNCTION_TIMEOUT=900 \\\n",
    "-e S3_BUCKET_NAME={S3_BUCKET_NAME} \\\n",
    "-p 9000:8080 dataengineer-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --filter ancestor=dataengineer-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test GetInformationForSemanticTypeDetection\n",
    "\n",
    "# sample request structure:\n",
    "request_body = {\n",
    "    \"apiPath\": \"/GetInformationForSemanticTypeDetection\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/Customers.csv\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataEngineerActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Athena table\n",
    "request_body = {\n",
    "    \"apiPath\": \"/QueryData\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"SQLQuery\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": \"SELECT * FROM customers limit 10;\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataEngineerActions\",\n",
    " }\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sql table definition\n",
    "request_body = {\n",
    "    \"apiPath\": \"/SaveSQLTableDefinition\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"SQL_Table_Definition\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": '''CREATE TABLE customer_data (\n",
    "                                    CustomerID INT PRIMARY KEY, -- customer_id: Used as primary key for customer identification and tracking in various analyses\n",
    "                                    FirstName VARCHAR(50), -- first_name: Customer's first name for personalized marketing and customer identity verification\n",
    "                                    LastName VARCHAR(50), -- customer_lastname: Customer's last name for personalized marketing and fraud detection\n",
    "                                    Email VARCHAR(100), -- email_address: Unique identifier for communication, used in email marketing and customer segmentation\n",
    "                                    Phone VARCHAR(20), -- phone_number: Contact information for customer service and marketing campaign targeting\n",
    "                                    Address VARCHAR(100), -- street_address: Used for geocoding, address validation, and targeted marketing\n",
    "                                    City VARCHAR(50), -- customer_city: Location data for geographic analysis and targeted marketing campaigns\n",
    "                                    State VARCHAR(50), -- state_name: Used for regional sales forecasting and geographic segmentation\n",
    "                                    Country VARCHAR(50), -- customer_country: Used for market targeting and international expansion planning\n",
    "                                    PostalCode INT, -- postal_code: Used for geographic segmentation and logistics optimization\n",
    "                                    DateOfBirth DATETIME, -- date_of_birth: Used for age calculation, demographic analysis, and personalized marketing\n",
    "                                    Gender VARCHAR(20), -- gender_identity: Used for customer segmentation and diversity analysis in marketing\n",
    "                                    CreatedDate DATETIME, -- customer_creation_datetime: Timestamp for customer acquisition trend analysis and cohort analysis\n",
    "                                    LastUpdated DATETIME -- last_updated_timestamp: Used for data quality monitoring and customer engagement analysis\n",
    "                                );'''\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"TableName\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": \"customer_data\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataEngineerActions\"\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Athena table\n",
    "request_body = {\n",
    "    \"apiPath\": \"/CreateAthenaTable\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"Athena_Table_Create_SQL_statement\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": '''CREATE EXTERNAL TABLE customer_data (\n",
    "                                    customer_id INT, \n",
    "                                    first_name STRING, \n",
    "                                    customer_lastname STRING, \n",
    "                                    email_address STRING, \n",
    "                                    phone_number STRING, \n",
    "                                    street_address STRING, \n",
    "                                    customer_city STRING, \n",
    "                                    state_name STRING, \n",
    "                                    customer_country STRING, \n",
    "                                    postal_code INT, \n",
    "                                    date_of_birth TIMESTAMP, \n",
    "                                    gender_identity STRING, \n",
    "                                    customer_creation_datetime TIMESTAMP, \n",
    "                                    last_updated_timestamp TIMESTAMP\n",
    "                                )\n",
    "                                ROW FORMAT DELIMITED\n",
    "                                FIELDS TERMINATED BY ','\n",
    "                                STORED AS TEXTFILE\n",
    "                                LOCATION 's3://huthmac-automatedinsights/uploads/'\n",
    "                                TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "                            '''\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"TableName\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": \"customer_data\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/Customers.csv\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataEngineerActions\"\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get database schema\n",
    "request_body = {\n",
    "    \"apiPath\": \"/GetDatabaseSchema\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataEngineerActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entity relationship diagram in json format\n",
    "request_body = {\n",
    "    \"apiPath\": \"/GetERM\",\n",
    "    \"requestBody\": {\n",
    "        \"content\": {\n",
    "            \"application/json\": {\n",
    "                \"properties\": [\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"httpMethod\": \"POST\",\n",
    "    \"actionGroup\": \"DataEngineerActions\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(\"http://localhost:9000/2015-03-31/functions/function/invocations\",\n",
    "                         json=request_body,\n",
    "                         timeout=900  # 15 minutes timeout\n",
    ")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the container\n",
    "!docker stop $(docker ps -q --filter ancestor=dataengineer-lambda)\n",
    "!docker ps --filter ancestor=dataengineer-lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload docker image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create ECR repository for dataengineer-lambda (if not already created in 1_environmentSetup.ipynb)\n",
    "#!aws ecr create-repository --repository-name automatedinsights/lambda_dataengineer --region {REGION} --profile {SESSION_PROFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload docker image to ECR\n",
    "!aws ecr get-login-password --region {REGION} --profile {SESSION_PROFILE} | docker login --username AWS --password-stdin {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com\n",
    "!docker tag dataengineer-lambda:latest {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_dataengineer:latest\n",
    "!docker push {AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_dataengineer:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Test Bedrock Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import string\n",
    "from utils.bedrock_agent import BedrockAgentScenarioWrapper\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "agent_name = \"DataEngineer\"\n",
    "prompt = \"Do the data preparation for the below details.\"\n",
    "\n",
    "request_body = json.dumps([\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/Customers.csv\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "  \n",
    "], ensure_ascii=False)\n",
    "\n",
    "prompt = prompt + str(request_body)\n",
    "\n",
    "instruction = \"\"\"Expert Data Engineer Agent\n",
    "You are an expert data engineer with access to a comprehensive set of data preparation and management tools. Your role is to help users process, analyze, and query data efficiently.\n",
    "\n",
    "Available Tools\n",
    "GetDatabaseSchema: Retrieves the current SQL database schema from S3.\n",
    "GetERM: Retrieves the Entity Relationship Diagram in JSON format.\n",
    "SaveERM: Saves an Entity Relationship Diagram in JSON format to S3.\n",
    "GetInformationForSemanticTypeDetection: Analyzes data to help detect semantic types.\n",
    "SaveSQLTableDefinition: Saves SQL table definition to S3.\n",
    "CreateAthenaTable: Creates an Athena table based on data and Athena table create definition.\n",
    "QueryData: Executes SQL queries against Athena databases.\n",
    "\n",
    "Workflow\n",
    "1) Data Analysis: When presented with new data, use GetInformationForSemanticTypeDetection to analyze the data structure and content. Identify semantic types for each column and 3 ML use cases where these semantic types could be used in.\n",
    "2) Table Schema Definition: Create appropriate SQL table definition with semantic column names based on the data analysis, considering semantic types and appropriate data types. \n",
    "3) Table Creation: Use the Table Schema Definition as a blueprint for the Athena table creation with the tool CreateAthenaTable to make the data available for querying.\n",
    "4) Data Querying: Verify successful table creation by querying the data using the QueryData tool.\n",
    "5) Entity RelationShip Diagram: Get the latest SQL schema with the GetDatabaseSchema tool.\n",
    "Then generate a new entity relationship diagram and save it with the tool SaveERM.\n",
    "\n",
    "Important Guidelines\n",
    "If you receive multiple datasets as input, process each one methodically and verify that all files have been processed before providing your final response.\n",
    "When creating table definitions, include SQL comments that explain the semantic type of each column and its potential use cases.\n",
    "For primary keys, add specific comments explaining the primary key constraint.\n",
    "Always ensure your SQL follows the appropriate format for the target system (ANSI SQL or Athena SQL).\"\"\"\n",
    "\n",
    "postfix = \"\".join(\n",
    "    random.choice(string.ascii_lowercase + \"0123456789\") for _ in range(8)\n",
    ")\n",
    "\n",
    "agent_name = agent_name + \"_\" + postfix\n",
    "\n",
    "IMAGE_URI = f'{AWS_ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/automatedinsights/lambda_dataengineer:latest'\n",
    "\n",
    "agentCollaboration = 'DISABLED' #'SUPERVISOR' #|'SUPERVISOR_ROUTER'|'DISABLED'\n",
    "\n",
    "sub_agents_list = []\n",
    "promptOverrideConfiguration = None\n",
    "\n",
    "lambda_environment_variables = {\n",
    "    \"S3_BUCKET_NAME\": S3_BUCKET_NAME,\n",
    "    \"MODEL_ID\": MODEL_ID\n",
    "}\n",
    "\n",
    "scenario = BedrockAgentScenarioWrapper(\n",
    "    bedrock_agent_client=bedrock_agent_client,\n",
    "    runtime_client=bedrock_agent_runtime_client,\n",
    "    lambda_client=lambda_client,\n",
    "    iam_resource=iam_resource,\n",
    "    postfix=postfix,\n",
    "    agent_name=agent_name,\n",
    "    model_id=MODEL_ID,\n",
    "    sub_agents_list=sub_agents_list,\n",
    "    prompt=prompt,\n",
    "    lambda_image_uri=IMAGE_URI,\n",
    "    lambda_environment_variables=lambda_environment_variables,\n",
    "    action_group_schema_path=\"action_groups/dataengineer_open_api_schema.yml\",\n",
    "    instruction=instruction,\n",
    "    agentCollaboration=agentCollaboration,\n",
    "    promptOverrideConfiguration=promptOverrideConfiguration\n",
    ")\n",
    "try:\n",
    "    scenario.run_scenario()\n",
    "except Exception as e:\n",
    "    logging.exception(f\"Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do the data preparation for the below details. Return the final Athena table name and the SQL table definition in the final response.[{\"name\": \"DataLocation\", \"type\": \"string\", \"value\": \"s3://huthmac-automatedinsights/uploads/Customers.csv\"}, {\"name\": \"AthenaDatabase\", \"type\": \"string\", \"value\": \"huthmac_automatedinsights\"}]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Do the data preparation for the below details. Return the final Athena table name and the SQL table definition in the final response.\"\n",
    "\n",
    "request_body = json.dumps([\n",
    "                    {\n",
    "                        \"name\": \"DataLocation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/Customers.csv\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"AthenaDatabase\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"\n",
    "                    }\n",
    "  \n",
    "], ensure_ascii=False)\n",
    "\n",
    "prompt = prompt + str(request_body)\n",
    "prompt\n",
    "# scenario.prompt = prompt\n",
    "\n",
    "# scenario.chat_with_agent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =scenario.agent\n",
    "\n",
    "AGENT_ID = agent.get('agentId')\n",
    "\n",
    "# get agent alias id\n",
    "agent_aliases = bedrock_agent_client.list_agent_aliases(agentId= AGENT_ID)\n",
    "\n",
    "AGENT_ALIAS_ID =  agent_aliases.get('agentAliasSummaries')[0].get('agentAliasId')\n",
    "print(f\"AGENT_ID: {AGENT_ID}\")\n",
    "print(f\"AGENT_ALIAS_ID: {AGENT_ALIAS_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save agent config to json file for evaluation\n",
    "agent_config = {\n",
    "    \"agent_id\": AGENT_ID,\n",
    "    \"agent_alias_id\": AGENT_ALIAS_ID,\n",
    "    \"human_id\": \"User\",\n",
    "    \"agent_name\": \"DataEngineer\",\n",
    "    \"agent_instruction\": instruction,\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"tool_name\": \"DataEngineerAPI\",\n",
    "            \"name\": \"DataEngineerAPI\",\n",
    "            \"description\": \"Data Preparation to make data available for AI/ML and Analytics workloads\",\n",
    "            \"actions\": [\n",
    "                {\n",
    "                    \"name\": \"GetDatabaseSchema\",\n",
    "                    \"description\": \"Retrieve the SQL database schema from S3\",\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"SQL table definition statements\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"GetERM\",\n",
    "                    \"description\": \"Retrieve the Entity Relationship Diagram in json format\",\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Entity Relationship Diagram in json format\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"SaveERM\",\n",
    "                    \"description\": \"Save the Entity Relationship Diagram in json format to S3\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"ERMData\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"Entity Relationship Diagram in json format\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"ERMData\"]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"erm_file_location\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"S3 location where the ERM was saved\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"GetInformationForSemanticTypeDetection\",\n",
    "                    \"description\": \"Get information for semantic type detection\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"S3 location of the data to analyze\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"DataLocation\"]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"column_names\": {\n",
    "                                \"data_type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"string\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"data_sample\": {\n",
    "                                \"data_type\": \"object\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"SaveSQLTableDefinition\",\n",
    "                    \"description\": \"Save SQL table definition to S3\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"SQL_Table_Definition\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"SQL table definition\"\n",
    "                            },\n",
    "                            \"TableName\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"File or table name\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"SQL_Table_Definition\"]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"sql_table_definition\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"sql_table_definition_file_location\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"CreateAthenaTable\",\n",
    "                    \"description\": \"Create an Athena table based on data and semantic types\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"DataLocation\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"S3 location of the data\"\n",
    "                            },\n",
    "                            \"AthenaDatabase\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"The Athena database to create the table in\"\n",
    "                            },\n",
    "                            \"Athena_Table_Create_SQL_statement\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"Athena SQL table create statement with LOCATION DataLocation\"\n",
    "                            },\n",
    "                            \"TableName\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"Name of the table to create\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"AthenaDatabase\", \"TableName\", \"Athena_Table_Create_SQL_statement\", \"DataLocation\"]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"status\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"table_name\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"table_location\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"table_definition\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"query_results\": {\n",
    "                                \"data_type\": \"object\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"QueryData\",\n",
    "                    \"description\": \"Execute SQL query against Athena database\",\n",
    "                    \"input_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"AthenaDatabase\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"The Athena database to query\"\n",
    "                            },\n",
    "                            \"SQLQuery\": {\n",
    "                                \"data_type\": \"string\",\n",
    "                                \"description\": \"SQL query to execute\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"AthenaDatabase\", \"SQLQuery\"]\n",
    "                    },\n",
    "                    \"output_schema\": {\n",
    "                        \"data_type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"status\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"query\": {\n",
    "                                \"data_type\": \"string\"\n",
    "                            },\n",
    "                            \"data\": {\n",
    "                                \"data_type\": \"object\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"requires_confirmation\": False,\n",
    "                    \"meta\": {}\n",
    "                }\n",
    "            ],\n",
    "            \"tool_type\": \"Module\",\n",
    "            \"meta\": {}\n",
    "        }\n",
    "    ],\n",
    "    \"reachable_agents\": []\n",
    "}\n",
    "# save agent config to json file\n",
    "with open('../dataengineer/agent.json', 'w') as f:\n",
    "    json.dump(agent_config, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define different evaluation scenarios\n",
    "evaluation_scenarios = {\n",
    "    \"scenarios\": [\n",
    "        {\n",
    "            \"scenario\": \"DataPreparation\",\n",
    "            \"input_problem\": (\n",
    "                'Do the data preparation with the following details. Return the final Athena table name and the SQL table definition in the final response. ' +\n",
    "                json.dumps([\n",
    "                    {\"name\": \"DataLocation\", \"type\": \"string\", \"value\": f\"s3://{S3_BUCKET_NAME}/uploads/Customers.csv\"},\n",
    "                    {\"name\": \"AthenaDatabase\", \"type\": \"string\", \"value\": f\"{S3_BUCKET_NAME.replace('-', '_')}\"}\n",
    "                ])\n",
    "            ),     \n",
    "            \"assertions\": [\n",
    "                \"agent: GetInformationForSemanticTypeDetection is executed to gather information about the data\",\n",
    "                \"agent: SaveSQLTableDefinition is executed to save the SQL table definition to S3\",\n",
    "                \"agent: CreateAthenaTable is executed to create the Athena table\",\n",
    "                \"agent: QueryData is executed to verify the Athena table creation\",\n",
    "                \"agent: GetDatabaseSchema is executed to get the latest database schema\",\n",
    "                \"agent: SaveERM is executed to save the Entity Relationship Diagram in json format to S3\"\n",
    "            ]\n",
    "        }  \n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "# save evaluation scenarios to json file\n",
    "with open('../dataengineer/scenarios.json', 'w') as f:\n",
    "    json.dump(evaluation_scenarios, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent evaluation\n",
    "from utils.benchmark import run_agent_evaluation\n",
    "\n",
    "dataset_dir = \"../dataengineer\"\n",
    "results = run_agent_evaluation(\n",
    "    scenario_filepath = f\"{dataset_dir}/scenarios.json\",\n",
    "    agent_filepath = f\"{dataset_dir}/agent.json\",\n",
    "    llm_judge_id = DATAENGINEER_AGENT_EVAL_PROFILE_ARN,\n",
    "    region = REGION,\n",
    "    session = session\n",
    ")\n",
    "\n",
    "# Check if results is not None before proceeding\n",
    "if results is not None:\n",
    "    # Create high-level metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'user_gsr': [results['user_gsr']],\n",
    "        'system_gsr': [results['system_gsr']],\n",
    "        'overall_gsr': [results['overall_gsr']],\n",
    "        'partial_gsr': [results['partial_gsr']],\n",
    "        'scenario_count': [results['scenario_count']],\n",
    "        'conversation_count': [results['conversation_count']]\n",
    "    })\n",
    "\n",
    "    # Create detailed assertions DataFrame\n",
    "    assertions_list = []\n",
    "    for eval_result in results['conversation_evals']:\n",
    "        trajectory_index = eval_result['trajectory_index']\n",
    "        for assertion in eval_result['report']:\n",
    "            assertions_list.append({\n",
    "                'trajectory_index': trajectory_index,\n",
    "                'assertion_type': assertion['assertion_type'],\n",
    "                'assertion': assertion['assertion'],\n",
    "                'answer': assertion['answer'],\n",
    "                'evidence': assertion['evidence']\n",
    "            })\n",
    "\n",
    "    assertions_df = pd.DataFrame(assertions_list)\n",
    "\n",
    "    # Display results\n",
    "    print(\"High-level Metrics:\")\n",
    "    display(metrics_df)\n",
    "\n",
    "    print(\"\\nDetailed Assertions:\")\n",
    "    display(assertions_df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: Please check for errors in the evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We first created a Docker container that contains all of the available tools/functions that the agent can use.\n",
    "- We then created a Bedrock Agent with an Action Group that uses the Docker container in a Lambda function as the execution environment.\n",
    "- We then created a set of evaluation scenarios that cover different aspects of the agent's behavior.\n",
    "- We then ran the agent evaluation and reviewed the results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
